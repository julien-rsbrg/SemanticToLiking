{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.nn\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from examples.introduction.my_GAT_implem import GNN_naive_framework\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"data/paired_data_newSim.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_pair</th>\n",
       "      <th>rated_similarity</th>\n",
       "      <th>abs_liking_diference</th>\n",
       "      <th>word1_liking</th>\n",
       "      <th>word2_liking</th>\n",
       "      <th>word1_experience</th>\n",
       "      <th>word2_experience</th>\n",
       "      <th>depression</th>\n",
       "      <th>depressionCont</th>\n",
       "      <th>female</th>\n",
       "      <th>age</th>\n",
       "      <th>participant</th>\n",
       "      <th>senenceBERT_mpnet_similarity</th>\n",
       "      <th>senenceBERT_miniLM_similarity</th>\n",
       "      <th>sense2vec_similarity</th>\n",
       "      <th>gptLarge_similarity</th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Art gallery. Autobiography book.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.375817</td>\n",
       "      <td>0.275882</td>\n",
       "      <td>0.337977</td>\n",
       "      <td>0.671500</td>\n",
       "      <td>Art gallery</td>\n",
       "      <td>Autobiography book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Art gallery. Baking cookies.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59</td>\n",
       "      <td>21</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.246449</td>\n",
       "      <td>0.146930</td>\n",
       "      <td>0.209372</td>\n",
       "      <td>0.593072</td>\n",
       "      <td>Art gallery</td>\n",
       "      <td>Baking cookies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Art gallery. Board games.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57</td>\n",
       "      <td>21</td>\n",
       "      <td>78</td>\n",
       "      <td>6</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.347372</td>\n",
       "      <td>0.224889</td>\n",
       "      <td>0.227290</td>\n",
       "      <td>0.726036</td>\n",
       "      <td>Art gallery</td>\n",
       "      <td>Board games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Art gallery. Book club.</td>\n",
       "      <td>52.0</td>\n",
       "      <td>38</td>\n",
       "      <td>21</td>\n",
       "      <td>59</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.390099</td>\n",
       "      <td>0.335998</td>\n",
       "      <td>0.307647</td>\n",
       "      <td>0.792389</td>\n",
       "      <td>Art gallery</td>\n",
       "      <td>Book club</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Art gallery. Bread making.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>75</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.270840</td>\n",
       "      <td>0.197813</td>\n",
       "      <td>0.225336</td>\n",
       "      <td>0.623973</td>\n",
       "      <td>Art gallery</td>\n",
       "      <td>Bread making</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          word_pair  rated_similarity  abs_liking_diference  \\\n",
       "0  Art gallery. Autobiography book.               NaN                     0   \n",
       "1      Art gallery. Baking cookies.               NaN                    59   \n",
       "2         Art gallery. Board games.               NaN                    57   \n",
       "3           Art gallery. Book club.              52.0                    38   \n",
       "4        Art gallery. Bread making.               NaN                    54   \n",
       "\n",
       "   word1_liking  word2_liking  word1_experience  word2_experience  depression  \\\n",
       "0            21            21                 6                 8           0   \n",
       "1            21            80                 6                14           0   \n",
       "2            21            78                 6                94           0   \n",
       "3            21            59                 6                 0           0   \n",
       "4            21            75                 6                16           0   \n",
       "\n",
       "   depressionCont  female  age  participant  senenceBERT_mpnet_similarity  \\\n",
       "0              12       1   29            1                      0.375817   \n",
       "1              12       1   29            1                      0.246449   \n",
       "2              12       1   29            1                      0.347372   \n",
       "3              12       1   29            1                      0.390099   \n",
       "4              12       1   29            1                      0.270840   \n",
       "\n",
       "   senenceBERT_miniLM_similarity  sense2vec_similarity  gptLarge_similarity  \\\n",
       "0                       0.275882              0.337977             0.671500   \n",
       "1                       0.146930              0.209372             0.593072   \n",
       "2                       0.224889              0.227290             0.726036   \n",
       "3                       0.335998              0.307647             0.792389   \n",
       "4                       0.197813              0.225336             0.623973   \n",
       "\n",
       "         word1               word2  \n",
       "0  Art gallery  Autobiography book  \n",
       "1  Art gallery      Baking cookies  \n",
       "2  Art gallery         Board games  \n",
       "3  Art gallery           Book club  \n",
       "4  Art gallery        Bread making  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_temp = data[\"word_pair\"].str.split(\".\")\n",
    "data[\"word1\"] = _temp.apply(func=lambda x: x[0])\n",
    "data[\"word2\"] = _temp.apply(func=lambda x: x[1][1:])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The edges of the word pairs are labelled in `word_pair`. We have the `rated_similarity` variable given by the participant with id stored in `participant`. Some similarity measures are given from different LLMs and stored under:\n",
    "- `senenceBERT_mpnet_similarity`,\n",
    "- `senenceBERT_miniLM_similarity`,\n",
    "- `sense2vec_similarity`,\n",
    "- `gptLarge_similarity`.\n",
    "\n",
    "For the edges, added to these similarity measures, we have the `abs_liking_difference` computed from `word1_liking` and `word2_liking`. The experience for the two words are stored in `word1_experience` and `word2_experience`. \n",
    "\n",
    "Depending directly on the participant, we have his/her `age`, gender under `female` (1 for female), and `depression` and `depressionCont` scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1_sc_liking</th>\n",
       "      <th>word2_sc_liking</th>\n",
       "      <th>sc_senenceBERT_mpnet_similarity</th>\n",
       "      <th>sc_depressionCont</th>\n",
       "      <th>sc_NoExp_Exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.172222</td>\n",
       "      <td>-1.152427</td>\n",
       "      <td>1.118413</td>\n",
       "      <td>-0.229448</td>\n",
       "      <td>-0.891164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.172222</td>\n",
       "      <td>0.605846</td>\n",
       "      <td>-0.021906</td>\n",
       "      <td>-0.229448</td>\n",
       "      <td>-0.891164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.172222</td>\n",
       "      <td>0.546244</td>\n",
       "      <td>0.867681</td>\n",
       "      <td>-0.229448</td>\n",
       "      <td>1.122128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.172222</td>\n",
       "      <td>-0.019980</td>\n",
       "      <td>1.244298</td>\n",
       "      <td>-0.229448</td>\n",
       "      <td>-0.891164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.172222</td>\n",
       "      <td>0.456840</td>\n",
       "      <td>0.193088</td>\n",
       "      <td>-0.229448</td>\n",
       "      <td>-0.891164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198235</th>\n",
       "      <td>0.456083</td>\n",
       "      <td>0.546244</td>\n",
       "      <td>0.040509</td>\n",
       "      <td>-0.708297</td>\n",
       "      <td>1.122128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198236</th>\n",
       "      <td>0.456083</td>\n",
       "      <td>1.201871</td>\n",
       "      <td>-0.668605</td>\n",
       "      <td>-0.708297</td>\n",
       "      <td>1.122128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198237</th>\n",
       "      <td>0.394638</td>\n",
       "      <td>0.546244</td>\n",
       "      <td>-1.800021</td>\n",
       "      <td>-0.708297</td>\n",
       "      <td>1.122128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198238</th>\n",
       "      <td>0.394638</td>\n",
       "      <td>1.201871</td>\n",
       "      <td>-0.165669</td>\n",
       "      <td>-0.708297</td>\n",
       "      <td>1.122128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198239</th>\n",
       "      <td>0.578974</td>\n",
       "      <td>1.201871</td>\n",
       "      <td>0.032073</td>\n",
       "      <td>-0.708297</td>\n",
       "      <td>-0.891164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198240 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word1_sc_liking  word2_sc_liking  sc_senenceBERT_mpnet_similarity  \\\n",
       "0             -1.172222        -1.152427                         1.118413   \n",
       "1             -1.172222         0.605846                        -0.021906   \n",
       "2             -1.172222         0.546244                         0.867681   \n",
       "3             -1.172222        -0.019980                         1.244298   \n",
       "4             -1.172222         0.456840                         0.193088   \n",
       "...                 ...              ...                              ...   \n",
       "198235         0.456083         0.546244                         0.040509   \n",
       "198236         0.456083         1.201871                        -0.668605   \n",
       "198237         0.394638         0.546244                        -1.800021   \n",
       "198238         0.394638         1.201871                        -0.165669   \n",
       "198239         0.578974         1.201871                         0.032073   \n",
       "\n",
       "        sc_depressionCont  sc_NoExp_Exp  \n",
       "0               -0.229448     -0.891164  \n",
       "1               -0.229448     -0.891164  \n",
       "2               -0.229448      1.122128  \n",
       "3               -0.229448     -0.891164  \n",
       "4               -0.229448     -0.891164  \n",
       "...                   ...           ...  \n",
       "198235          -0.708297      1.122128  \n",
       "198236          -0.708297      1.122128  \n",
       "198237          -0.708297      1.122128  \n",
       "198238          -0.708297      1.122128  \n",
       "198239          -0.708297     -0.891164  \n",
       "\n",
       "[198240 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def nor_function(a,b):\n",
    "    return (a or b) and not(a and b)\n",
    "\n",
    "data[\"NoExp_Exp\"] = data.apply(lambda row: nor_function(row[\"word1_experience\"]>50,row[\"word2_experience\"]>50),axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data.loc[:,[\"word1_sc_liking\",\"word2_sc_liking\",\"sc_senenceBERT_mpnet_similarity\",\"sc_depressionCont\",\"sc_NoExp_Exp\"]] = scaler.fit_transform(data.loc[:,[\"word1_liking\",\"word2_liking\",\"senenceBERT_mpnet_similarity\",\"depressionCont\",\"NoExp_Exp\"]])\n",
    "data.loc[:,[\"word1_sc_liking\",\"word2_sc_liking\",\"sc_senenceBERT_mpnet_similarity\",\"sc_depressionCont\",\"sc_NoExp_Exp\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test function convert_table_to_graph\n",
      "Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "validate: True\n",
      "is undirected: True\n",
      "has_self_loop: tensor(False)\n",
      "end Test function convert_table_to_graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6355/3484179637.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  extracted_features_word1.rename(columns=col_renaming,inplace=True)\n",
      "/tmp/ipykernel_6355/3484179637.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  extracted_features_word2.rename(columns=col_renaming,inplace=True)\n",
      "/tmp/ipykernel_6355/3484179637.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  complete_data_table[\"word1_index\"] = complete_data_table[\"word1\"].apply(lambda single_word: translater_word_to_index[single_word])\n",
      "/tmp/ipykernel_6355/3484179637.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  complete_data_table[\"word2_index\"] = complete_data_table[\"word2\"].apply(lambda single_word: translater_word_to_index[single_word])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1722],\n",
       "        [-1.1722],\n",
       "        [ 0.6404],\n",
       "        [ 0.5790],\n",
       "        [-0.0048],\n",
       "        [ 0.4868],\n",
       "        [ 0.8862],\n",
       "        [ 0.9476],\n",
       "        [ 0.8862],\n",
       "        [ 0.7940],\n",
       "        [-1.6023],\n",
       "        [-0.9264],\n",
       "        [ 0.5483],\n",
       "        [-0.1891],\n",
       "        [ 0.9169],\n",
       "        [-0.1891],\n",
       "        [ 0.5483],\n",
       "        [-0.6499],\n",
       "        [-0.2813],\n",
       "        [ 0.8555],\n",
       "        [ 0.7326],\n",
       "        [ 0.7633],\n",
       "        [ 1.2549],\n",
       "        [ 1.2549],\n",
       "        [ 1.0705],\n",
       "        [-0.2813],\n",
       "        [-1.2029],\n",
       "        [ 1.0705],\n",
       "        [-1.4180],\n",
       "        [ 0.9476],\n",
       "        [-1.2951],\n",
       "        [ 0.5483],\n",
       "        [-0.1891],\n",
       "        [ 0.7326],\n",
       "        [ 0.5175],\n",
       "        [ 1.2549],\n",
       "        [ 0.7326],\n",
       "        [-0.7421],\n",
       "        [-0.0969],\n",
       "        [-1.0801],\n",
       "        [-1.1415],\n",
       "        [ 0.9476],\n",
       "        [-0.9264],\n",
       "        [ 0.6097],\n",
       "        [ 1.2549],\n",
       "        [-1.6331],\n",
       "        [ 0.0874],\n",
       "        [-0.4042],\n",
       "        [-1.2951],\n",
       "        [ 1.0091],\n",
       "        [-0.4042],\n",
       "        [-0.5885],\n",
       "        [-1.5409],\n",
       "        [ 0.6097],\n",
       "        [-1.4180],\n",
       "        [ 0.4254],\n",
       "        [-0.4349],\n",
       "        [ 0.4254],\n",
       "        [ 1.2549],\n",
       "        [ 1.2019]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdata = data[data[\"participant\"] == 1]\n",
    "\n",
    "\n",
    "participant_graph = convert_table_to_graph(\n",
    "    complete_data_table=subdata,\n",
    "    node_attr_names=[\"sc_liking\"],\n",
    "    node_label_names=[\"sc_liking\"],\n",
    "    edge_attr_names=[\"senenceBERT_mpnet_similarity\"])\n",
    "\n",
    "participant_graph.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_module = torch_geometric.nn.GATConv(\n",
    "    in_channels=(1,1),\n",
    "    out_channels=1,\n",
    "    heads=1,\n",
    "    negative_slope=1.0,\n",
    "    add_self_loops=False,\n",
    "    edge_dim=1)\n",
    "# my_module(x=participant_graph.x,edge_index=participant_graph.edge_index,edge_attr=participant_graph.edge_attr)\n",
    "\n",
    "complete_model = GNN_naive_framework(my_module,device)\n",
    "opt = complete_model.configure_optimizer(lr=1)\n",
    "scheduler = complete_model.configure_scheduler(opt,1,1,10)\n",
    "\n",
    "participant_graph_batch_0 = Data(x=participant_graph.x[:30],\n",
    "                                 y=participant_graph.x[:30], \n",
    "                                 train_mask = torch.ones(30), \n",
    "                                 edge_index=torch.Tensor([[]]),\n",
    "                                 edge_attr=torch.Tensor([[]]))\n",
    "\n",
    "participant_graph_batch_1 = Data(x=participant_graph.x[30:],\n",
    "                                 y=participant_graph.x[30:], \n",
    "                                 train_mask = torch.ones(30), \n",
    "                                 edge_index=torch.Tensor([[]]),\n",
    "                                 edge_attr=torch.Tensor([[]]))\n",
    "\n",
    "complete_model.train([participant_graph_batch_0,participant_graph_batch_1],10000,1,opt,scheduler,\"train_loss\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.8243]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.2371], requires_grad=True), Parameter containing:\n",
      "tensor([[0.5436]], requires_grad=True), Parameter containing:\n",
      "tensor([0.8273], requires_grad=True)]\n",
      "== start training ==\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[1.1941],\n",
      "        [1.1941],\n",
      "        [1.0000],\n",
      "        [1.0060],\n",
      "        [1.0676],\n",
      "        [1.0152],\n",
      "        [0.9770],\n",
      "        [0.9716],\n",
      "        [0.9770],\n",
      "        [0.9854],\n",
      "        [1.2336],\n",
      "        [1.1691],\n",
      "        [1.0091],\n",
      "        [1.0881],\n",
      "        [0.9743],\n",
      "        [1.0881],\n",
      "        [1.0091],\n",
      "        [1.1395],\n",
      "        [1.0984],\n",
      "        [0.9798],\n",
      "        [0.9912],\n",
      "        [0.9883],\n",
      "        [0.9464],\n",
      "        [0.9464],\n",
      "        [0.9611],\n",
      "        [1.0984],\n",
      "        [1.1971],\n",
      "        [0.9611],\n",
      "        [1.2174],\n",
      "        [0.9716],\n",
      "        [1.2060],\n",
      "        [1.0091],\n",
      "        [1.0881],\n",
      "        [0.9912],\n",
      "        [1.0121],\n",
      "        [0.9464],\n",
      "        [0.9912],\n",
      "        [1.1495],\n",
      "        [1.0778],\n",
      "        [1.1849],\n",
      "        [1.1911],\n",
      "        [0.9716],\n",
      "        [1.1691],\n",
      "        [1.0030],\n",
      "        [0.9464],\n",
      "        [1.2361],\n",
      "        [1.0575],\n",
      "        [1.1122],\n",
      "        [1.2060],\n",
      "        [0.9663],\n",
      "        [1.1122],\n",
      "        [1.1327],\n",
      "        [1.2283],\n",
      "        [1.0030],\n",
      "        [1.2174],\n",
      "        [1.0215],\n",
      "        [1.1156],\n",
      "        [1.0215],\n",
      "        [0.9464],\n",
      "        [0.9505]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 1/10000,\n",
      " train_loss: 1.9826,\n",
      " train_mae: 0.8801,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-0.2599],\n",
      "        [-0.2599],\n",
      "        [-0.2845],\n",
      "        [-0.2836],\n",
      "        [-0.2752],\n",
      "        [-0.2823],\n",
      "        [-0.2882],\n",
      "        [-0.2892],\n",
      "        [-0.2882],\n",
      "        [-0.2868],\n",
      "        [-0.2546],\n",
      "        [-0.2629],\n",
      "        [-0.2832],\n",
      "        [-0.2727],\n",
      "        [-0.2887],\n",
      "        [-0.2727],\n",
      "        [-0.2832],\n",
      "        [-0.2665],\n",
      "        [-0.2714],\n",
      "        [-0.2878],\n",
      "        [-0.2859],\n",
      "        [-0.2864],\n",
      "        [-0.2939],\n",
      "        [-0.2939],\n",
      "        [-0.2910],\n",
      "        [-0.2714],\n",
      "        [-0.2595],\n",
      "        [-0.2910],\n",
      "        [-0.2569],\n",
      "        [-0.2892],\n",
      "        [-0.2583],\n",
      "        [-0.2832],\n",
      "        [-0.2727],\n",
      "        [-0.2859],\n",
      "        [-0.2827],\n",
      "        [-0.2939],\n",
      "        [-0.2859],\n",
      "        [-0.2653],\n",
      "        [-0.2740],\n",
      "        [-0.2610],\n",
      "        [-0.2602],\n",
      "        [-0.2892],\n",
      "        [-0.2629],\n",
      "        [-0.2841],\n",
      "        [-0.2939],\n",
      "        [-0.2543],\n",
      "        [-0.2765],\n",
      "        [-0.2698],\n",
      "        [-0.2583],\n",
      "        [-0.2901],\n",
      "        [-0.2698],\n",
      "        [-0.2673],\n",
      "        [-0.2554],\n",
      "        [-0.2841],\n",
      "        [-0.2569],\n",
      "        [-0.2814],\n",
      "        [-0.2694],\n",
      "        [-0.2814],\n",
      "        [-0.2939],\n",
      "        [-0.2931]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 2/10000,\n",
      " train_loss: 0.9752,\n",
      " train_mae: 1.0448,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-0.6342],\n",
      "        [-0.6342],\n",
      "        [-0.6813],\n",
      "        [-0.6794],\n",
      "        [-0.6623],\n",
      "        [-0.6765],\n",
      "        [-0.6892],\n",
      "        [-0.6913],\n",
      "        [-0.6892],\n",
      "        [-0.6862],\n",
      "        [-0.6256],\n",
      "        [-0.6395],\n",
      "        [-0.6784],\n",
      "        [-0.6574],\n",
      "        [-0.6902],\n",
      "        [-0.6574],\n",
      "        [-0.6784],\n",
      "        [-0.6458],\n",
      "        [-0.6550],\n",
      "        [-0.6882],\n",
      "        [-0.6842],\n",
      "        [-0.6852],\n",
      "        [-0.7019],\n",
      "        [-0.7019],\n",
      "        [-0.6954],\n",
      "        [-0.6550],\n",
      "        [-0.6335],\n",
      "        [-0.6954],\n",
      "        [-0.6292],\n",
      "        [-0.6913],\n",
      "        [-0.6316],\n",
      "        [-0.6784],\n",
      "        [-0.6574],\n",
      "        [-0.6842],\n",
      "        [-0.6775],\n",
      "        [-0.7019],\n",
      "        [-0.6842],\n",
      "        [-0.6437],\n",
      "        [-0.6598],\n",
      "        [-0.6361],\n",
      "        [-0.6348],\n",
      "        [-0.6913],\n",
      "        [-0.6395],\n",
      "        [-0.6803],\n",
      "        [-0.7019],\n",
      "        [-0.6250],\n",
      "        [-0.6649],\n",
      "        [-0.6518],\n",
      "        [-0.6316],\n",
      "        [-0.6933],\n",
      "        [-0.6518],\n",
      "        [-0.6473],\n",
      "        [-0.6268],\n",
      "        [-0.6803],\n",
      "        [-0.6292],\n",
      "        [-0.6747],\n",
      "        [-0.6511],\n",
      "        [-0.6747],\n",
      "        [-0.7019],\n",
      "        [-0.7000]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 3/10000,\n",
      " train_loss: 1.4243,\n",
      " train_mae: 0.9430,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-0.4893],\n",
      "        [-0.4893],\n",
      "        [-0.4828],\n",
      "        [-0.4830],\n",
      "        [-0.4851],\n",
      "        [-0.4833],\n",
      "        [-0.4820],\n",
      "        [-0.4818],\n",
      "        [-0.4820],\n",
      "        [-0.4823],\n",
      "        [-0.4910],\n",
      "        [-0.4884],\n",
      "        [-0.4831],\n",
      "        [-0.4857],\n",
      "        [-0.4819],\n",
      "        [-0.4857],\n",
      "        [-0.4831],\n",
      "        [-0.4874],\n",
      "        [-0.4861],\n",
      "        [-0.4821],\n",
      "        [-0.4825],\n",
      "        [-0.4824],\n",
      "        [-0.4807],\n",
      "        [-0.4807],\n",
      "        [-0.4814],\n",
      "        [-0.4861],\n",
      "        [-0.4895],\n",
      "        [-0.4814],\n",
      "        [-0.4903],\n",
      "        [-0.4818],\n",
      "        [-0.4898],\n",
      "        [-0.4831],\n",
      "        [-0.4857],\n",
      "        [-0.4825],\n",
      "        [-0.4832],\n",
      "        [-0.4807],\n",
      "        [-0.4825],\n",
      "        [-0.4877],\n",
      "        [-0.4854],\n",
      "        [-0.4890],\n",
      "        [-0.4892],\n",
      "        [-0.4818],\n",
      "        [-0.4884],\n",
      "        [-0.4829],\n",
      "        [-0.4807],\n",
      "        [-0.4911],\n",
      "        [-0.4847],\n",
      "        [-0.4865],\n",
      "        [-0.4898],\n",
      "        [-0.4816],\n",
      "        [-0.4865],\n",
      "        [-0.4872],\n",
      "        [-0.4908],\n",
      "        [-0.4829],\n",
      "        [-0.4903],\n",
      "        [-0.4836],\n",
      "        [-0.4866],\n",
      "        [-0.4836],\n",
      "        [-0.4807],\n",
      "        [-0.4809]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 4/10000,\n",
      " train_loss: 1.1360,\n",
      " train_mae: 0.8221,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-0.1508],\n",
      "        [-0.1508],\n",
      "        [-0.1173],\n",
      "        [-0.1180],\n",
      "        [-0.1261],\n",
      "        [-0.1192],\n",
      "        [-0.1147],\n",
      "        [-0.1140],\n",
      "        [-0.1147],\n",
      "        [-0.1156],\n",
      "        [-0.1639],\n",
      "        [-0.1444],\n",
      "        [-0.1184],\n",
      "        [-0.1291],\n",
      "        [-0.1144],\n",
      "        [-0.1291],\n",
      "        [-0.1184],\n",
      "        [-0.1380],\n",
      "        [-0.1307],\n",
      "        [-0.1150],\n",
      "        [-0.1163],\n",
      "        [-0.1160],\n",
      "        [-0.1112],\n",
      "        [-0.1112],\n",
      "        [-0.1129],\n",
      "        [-0.1307],\n",
      "        [-0.1516],\n",
      "        [-0.1129],\n",
      "        [-0.1579],\n",
      "        [-0.1140],\n",
      "        [-0.1542],\n",
      "        [-0.1184],\n",
      "        [-0.1291],\n",
      "        [-0.1163],\n",
      "        [-0.1188],\n",
      "        [-0.1112],\n",
      "        [-0.1163],\n",
      "        [-0.1400],\n",
      "        [-0.1275],\n",
      "        [-0.1483],\n",
      "        [-0.1499],\n",
      "        [-0.1140],\n",
      "        [-0.1444],\n",
      "        [-0.1177],\n",
      "        [-0.1112],\n",
      "        [-0.1649],\n",
      "        [-0.1246],\n",
      "        [-0.1330],\n",
      "        [-0.1542],\n",
      "        [-0.1134],\n",
      "        [-0.1330],\n",
      "        [-0.1367],\n",
      "        [-0.1618],\n",
      "        [-0.1177],\n",
      "        [-0.1579],\n",
      "        [-0.1199],\n",
      "        [-0.1336],\n",
      "        [-0.1199],\n",
      "        [-0.1112],\n",
      "        [-0.1117]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 5/10000,\n",
      " train_loss: 0.8407,\n",
      " train_mae: 0.7816,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[0.2041],\n",
      "        [0.2041],\n",
      "        [0.2502],\n",
      "        [0.2496],\n",
      "        [0.2417],\n",
      "        [0.2487],\n",
      "        [0.2523],\n",
      "        [0.2528],\n",
      "        [0.2523],\n",
      "        [0.2516],\n",
      "        [0.1763],\n",
      "        [0.2157],\n",
      "        [0.2493],\n",
      "        [0.2382],\n",
      "        [0.2525],\n",
      "        [0.2382],\n",
      "        [0.2493],\n",
      "        [0.2260],\n",
      "        [0.2362],\n",
      "        [0.2521],\n",
      "        [0.2511],\n",
      "        [0.2513],\n",
      "        [0.2546],\n",
      "        [0.2546],\n",
      "        [0.2536],\n",
      "        [0.2362],\n",
      "        [0.2025],\n",
      "        [0.2536],\n",
      "        [0.1895],\n",
      "        [0.2528],\n",
      "        [0.1972],\n",
      "        [0.2493],\n",
      "        [0.2382],\n",
      "        [0.2511],\n",
      "        [0.2490],\n",
      "        [0.2546],\n",
      "        [0.2511],\n",
      "        [0.2229],\n",
      "        [0.2400],\n",
      "        [0.2088],\n",
      "        [0.2057],\n",
      "        [0.2528],\n",
      "        [0.2157],\n",
      "        [0.2499],\n",
      "        [0.2546],\n",
      "        [0.1739],\n",
      "        [0.2433],\n",
      "        [0.2332],\n",
      "        [0.1972],\n",
      "        [0.2532],\n",
      "        [0.2332],\n",
      "        [0.2280],\n",
      "        [0.1810],\n",
      "        [0.2499],\n",
      "        [0.1895],\n",
      "        [0.2480],\n",
      "        [0.2324],\n",
      "        [0.2480],\n",
      "        [0.2546],\n",
      "        [0.2543]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 6/10000,\n",
      " train_loss: 0.8144,\n",
      " train_mae: 0.7653,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[0.4386],\n",
      "        [0.4386],\n",
      "        [0.5000],\n",
      "        [0.4996],\n",
      "        [0.4926],\n",
      "        [0.4988],\n",
      "        [0.5015],\n",
      "        [0.5017],\n",
      "        [0.5015],\n",
      "        [0.5010],\n",
      "        [0.3837],\n",
      "        [0.4582],\n",
      "        [0.4994],\n",
      "        [0.4888],\n",
      "        [0.5016],\n",
      "        [0.4888],\n",
      "        [0.4994],\n",
      "        [0.4736],\n",
      "        [0.4865],\n",
      "        [0.5013],\n",
      "        [0.5006],\n",
      "        [0.5008],\n",
      "        [0.5028],\n",
      "        [0.5028],\n",
      "        [0.5022],\n",
      "        [0.4865],\n",
      "        [0.4356],\n",
      "        [0.5022],\n",
      "        [0.4111],\n",
      "        [0.5017],\n",
      "        [0.4260],\n",
      "        [0.4994],\n",
      "        [0.4888],\n",
      "        [0.5006],\n",
      "        [0.4991],\n",
      "        [0.5028],\n",
      "        [0.5006],\n",
      "        [0.4691],\n",
      "        [0.4908],\n",
      "        [0.4467],\n",
      "        [0.4414],\n",
      "        [0.5017],\n",
      "        [0.4582],\n",
      "        [0.4998],\n",
      "        [0.5028],\n",
      "        [0.3784],\n",
      "        [0.4941],\n",
      "        [0.4829],\n",
      "        [0.4260],\n",
      "        [0.5020],\n",
      "        [0.4829],\n",
      "        [0.4762],\n",
      "        [0.3935],\n",
      "        [0.4998],\n",
      "        [0.4111],\n",
      "        [0.4983],\n",
      "        [0.4819],\n",
      "        [0.4983],\n",
      "        [0.5028],\n",
      "        [0.5027]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 7/10000,\n",
      " train_loss: 0.9380,\n",
      " train_mae: 0.7593,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[0.4740],\n",
      "        [0.4740],\n",
      "        [0.5692],\n",
      "        [0.5689],\n",
      "        [0.5623],\n",
      "        [0.5683],\n",
      "        [0.5703],\n",
      "        [0.5704],\n",
      "        [0.5703],\n",
      "        [0.5699],\n",
      "        [0.3525],\n",
      "        [0.5113],\n",
      "        [0.5687],\n",
      "        [0.5580],\n",
      "        [0.5704],\n",
      "        [0.5580],\n",
      "        [0.5687],\n",
      "        [0.5368],\n",
      "        [0.5551],\n",
      "        [0.5702],\n",
      "        [0.5697],\n",
      "        [0.5698],\n",
      "        [0.5711],\n",
      "        [0.5711],\n",
      "        [0.5707],\n",
      "        [0.5551],\n",
      "        [0.4681],\n",
      "        [0.5707],\n",
      "        [0.4157],\n",
      "        [0.5704],\n",
      "        [0.4480],\n",
      "        [0.5687],\n",
      "        [0.5580],\n",
      "        [0.5697],\n",
      "        [0.5685],\n",
      "        [0.5711],\n",
      "        [0.5697],\n",
      "        [0.5298],\n",
      "        [0.5603],\n",
      "        [0.4900],\n",
      "        [0.4797],\n",
      "        [0.5704],\n",
      "        [0.5113],\n",
      "        [0.5691],\n",
      "        [0.5711],\n",
      "        [0.3401],\n",
      "        [0.5639],\n",
      "        [0.5504],\n",
      "        [0.4480],\n",
      "        [0.5706],\n",
      "        [0.5504],\n",
      "        [0.5409],\n",
      "        [0.3758],\n",
      "        [0.5691],\n",
      "        [0.4157],\n",
      "        [0.5678],\n",
      "        [0.5491],\n",
      "        [0.5678],\n",
      "        [0.5711],\n",
      "        [0.5710]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 8/10000,\n",
      " train_loss: 0.9543,\n",
      " train_mae: 0.7180,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[ 0.2904],\n",
      "        [ 0.2904],\n",
      "        [ 0.4789],\n",
      "        [ 0.4787],\n",
      "        [ 0.4714],\n",
      "        [ 0.4781],\n",
      "        [ 0.4797],\n",
      "        [ 0.4799],\n",
      "        [ 0.4797],\n",
      "        [ 0.4795],\n",
      "        [-0.0241],\n",
      "        [ 0.3775],\n",
      "        [ 0.4785],\n",
      "        [ 0.4656],\n",
      "        [ 0.4798],\n",
      "        [ 0.4656],\n",
      "        [ 0.4785],\n",
      "        [ 0.4300],\n",
      "        [ 0.4614],\n",
      "        [ 0.4797],\n",
      "        [ 0.4793],\n",
      "        [ 0.4794],\n",
      "        [ 0.4803],\n",
      "        [ 0.4803],\n",
      "        [ 0.4801],\n",
      "        [ 0.4614],\n",
      "        [ 0.2757],\n",
      "        [ 0.4801],\n",
      "        [ 0.1415],\n",
      "        [ 0.4799],\n",
      "        [ 0.2252],\n",
      "        [ 0.4785],\n",
      "        [ 0.4656],\n",
      "        [ 0.4793],\n",
      "        [ 0.4783],\n",
      "        [ 0.4803],\n",
      "        [ 0.4793],\n",
      "        [ 0.4164],\n",
      "        [ 0.4688],\n",
      "        [ 0.3289],\n",
      "        [ 0.3041],\n",
      "        [ 0.4799],\n",
      "        [ 0.3775],\n",
      "        [ 0.4788],\n",
      "        [ 0.4803],\n",
      "        [-0.0566],\n",
      "        [ 0.4734],\n",
      "        [ 0.4541],\n",
      "        [ 0.2252],\n",
      "        [ 0.4800],\n",
      "        [ 0.4541],\n",
      "        [ 0.4375],\n",
      "        [ 0.0367],\n",
      "        [ 0.4788],\n",
      "        [ 0.1415],\n",
      "        [ 0.4777],\n",
      "        [ 0.4518],\n",
      "        [ 0.4777],\n",
      "        [ 0.4803],\n",
      "        [ 0.4802]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 9/10000,\n",
      " train_loss: 0.7680,\n",
      " train_mae: 0.6264,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-0.1856],\n",
      "        [-0.1856],\n",
      "        [ 0.2965],\n",
      "        [ 0.2962],\n",
      "        [ 0.2864],\n",
      "        [ 0.2957],\n",
      "        [ 0.2972],\n",
      "        [ 0.2974],\n",
      "        [ 0.2972],\n",
      "        [ 0.2970],\n",
      "        [-0.9492],\n",
      "        [ 0.0611],\n",
      "        [ 0.2961],\n",
      "        [ 0.2766],\n",
      "        [ 0.2973],\n",
      "        [ 0.2766],\n",
      "        [ 0.2961],\n",
      "        [ 0.1995],\n",
      "        [ 0.2689],\n",
      "        [ 0.2972],\n",
      "        [ 0.2969],\n",
      "        [ 0.2970],\n",
      "        [ 0.2976],\n",
      "        [ 0.2976],\n",
      "        [ 0.2975],\n",
      "        [ 0.2689],\n",
      "        [-0.2271],\n",
      "        [ 0.2975],\n",
      "        [-0.5836],\n",
      "        [ 0.2974],\n",
      "        [-0.3665],\n",
      "        [ 0.2961],\n",
      "        [ 0.2766],\n",
      "        [ 0.2969],\n",
      "        [ 0.2959],\n",
      "        [ 0.2976],\n",
      "        [ 0.2969],\n",
      "        [ 0.1652],\n",
      "        [ 0.2823],\n",
      "        [-0.0759],\n",
      "        [-0.1466],\n",
      "        [ 0.2974],\n",
      "        [ 0.0611],\n",
      "        [ 0.2964],\n",
      "        [ 0.2976],\n",
      "        [-1.0110],\n",
      "        [ 0.2895],\n",
      "        [ 0.2542],\n",
      "        [-0.3665],\n",
      "        [ 0.2974],\n",
      "        [ 0.2542],\n",
      "        [ 0.2174],\n",
      "        [-0.8251],\n",
      "        [ 0.2964],\n",
      "        [-0.5836],\n",
      "        [ 0.2952],\n",
      "        [ 0.2495],\n",
      "        [ 0.2952],\n",
      "        [ 0.2976],\n",
      "        [ 0.2976]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 10/10000,\n",
      " train_loss: 0.4641,\n",
      " train_mae: 0.5216,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1076],\n",
      "        [-1.1076],\n",
      "        [ 0.1175],\n",
      "        [ 0.1171],\n",
      "        [ 0.1017],\n",
      "        [ 0.1164],\n",
      "        [ 0.1182],\n",
      "        [ 0.1183],\n",
      "        [ 0.1182],\n",
      "        [ 0.1180],\n",
      "        [-2.0724],\n",
      "        [-0.5084],\n",
      "        [ 0.1169],\n",
      "        [ 0.0821],\n",
      "        [ 0.1182],\n",
      "        [ 0.0821],\n",
      "        [ 0.1169],\n",
      "        [-0.1161],\n",
      "        [ 0.0651],\n",
      "        [ 0.1181],\n",
      "        [ 0.1178],\n",
      "        [ 0.1179],\n",
      "        [ 0.1185],\n",
      "        [ 0.1185],\n",
      "        [ 0.1184],\n",
      "        [ 0.0651],\n",
      "        [-1.1913],\n",
      "        [ 0.1184],\n",
      "        [-1.7377],\n",
      "        [ 0.1183],\n",
      "        [-1.4389],\n",
      "        [ 0.1169],\n",
      "        [ 0.0821],\n",
      "        [ 0.1178],\n",
      "        [ 0.1167],\n",
      "        [ 0.1185],\n",
      "        [ 0.1178],\n",
      "        [-0.2132],\n",
      "        [ 0.0937],\n",
      "        [-0.8626],\n",
      "        [-1.0245],\n",
      "        [ 0.1183],\n",
      "        [-0.5084],\n",
      "        [ 0.1173],\n",
      "        [ 0.1185],\n",
      "        [-2.1141],\n",
      "        [ 0.1071],\n",
      "        [ 0.0301],\n",
      "        [-1.4389],\n",
      "        [ 0.1183],\n",
      "        [ 0.0301],\n",
      "        [-0.0664],\n",
      "        [-1.9773],\n",
      "        [ 0.1173],\n",
      "        [-1.7377],\n",
      "        [ 0.1158],\n",
      "        [ 0.0183],\n",
      "        [ 0.1158],\n",
      "        [ 0.1185],\n",
      "        [ 0.1185]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 11/10000,\n",
      " train_loss: 0.3654,\n",
      " train_mae: 0.6489,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.9712e+00],\n",
      "        [-1.9712e+00],\n",
      "        [ 5.5840e-02],\n",
      "        [ 5.5481e-02],\n",
      "        [ 3.2720e-02],\n",
      "        [ 5.4692e-02],\n",
      "        [ 5.6553e-02],\n",
      "        [ 5.6632e-02],\n",
      "        [ 5.6553e-02],\n",
      "        [ 5.6379e-02],\n",
      "        [-2.5907e+00],\n",
      "        [-1.1905e+00],\n",
      "        [ 5.5257e-02],\n",
      "        [-2.1856e-03],\n",
      "        [ 5.6595e-02],\n",
      "        [-2.1856e-03],\n",
      "        [ 5.5257e-02],\n",
      "        [-4.2460e-01],\n",
      "        [-3.4955e-02],\n",
      "        [ 5.6503e-02],\n",
      "        [ 5.6210e-02],\n",
      "        [ 5.6301e-02],\n",
      "        [ 5.6807e-02],\n",
      "        [ 5.6807e-02],\n",
      "        [ 5.6734e-02],\n",
      "        [-3.4955e-02],\n",
      "        [-2.0499e+00],\n",
      "        [ 5.6734e-02],\n",
      "        [-2.4367e+00],\n",
      "        [ 5.6632e-02],\n",
      "        [-2.2497e+00],\n",
      "        [ 5.5257e-02],\n",
      "        [-2.1856e-03],\n",
      "        [ 5.6210e-02],\n",
      "        [ 5.4996e-02],\n",
      "        [ 5.6807e-02],\n",
      "        [ 5.6210e-02],\n",
      "        [-6.3208e-01],\n",
      "        [ 1.9053e-02],\n",
      "        [-1.7026e+00],\n",
      "        [-1.8868e+00],\n",
      "        [ 5.6632e-02],\n",
      "        [-1.1905e+00],\n",
      "        [ 5.5674e-02],\n",
      "        [ 5.6807e-02],\n",
      "        [-2.6063e+00],\n",
      "        [ 4.1474e-02],\n",
      "        [-1.0674e-01],\n",
      "        [-2.2497e+00],\n",
      "        [ 5.6691e-02],\n",
      "        [-1.0674e-01],\n",
      "        [-3.1589e-01],\n",
      "        [-2.5522e+00],\n",
      "        [ 5.5674e-02],\n",
      "        [-2.4367e+00],\n",
      "        [ 5.3928e-02],\n",
      "        [-1.3163e-01],\n",
      "        [ 5.3928e-02],\n",
      "        [ 5.6807e-02],\n",
      "        [ 5.6792e-02]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 12/10000,\n",
      " train_loss: 0.5368,\n",
      " train_mae: 0.6393,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-2.1985],\n",
      "        [-2.1985],\n",
      "        [ 0.1611],\n",
      "        [ 0.1607],\n",
      "        [ 0.1343],\n",
      "        [ 0.1600],\n",
      "        [ 0.1617],\n",
      "        [ 0.1618],\n",
      "        [ 0.1617],\n",
      "        [ 0.1616],\n",
      "        [-2.5890],\n",
      "        [-1.4840],\n",
      "        [ 0.1605],\n",
      "        [ 0.0887],\n",
      "        [ 0.1617],\n",
      "        [ 0.0887],\n",
      "        [ 0.1605],\n",
      "        [-0.5193],\n",
      "        [ 0.0436],\n",
      "        [ 0.1617],\n",
      "        [ 0.1614],\n",
      "        [ 0.1615],\n",
      "        [ 0.1619],\n",
      "        [ 0.1619],\n",
      "        [ 0.1618],\n",
      "        [ 0.0436],\n",
      "        [-2.2566],\n",
      "        [ 0.1618],\n",
      "        [-2.5069],\n",
      "        [ 0.1618],\n",
      "        [-2.3933],\n",
      "        [ 0.1605],\n",
      "        [ 0.0887],\n",
      "        [ 0.1614],\n",
      "        [ 0.1603],\n",
      "        [ 0.1619],\n",
      "        [ 0.1614],\n",
      "        [-0.8059],\n",
      "        [ 0.1169],\n",
      "        [-1.9816],\n",
      "        [-2.1335],\n",
      "        [ 0.1618],\n",
      "        [-1.4840],\n",
      "        [ 0.1609],\n",
      "        [ 0.1619],\n",
      "        [-2.5966],\n",
      "        [ 0.1451],\n",
      "        [-0.0588],\n",
      "        [-2.3933],\n",
      "        [ 0.1618],\n",
      "        [-0.0588],\n",
      "        [-0.3633],\n",
      "        [-2.5696],\n",
      "        [ 0.1609],\n",
      "        [-2.5069],\n",
      "        [ 0.1592],\n",
      "        [-0.0948],\n",
      "        [ 0.1592],\n",
      "        [ 0.1619],\n",
      "        [ 0.1619]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 13/10000,\n",
      " train_loss: 0.5178,\n",
      " train_mae: 0.5062,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.9792],\n",
      "        [-1.9792],\n",
      "        [ 0.3947],\n",
      "        [ 0.3944],\n",
      "        [ 0.3690],\n",
      "        [ 0.3938],\n",
      "        [ 0.3952],\n",
      "        [ 0.3953],\n",
      "        [ 0.3952],\n",
      "        [ 0.3951],\n",
      "        [-2.2687],\n",
      "        [-1.3502],\n",
      "        [ 0.3942],\n",
      "        [ 0.3217],\n",
      "        [ 0.3953],\n",
      "        [ 0.3217],\n",
      "        [ 0.3942],\n",
      "        [-0.3515],\n",
      "        [ 0.2733],\n",
      "        [ 0.3952],\n",
      "        [ 0.3950],\n",
      "        [ 0.3951],\n",
      "        [ 0.3954],\n",
      "        [ 0.3954],\n",
      "        [ 0.3953],\n",
      "        [ 0.2733],\n",
      "        [-2.0253],\n",
      "        [ 0.3953],\n",
      "        [-2.2126],\n",
      "        [ 0.3953],\n",
      "        [-2.1301],\n",
      "        [ 0.3942],\n",
      "        [ 0.3217],\n",
      "        [ 0.3950],\n",
      "        [ 0.3940],\n",
      "        [ 0.3954],\n",
      "        [ 0.3950],\n",
      "        [-0.6628],\n",
      "        [ 0.3512],\n",
      "        [-1.7999],\n",
      "        [-1.9267],\n",
      "        [ 0.3953],\n",
      "        [-1.3502],\n",
      "        [ 0.3946],\n",
      "        [ 0.3954],\n",
      "        [-2.2737],\n",
      "        [ 0.3797],\n",
      "        [ 0.1611],\n",
      "        [-2.1301],\n",
      "        [ 0.3953],\n",
      "        [ 0.1611],\n",
      "        [-0.1786],\n",
      "        [-2.2558],\n",
      "        [ 0.3946],\n",
      "        [-2.2126],\n",
      "        [ 0.3931],\n",
      "        [ 0.1211],\n",
      "        [ 0.3931],\n",
      "        [ 0.3954],\n",
      "        [ 0.3954]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 14/10000,\n",
      " train_loss: 0.3161,\n",
      " train_mae: 0.3747,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.5633],\n",
      "        [-1.5633],\n",
      "        [ 0.6826],\n",
      "        [ 0.6824],\n",
      "        [ 0.6598],\n",
      "        [ 0.6819],\n",
      "        [ 0.6831],\n",
      "        [ 0.6831],\n",
      "        [ 0.6831],\n",
      "        [ 0.6830],\n",
      "        [-1.7946],\n",
      "        [-1.0096],\n",
      "        [ 0.6823],\n",
      "        [ 0.6155],\n",
      "        [ 0.6831],\n",
      "        [ 0.6155],\n",
      "        [ 0.6823],\n",
      "        [-0.0496],\n",
      "        [ 0.5692],\n",
      "        [ 0.6830],\n",
      "        [ 0.6829],\n",
      "        [ 0.6829],\n",
      "        [ 0.6832],\n",
      "        [ 0.6832],\n",
      "        [ 0.6831],\n",
      "        [ 0.5692],\n",
      "        [-1.6015],\n",
      "        [ 0.6831],\n",
      "        [-1.7519],\n",
      "        [ 0.6831],\n",
      "        [-1.6868],\n",
      "        [ 0.6823],\n",
      "        [ 0.6155],\n",
      "        [ 0.6829],\n",
      "        [ 0.6821],\n",
      "        [ 0.6832],\n",
      "        [ 0.6829],\n",
      "        [-0.3563],\n",
      "        [ 0.6433],\n",
      "        [-1.4111],\n",
      "        [-1.5192],\n",
      "        [ 0.6831],\n",
      "        [-1.0096],\n",
      "        [ 0.6825],\n",
      "        [ 0.6832],\n",
      "        [-1.7983],\n",
      "        [ 0.6695],\n",
      "        [ 0.4597],\n",
      "        [-1.6868],\n",
      "        [ 0.6831],\n",
      "        [ 0.4597],\n",
      "        [ 0.1223],\n",
      "        [-1.7850],\n",
      "        [ 0.6825],\n",
      "        [-1.7519],\n",
      "        [ 0.6813],\n",
      "        [ 0.4202],\n",
      "        [ 0.6813],\n",
      "        [ 0.6832],\n",
      "        [ 0.6832]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 15/10000,\n",
      " train_loss: 0.2054,\n",
      " train_mae: 0.3896,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1593],\n",
      "        [-1.1593],\n",
      "        [ 0.9368],\n",
      "        [ 0.9366],\n",
      "        [ 0.9159],\n",
      "        [ 0.9362],\n",
      "        [ 0.9372],\n",
      "        [ 0.9372],\n",
      "        [ 0.9372],\n",
      "        [ 0.9371],\n",
      "        [-1.3371],\n",
      "        [-0.6904],\n",
      "        [ 0.9365],\n",
      "        [ 0.8735],\n",
      "        [ 0.9372],\n",
      "        [ 0.8735],\n",
      "        [ 0.9365],\n",
      "        [ 0.2126],\n",
      "        [ 0.8282],\n",
      "        [ 0.9372],\n",
      "        [ 0.9370],\n",
      "        [ 0.9371],\n",
      "        [ 0.9373],\n",
      "        [ 0.9373],\n",
      "        [ 0.9372],\n",
      "        [ 0.8282],\n",
      "        [-1.1897],\n",
      "        [ 0.9372],\n",
      "        [-1.3058],\n",
      "        [ 0.9372],\n",
      "        [-1.2563],\n",
      "        [ 0.9365],\n",
      "        [ 0.8735],\n",
      "        [ 0.9370],\n",
      "        [ 0.9364],\n",
      "        [ 0.9373],\n",
      "        [ 0.9370],\n",
      "        [-0.0859],\n",
      "        [ 0.9003],\n",
      "        [-1.0352],\n",
      "        [-1.1238],\n",
      "        [ 0.9372],\n",
      "        [-0.6904],\n",
      "        [ 0.9368],\n",
      "        [ 0.9373],\n",
      "        [-1.3397],\n",
      "        [ 0.9250],\n",
      "        [ 0.7197],\n",
      "        [-1.2563],\n",
      "        [ 0.9372],\n",
      "        [ 0.7197],\n",
      "        [ 0.3828],\n",
      "        [-1.3301],\n",
      "        [ 0.9368],\n",
      "        [-1.3058],\n",
      "        [ 0.9357],\n",
      "        [ 0.6803],\n",
      "        [ 0.9357],\n",
      "        [ 0.9373],\n",
      "        [ 0.9373]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 16/10000,\n",
      " train_loss: 0.2868,\n",
      " train_mae: 0.5109,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-9.2065e-01],\n",
      "        [-9.2065e-01],\n",
      "        [ 1.0797e+00],\n",
      "        [ 1.0795e+00],\n",
      "        [ 1.0583e+00],\n",
      "        [ 1.0790e+00],\n",
      "        [ 1.0800e+00],\n",
      "        [ 1.0800e+00],\n",
      "        [ 1.0800e+00],\n",
      "        [ 1.0799e+00],\n",
      "        [-1.0441e+00],\n",
      "        [-5.5091e-01],\n",
      "        [ 1.0794e+00],\n",
      "        [ 1.0127e+00],\n",
      "        [ 1.0800e+00],\n",
      "        [ 1.0127e+00],\n",
      "        [ 1.0794e+00],\n",
      "        [ 2.9866e-01],\n",
      "        [ 9.6324e-01],\n",
      "        [ 1.0799e+00],\n",
      "        [ 1.0798e+00],\n",
      "        [ 1.0799e+00],\n",
      "        [ 1.0800e+00],\n",
      "        [ 1.0800e+00],\n",
      "        [ 1.0800e+00],\n",
      "        [ 9.6324e-01],\n",
      "        [-9.4266e-01],\n",
      "        [ 1.0800e+00],\n",
      "        [-1.0235e+00],\n",
      "        [ 1.0800e+00],\n",
      "        [-9.8978e-01],\n",
      "        [ 1.0794e+00],\n",
      "        [ 1.0127e+00],\n",
      "        [ 1.0798e+00],\n",
      "        [ 1.0792e+00],\n",
      "        [ 1.0800e+00],\n",
      "        [ 1.0798e+00],\n",
      "        [-9.8956e-04],\n",
      "        [ 1.0416e+00],\n",
      "        [-8.2814e-01],\n",
      "        [-8.9462e-01],\n",
      "        [ 1.0800e+00],\n",
      "        [-5.5091e-01],\n",
      "        [ 1.0796e+00],\n",
      "        [ 1.0800e+00],\n",
      "        [-1.0458e+00],\n",
      "        [ 1.0677e+00],\n",
      "        [ 8.4352e-01],\n",
      "        [-9.8978e-01],\n",
      "        [ 1.0800e+00],\n",
      "        [ 8.4352e-01],\n",
      "        [ 4.7719e-01],\n",
      "        [-1.0396e+00],\n",
      "        [ 1.0796e+00],\n",
      "        [-1.0235e+00],\n",
      "        [ 1.0785e+00],\n",
      "        [ 8.0010e-01],\n",
      "        [ 1.0785e+00],\n",
      "        [ 1.0800e+00],\n",
      "        [ 1.0800e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 17/10000,\n",
      " train_loss: 0.4037,\n",
      " train_mae: 0.5029,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-0.9071],\n",
      "        [-0.9071],\n",
      "        [ 1.0808],\n",
      "        [ 1.0806],\n",
      "        [ 1.0556],\n",
      "        [ 1.0802],\n",
      "        [ 1.0811],\n",
      "        [ 1.0812],\n",
      "        [ 1.0811],\n",
      "        [ 1.0811],\n",
      "        [-0.9843],\n",
      "        [-0.6390],\n",
      "        [ 1.0805],\n",
      "        [ 0.9991],\n",
      "        [ 1.0812],\n",
      "        [ 0.9991],\n",
      "        [ 1.0805],\n",
      "        [ 0.1494],\n",
      "        [ 0.9366],\n",
      "        [ 1.0811],\n",
      "        [ 1.0810],\n",
      "        [ 1.0810],\n",
      "        [ 1.0812],\n",
      "        [ 1.0812],\n",
      "        [ 1.0812],\n",
      "        [ 0.9366],\n",
      "        [-0.9215],\n",
      "        [ 1.0812],\n",
      "        [-0.9722],\n",
      "        [ 1.0812],\n",
      "        [-0.9516],\n",
      "        [ 1.0805],\n",
      "        [ 0.9991],\n",
      "        [ 1.0810],\n",
      "        [ 1.0803],\n",
      "        [ 1.0812],\n",
      "        [ 1.0810],\n",
      "        [-0.1568],\n",
      "        [ 1.0352],\n",
      "        [-0.8445],\n",
      "        [-0.8899],\n",
      "        [ 1.0812],\n",
      "        [-0.6390],\n",
      "        [ 1.0807],\n",
      "        [ 1.0812],\n",
      "        [-0.9852],\n",
      "        [ 1.0671],\n",
      "        [ 0.7858],\n",
      "        [-0.9516],\n",
      "        [ 1.0812],\n",
      "        [ 0.7858],\n",
      "        [ 0.3469],\n",
      "        [-0.9817],\n",
      "        [ 1.0807],\n",
      "        [-0.9722],\n",
      "        [ 1.0796],\n",
      "        [ 0.7317],\n",
      "        [ 1.0796],\n",
      "        [ 1.0812],\n",
      "        [ 1.0812]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 18/10000,\n",
      " train_loss: 0.3843,\n",
      " train_mae: 0.3801,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.0873e+00],\n",
      "        [-1.0873e+00],\n",
      "        [ 9.6218e-01],\n",
      "        [ 9.6195e-01],\n",
      "        [ 9.2856e-01],\n",
      "        [ 9.6137e-01],\n",
      "        [ 9.6254e-01],\n",
      "        [ 9.6257e-01],\n",
      "        [ 9.6254e-01],\n",
      "        [ 9.6246e-01],\n",
      "        [-1.1319e+00],\n",
      "        [-9.0840e-01],\n",
      "        [ 9.6179e-01],\n",
      "        [ 8.4907e-01],\n",
      "        [ 9.6255e-01],\n",
      "        [ 8.4907e-01],\n",
      "        [ 9.6179e-01],\n",
      "        [-2.1537e-01],\n",
      "        [ 7.6061e-01],\n",
      "        [ 9.6252e-01],\n",
      "        [ 9.6238e-01],\n",
      "        [ 9.6243e-01],\n",
      "        [ 9.6261e-01],\n",
      "        [ 9.6261e-01],\n",
      "        [ 9.6260e-01],\n",
      "        [ 7.6061e-01],\n",
      "        [-1.0960e+00],\n",
      "        [ 9.6260e-01],\n",
      "        [-1.1253e+00],\n",
      "        [ 9.6257e-01],\n",
      "        [-1.1136e+00],\n",
      "        [ 9.6179e-01],\n",
      "        [ 8.4907e-01],\n",
      "        [ 9.6238e-01],\n",
      "        [ 9.6160e-01],\n",
      "        [ 9.6261e-01],\n",
      "        [ 9.6238e-01],\n",
      "        [-5.1525e-01],\n",
      "        [ 9.0007e-01],\n",
      "        [-1.0483e+00],\n",
      "        [-1.0767e+00],\n",
      "        [ 9.6257e-01],\n",
      "        [-9.0840e-01],\n",
      "        [ 9.6207e-01],\n",
      "        [ 9.6261e-01],\n",
      "        [-1.1324e+00],\n",
      "        [ 9.4419e-01],\n",
      "        [ 5.5101e-01],\n",
      "        [-1.1136e+00],\n",
      "        [ 9.6258e-01],\n",
      "        [ 5.5101e-01],\n",
      "        [ 1.0669e-03],\n",
      "        [-1.1305e+00],\n",
      "        [ 9.6207e-01],\n",
      "        [-1.1253e+00],\n",
      "        [ 9.6072e-01],\n",
      "        [ 4.7813e-01],\n",
      "        [ 9.6072e-01],\n",
      "        [ 9.6261e-01],\n",
      "        [ 9.6261e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 19/10000,\n",
      " train_loss: 0.2488,\n",
      " train_mae: 0.3007,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.3709],\n",
      "        [-1.3709],\n",
      "        [ 0.7758],\n",
      "        [ 0.7755],\n",
      "        [ 0.7275],\n",
      "        [ 0.7747],\n",
      "        [ 0.7762],\n",
      "        [ 0.7763],\n",
      "        [ 0.7762],\n",
      "        [ 0.7762],\n",
      "        [-1.3954],\n",
      "        [-1.2593],\n",
      "        [ 0.7753],\n",
      "        [ 0.6082],\n",
      "        [ 0.7763],\n",
      "        [ 0.6082],\n",
      "        [ 0.7753],\n",
      "        [-0.7022],\n",
      "        [ 0.4764],\n",
      "        [ 0.7762],\n",
      "        [ 0.7761],\n",
      "        [ 0.7761],\n",
      "        [ 0.7763],\n",
      "        [ 0.7763],\n",
      "        [ 0.7763],\n",
      "        [ 0.4764],\n",
      "        [-1.3758],\n",
      "        [ 0.7763],\n",
      "        [-1.3920],\n",
      "        [ 0.7763],\n",
      "        [-1.3857],\n",
      "        [ 0.7753],\n",
      "        [ 0.6082],\n",
      "        [ 0.7761],\n",
      "        [ 0.7751],\n",
      "        [ 0.7763],\n",
      "        [ 0.7761],\n",
      "        [-0.9677],\n",
      "        [ 0.6849],\n",
      "        [-1.3479],\n",
      "        [-1.3647],\n",
      "        [ 0.7763],\n",
      "        [-1.2593],\n",
      "        [ 0.7757],\n",
      "        [ 0.7763],\n",
      "        [-1.3956],\n",
      "        [ 0.7505],\n",
      "        [ 0.1787],\n",
      "        [-1.3857],\n",
      "        [ 0.7763],\n",
      "        [ 0.1787],\n",
      "        [-0.4849],\n",
      "        [-1.3947],\n",
      "        [ 0.7757],\n",
      "        [-1.3920],\n",
      "        [ 0.7739],\n",
      "        [ 0.0809],\n",
      "        [ 0.7739],\n",
      "        [ 0.7763],\n",
      "        [ 0.7763]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 20/10000,\n",
      " train_loss: 0.1433,\n",
      " train_mae: 0.3334,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.6349],\n",
      "        [-1.6349],\n",
      "        [ 0.5874],\n",
      "        [ 0.5870],\n",
      "        [ 0.5174],\n",
      "        [ 0.5860],\n",
      "        [ 0.5879],\n",
      "        [ 0.5880],\n",
      "        [ 0.5879],\n",
      "        [ 0.5878],\n",
      "        [-1.6483],\n",
      "        [-1.5680],\n",
      "        [ 0.5867],\n",
      "        [ 0.3399],\n",
      "        [ 0.5880],\n",
      "        [ 0.3399],\n",
      "        [ 0.5867],\n",
      "        [-1.1591],\n",
      "        [ 0.1493],\n",
      "        [ 0.5879],\n",
      "        [ 0.5877],\n",
      "        [ 0.5878],\n",
      "        [ 0.5880],\n",
      "        [ 0.5880],\n",
      "        [ 0.5880],\n",
      "        [ 0.1493],\n",
      "        [-1.6377],\n",
      "        [ 0.5880],\n",
      "        [-1.6465],\n",
      "        [ 0.5880],\n",
      "        [-1.6431],\n",
      "        [ 0.5867],\n",
      "        [ 0.3399],\n",
      "        [ 0.5877],\n",
      "        [ 0.5864],\n",
      "        [ 0.5880],\n",
      "        [ 0.5877],\n",
      "        [-1.3684],\n",
      "        [ 0.4539],\n",
      "        [-1.6218],\n",
      "        [-1.6315],\n",
      "        [ 0.5880],\n",
      "        [-1.5680],\n",
      "        [ 0.5872],\n",
      "        [ 0.5880],\n",
      "        [-1.6484],\n",
      "        [ 0.5513],\n",
      "        [-0.2479],\n",
      "        [-1.6431],\n",
      "        [ 0.5880],\n",
      "        [-0.2479],\n",
      "        [-0.9672],\n",
      "        [-1.6479],\n",
      "        [ 0.5872],\n",
      "        [-1.6465],\n",
      "        [ 0.5849],\n",
      "        [-0.3678],\n",
      "        [ 0.5849],\n",
      "        [ 0.5880],\n",
      "        [ 0.5880]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 21/10000,\n",
      " train_loss: 0.1564,\n",
      " train_mae: 0.4031,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.7593],\n",
      "        [-1.7593],\n",
      "        [ 0.4598],\n",
      "        [ 0.4594],\n",
      "        [ 0.3640],\n",
      "        [ 0.4581],\n",
      "        [ 0.4605],\n",
      "        [ 0.4605],\n",
      "        [ 0.4605],\n",
      "        [ 0.4604],\n",
      "        [-1.7667],\n",
      "        [-1.7193],\n",
      "        [ 0.4590],\n",
      "        [ 0.1196],\n",
      "        [ 0.4605],\n",
      "        [ 0.1196],\n",
      "        [ 0.4590],\n",
      "        [-1.4364],\n",
      "        [-0.1301],\n",
      "        [ 0.4605],\n",
      "        [ 0.4602],\n",
      "        [ 0.4603],\n",
      "        [ 0.4606],\n",
      "        [ 0.4606],\n",
      "        [ 0.4606],\n",
      "        [-0.1301],\n",
      "        [-1.7609],\n",
      "        [ 0.4606],\n",
      "        [-1.7658],\n",
      "        [ 0.4605],\n",
      "        [-1.7640],\n",
      "        [ 0.4590],\n",
      "        [ 0.1196],\n",
      "        [ 0.4602],\n",
      "        [ 0.4586],\n",
      "        [ 0.4606],\n",
      "        [ 0.4602],\n",
      "        [-1.5882],\n",
      "        [ 0.2757],\n",
      "        [-1.7517],\n",
      "        [-1.7574],\n",
      "        [ 0.4605],\n",
      "        [-1.7193],\n",
      "        [ 0.4596],\n",
      "        [ 0.4606],\n",
      "        [-1.7668],\n",
      "        [ 0.4112],\n",
      "        [-0.5991],\n",
      "        [-1.7640],\n",
      "        [ 0.4606],\n",
      "        [-0.5991],\n",
      "        [-1.2846],\n",
      "        [-1.7665],\n",
      "        [ 0.4596],\n",
      "        [-1.7658],\n",
      "        [ 0.4566],\n",
      "        [-0.7274],\n",
      "        [ 0.4566],\n",
      "        [ 0.4606],\n",
      "        [ 0.4606]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 22/10000,\n",
      " train_loss: 0.2215,\n",
      " train_mae: 0.3956,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.6811],\n",
      "        [-1.6811],\n",
      "        [ 0.4289],\n",
      "        [ 0.4284],\n",
      "        [ 0.3103],\n",
      "        [ 0.4269],\n",
      "        [ 0.4296],\n",
      "        [ 0.4297],\n",
      "        [ 0.4296],\n",
      "        [ 0.4295],\n",
      "        [-1.6854],\n",
      "        [-1.6565],\n",
      "        [ 0.4280],\n",
      "        [ 0.0106],\n",
      "        [ 0.4297],\n",
      "        [ 0.0106],\n",
      "        [ 0.4280],\n",
      "        [-1.4642],\n",
      "        [-0.2773],\n",
      "        [ 0.4296],\n",
      "        [ 0.4293],\n",
      "        [ 0.4294],\n",
      "        [ 0.4298],\n",
      "        [ 0.4298],\n",
      "        [ 0.4297],\n",
      "        [-0.2773],\n",
      "        [-1.6820],\n",
      "        [ 0.4297],\n",
      "        [-1.6849],\n",
      "        [ 0.4297],\n",
      "        [-1.6838],\n",
      "        [ 0.4280],\n",
      "        [ 0.0106],\n",
      "        [ 0.4293],\n",
      "        [ 0.4275],\n",
      "        [ 0.4298],\n",
      "        [ 0.4293],\n",
      "        [-1.5705],\n",
      "        [ 0.2004],\n",
      "        [-1.6765],\n",
      "        [-1.6799],\n",
      "        [ 0.4297],\n",
      "        [-1.6565],\n",
      "        [ 0.4287],\n",
      "        [ 0.4298],\n",
      "        [-1.6854],\n",
      "        [ 0.3692],\n",
      "        [-0.7626],\n",
      "        [-1.6838],\n",
      "        [ 0.4297],\n",
      "        [-0.7626],\n",
      "        [-1.3515],\n",
      "        [-1.6853],\n",
      "        [ 0.4287],\n",
      "        [-1.6849],\n",
      "        [ 0.4252],\n",
      "        [-0.8832],\n",
      "        [ 0.4252],\n",
      "        [ 0.4298],\n",
      "        [ 0.4298]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 23/10000,\n",
      " train_loss: 0.2203,\n",
      " train_mae: 0.3154,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.4233],\n",
      "        [-1.4233],\n",
      "        [ 0.4900],\n",
      "        [ 0.4894],\n",
      "        [ 0.3572],\n",
      "        [ 0.4878],\n",
      "        [ 0.4907],\n",
      "        [ 0.4908],\n",
      "        [ 0.4907],\n",
      "        [ 0.4906],\n",
      "        [-1.4259],\n",
      "        [-1.4076],\n",
      "        [ 0.4890],\n",
      "        [ 0.0281],\n",
      "        [ 0.4908],\n",
      "        [ 0.0281],\n",
      "        [ 0.4890],\n",
      "        [-1.2766],\n",
      "        [-0.2683],\n",
      "        [ 0.4907],\n",
      "        [ 0.4904],\n",
      "        [ 0.4905],\n",
      "        [ 0.4909],\n",
      "        [ 0.4909],\n",
      "        [ 0.4908],\n",
      "        [-0.2683],\n",
      "        [-1.4239],\n",
      "        [ 0.4908],\n",
      "        [-1.4256],\n",
      "        [ 0.4908],\n",
      "        [-1.4250],\n",
      "        [ 0.4890],\n",
      "        [ 0.0281],\n",
      "        [ 0.4904],\n",
      "        [ 0.4885],\n",
      "        [ 0.4909],\n",
      "        [ 0.4904],\n",
      "        [-1.3504],\n",
      "        [ 0.2345],\n",
      "        [-1.4204],\n",
      "        [-1.4226],\n",
      "        [ 0.4908],\n",
      "        [-1.4076],\n",
      "        [ 0.4897],\n",
      "        [ 0.4909],\n",
      "        [-1.4259],\n",
      "        [ 0.4236],\n",
      "        [-0.7212],\n",
      "        [-1.4250],\n",
      "        [ 0.4908],\n",
      "        [-0.7212],\n",
      "        [-1.1951],\n",
      "        [-1.4258],\n",
      "        [ 0.4897],\n",
      "        [-1.4256],\n",
      "        [ 0.4860],\n",
      "        [-0.8248],\n",
      "        [ 0.4860],\n",
      "        [ 0.4909],\n",
      "        [ 0.4909]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 24/10000,\n",
      " train_loss: 0.1495,\n",
      " train_mae: 0.2689,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.0779],\n",
      "        [-1.0779],\n",
      "        [ 0.6066],\n",
      "        [ 0.6061],\n",
      "        [ 0.4670],\n",
      "        [ 0.6045],\n",
      "        [ 0.6074],\n",
      "        [ 0.6075],\n",
      "        [ 0.6074],\n",
      "        [ 0.6073],\n",
      "        [-1.0796],\n",
      "        [-1.0678],\n",
      "        [ 0.6057],\n",
      "        [ 0.1297],\n",
      "        [ 0.6074],\n",
      "        [ 0.1297],\n",
      "        [ 0.6057],\n",
      "        [-0.9779],\n",
      "        [-0.1547],\n",
      "        [ 0.6074],\n",
      "        [ 0.6071],\n",
      "        [ 0.6072],\n",
      "        [ 0.6075],\n",
      "        [ 0.6075],\n",
      "        [ 0.6075],\n",
      "        [-0.1547],\n",
      "        [-1.0783],\n",
      "        [ 0.6075],\n",
      "        [-1.0794],\n",
      "        [ 0.6075],\n",
      "        [-1.0790],\n",
      "        [ 0.6057],\n",
      "        [ 0.1297],\n",
      "        [ 0.6071],\n",
      "        [ 0.6052],\n",
      "        [ 0.6075],\n",
      "        [ 0.6071],\n",
      "        [-1.0293],\n",
      "        [ 0.3388],\n",
      "        [-1.0761],\n",
      "        [-1.0775],\n",
      "        [ 0.6075],\n",
      "        [-1.0678],\n",
      "        [ 0.6064],\n",
      "        [ 0.6075],\n",
      "        [-1.0796],\n",
      "        [ 0.5371],\n",
      "        [-0.5517],\n",
      "        [-1.0790],\n",
      "        [ 0.6075],\n",
      "        [-0.5517],\n",
      "        [-0.9197],\n",
      "        [-1.0795],\n",
      "        [ 0.6064],\n",
      "        [-1.0794],\n",
      "        [ 0.6026],\n",
      "        [-0.6363],\n",
      "        [ 0.6026],\n",
      "        [ 0.6075],\n",
      "        [ 0.6075]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 25/10000,\n",
      " train_loss: 0.1072,\n",
      " train_mae: 0.3033,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-0.7727],\n",
      "        [-0.7727],\n",
      "        [ 0.7278],\n",
      "        [ 0.7272],\n",
      "        [ 0.5802],\n",
      "        [ 0.7256],\n",
      "        [ 0.7286],\n",
      "        [ 0.7286],\n",
      "        [ 0.7286],\n",
      "        [ 0.7284],\n",
      "        [-0.7737],\n",
      "        [-0.7659],\n",
      "        [ 0.7268],\n",
      "        [ 0.2364],\n",
      "        [ 0.7286],\n",
      "        [ 0.2364],\n",
      "        [ 0.7268],\n",
      "        [-0.7032],\n",
      "        [-0.0327],\n",
      "        [ 0.7285],\n",
      "        [ 0.7283],\n",
      "        [ 0.7284],\n",
      "        [ 0.7287],\n",
      "        [ 0.7287],\n",
      "        [ 0.7287],\n",
      "        [-0.0327],\n",
      "        [-0.7729],\n",
      "        [ 0.7287],\n",
      "        [-0.7736],\n",
      "        [ 0.7286],\n",
      "        [-0.7734],\n",
      "        [ 0.7268],\n",
      "        [ 0.2364],\n",
      "        [ 0.7283],\n",
      "        [ 0.7263],\n",
      "        [ 0.7287],\n",
      "        [ 0.7283],\n",
      "        [-0.7395],\n",
      "        [ 0.4466],\n",
      "        [-0.7715],\n",
      "        [-0.7724],\n",
      "        [ 0.7286],\n",
      "        [-0.7659],\n",
      "        [ 0.7276],\n",
      "        [ 0.7287],\n",
      "        [-0.7737],\n",
      "        [ 0.6544],\n",
      "        [-0.3760],\n",
      "        [-0.7734],\n",
      "        [ 0.7286],\n",
      "        [-0.3760],\n",
      "        [-0.6612],\n",
      "        [-0.7737],\n",
      "        [ 0.7276],\n",
      "        [-0.7736],\n",
      "        [ 0.7236],\n",
      "        [-0.4444],\n",
      "        [ 0.7236],\n",
      "        [ 0.7287],\n",
      "        [ 0.7287]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 26/10000,\n",
      " train_loss: 0.1402,\n",
      " train_mae: 0.3460,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-6.2111e-01],\n",
      "        [-6.2111e-01],\n",
      "        [ 8.0716e-01],\n",
      "        [ 8.0653e-01],\n",
      "        [ 6.3892e-01],\n",
      "        [ 8.0473e-01],\n",
      "        [ 8.0800e-01],\n",
      "        [ 8.0805e-01],\n",
      "        [ 8.0800e-01],\n",
      "        [ 8.0785e-01],\n",
      "        [-6.2182e-01],\n",
      "        [-6.1632e-01],\n",
      "        [ 8.0608e-01],\n",
      "        [ 2.6691e-01],\n",
      "        [ 8.0803e-01],\n",
      "        [ 2.6691e-01],\n",
      "        [ 8.0608e-01],\n",
      "        [-5.7023e-01],\n",
      "        [ 6.5229e-04],\n",
      "        [ 8.0796e-01],\n",
      "        [ 8.0767e-01],\n",
      "        [ 8.0777e-01],\n",
      "        [ 8.0812e-01],\n",
      "        [ 8.0812e-01],\n",
      "        [ 8.0810e-01],\n",
      "        [ 6.5229e-04],\n",
      "        [-6.2127e-01],\n",
      "        [ 8.0810e-01],\n",
      "        [-6.2175e-01],\n",
      "        [ 8.0805e-01],\n",
      "        [-6.2157e-01],\n",
      "        [ 8.0608e-01],\n",
      "        [ 2.6691e-01],\n",
      "        [ 8.0767e-01],\n",
      "        [ 8.0549e-01],\n",
      "        [ 8.0812e-01],\n",
      "        [ 8.0767e-01],\n",
      "        [-5.9714e-01],\n",
      "        [ 4.9021e-01],\n",
      "        [-6.2027e-01],\n",
      "        [-6.2090e-01],\n",
      "        [ 8.0805e-01],\n",
      "        [-6.1632e-01],\n",
      "        [ 8.0689e-01],\n",
      "        [ 8.0812e-01],\n",
      "        [-6.2183e-01],\n",
      "        [ 7.2337e-01],\n",
      "        [-3.0792e-01],\n",
      "        [-6.2157e-01],\n",
      "        [ 8.0808e-01],\n",
      "        [-3.0792e-01],\n",
      "        [-5.3837e-01],\n",
      "        [-6.2181e-01],\n",
      "        [ 8.0689e-01],\n",
      "        [-6.2175e-01],\n",
      "        [ 8.0252e-01],\n",
      "        [-3.6532e-01],\n",
      "        [ 8.0252e-01],\n",
      "        [ 8.0812e-01],\n",
      "        [ 8.0812e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 27/10000,\n",
      " train_loss: 0.1814,\n",
      " train_mae: 0.3188,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-0.6669],\n",
      "        [-0.6669],\n",
      "        [ 0.8232],\n",
      "        [ 0.8224],\n",
      "        [ 0.6103],\n",
      "        [ 0.8201],\n",
      "        [ 0.8242],\n",
      "        [ 0.8242],\n",
      "        [ 0.8242],\n",
      "        [ 0.8240],\n",
      "        [-0.6675],\n",
      "        [-0.6633],\n",
      "        [ 0.8218],\n",
      "        [ 0.1728],\n",
      "        [ 0.8242],\n",
      "        [ 0.1728],\n",
      "        [ 0.8218],\n",
      "        [-0.6269],\n",
      "        [-0.1084],\n",
      "        [ 0.8241],\n",
      "        [ 0.8238],\n",
      "        [ 0.8239],\n",
      "        [ 0.8243],\n",
      "        [ 0.8243],\n",
      "        [ 0.8243],\n",
      "        [-0.1084],\n",
      "        [-0.6670],\n",
      "        [ 0.8243],\n",
      "        [-0.6674],\n",
      "        [ 0.8242],\n",
      "        [-0.6673],\n",
      "        [ 0.8218],\n",
      "        [ 0.1728],\n",
      "        [ 0.8238],\n",
      "        [ 0.8211],\n",
      "        [ 0.8243],\n",
      "        [ 0.8238],\n",
      "        [-0.6483],\n",
      "        [ 0.4291],\n",
      "        [-0.6663],\n",
      "        [-0.6668],\n",
      "        [ 0.8242],\n",
      "        [-0.6633],\n",
      "        [ 0.8228],\n",
      "        [ 0.8243],\n",
      "        [-0.6675],\n",
      "        [ 0.7166],\n",
      "        [-0.4026],\n",
      "        [-0.6673],\n",
      "        [ 0.8243],\n",
      "        [-0.4026],\n",
      "        [-0.6010],\n",
      "        [-0.6674],\n",
      "        [ 0.8228],\n",
      "        [-0.6674],\n",
      "        [ 0.8174],\n",
      "        [-0.4538],\n",
      "        [ 0.8174],\n",
      "        [ 0.8243],\n",
      "        [ 0.8243]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 28/10000,\n",
      " train_loss: 0.1594,\n",
      " train_mae: 0.2695,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-0.8702],\n",
      "        [-0.8702],\n",
      "        [ 0.7865],\n",
      "        [ 0.7855],\n",
      "        [ 0.4984],\n",
      "        [ 0.7825],\n",
      "        [ 0.7879],\n",
      "        [ 0.7879],\n",
      "        [ 0.7879],\n",
      "        [ 0.7876],\n",
      "        [-0.8706],\n",
      "        [-0.8673],\n",
      "        [ 0.7847],\n",
      "        [-0.0404],\n",
      "        [ 0.7879],\n",
      "        [-0.0404],\n",
      "        [ 0.7847],\n",
      "        [-0.8369],\n",
      "        [-0.3469],\n",
      "        [ 0.7878],\n",
      "        [ 0.7873],\n",
      "        [ 0.7875],\n",
      "        [ 0.7880],\n",
      "        [ 0.7880],\n",
      "        [ 0.7880],\n",
      "        [-0.3469],\n",
      "        [-0.8703],\n",
      "        [ 0.7880],\n",
      "        [-0.8706],\n",
      "        [ 0.7879],\n",
      "        [-0.8705],\n",
      "        [ 0.7847],\n",
      "        [-0.0404],\n",
      "        [ 0.7873],\n",
      "        [ 0.7837],\n",
      "        [ 0.7880],\n",
      "        [ 0.7873],\n",
      "        [-0.8549],\n",
      "        [ 0.2659],\n",
      "        [-0.8697],\n",
      "        [-0.8701],\n",
      "        [ 0.7879],\n",
      "        [-0.8673],\n",
      "        [ 0.7861],\n",
      "        [ 0.7880],\n",
      "        [-0.8706],\n",
      "        [ 0.6408],\n",
      "        [-0.6368],\n",
      "        [-0.8705],\n",
      "        [ 0.7880],\n",
      "        [-0.6368],\n",
      "        [-0.8149],\n",
      "        [-0.8706],\n",
      "        [ 0.7861],\n",
      "        [-0.8706],\n",
      "        [ 0.7787],\n",
      "        [-0.6841],\n",
      "        [ 0.7787],\n",
      "        [ 0.7880],\n",
      "        [ 0.7880]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 29/10000,\n",
      " train_loss: 0.1046,\n",
      " train_mae: 0.2541,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1331],\n",
      "        [-1.1331],\n",
      "        [ 0.7322],\n",
      "        [ 0.7308],\n",
      "        [ 0.3432],\n",
      "        [ 0.7267],\n",
      "        [ 0.7340],\n",
      "        [ 0.7341],\n",
      "        [ 0.7340],\n",
      "        [ 0.7337],\n",
      "        [-1.1334],\n",
      "        [-1.1306],\n",
      "        [ 0.7298],\n",
      "        [-0.3108],\n",
      "        [ 0.7340],\n",
      "        [-0.3108],\n",
      "        [ 0.7298],\n",
      "        [-1.1047],\n",
      "        [-0.6397],\n",
      "        [ 0.7339],\n",
      "        [ 0.7333],\n",
      "        [ 0.7335],\n",
      "        [ 0.7342],\n",
      "        [ 0.7342],\n",
      "        [ 0.7342],\n",
      "        [-0.6397],\n",
      "        [-1.1331],\n",
      "        [ 0.7342],\n",
      "        [-1.1334],\n",
      "        [ 0.7341],\n",
      "        [-1.1333],\n",
      "        [ 0.7298],\n",
      "        [-0.3108],\n",
      "        [ 0.7333],\n",
      "        [ 0.7284],\n",
      "        [ 0.7342],\n",
      "        [ 0.7333],\n",
      "        [-1.1202],\n",
      "        [ 0.0486],\n",
      "        [-1.1326],\n",
      "        [-1.1329],\n",
      "        [ 0.7341],\n",
      "        [-1.1306],\n",
      "        [ 0.7316],\n",
      "        [ 0.7342],\n",
      "        [-1.1334],\n",
      "        [ 0.5328],\n",
      "        [-0.9239],\n",
      "        [-1.1333],\n",
      "        [ 0.7342],\n",
      "        [-0.9239],\n",
      "        [-1.0855],\n",
      "        [-1.1334],\n",
      "        [ 0.7316],\n",
      "        [-1.1334],\n",
      "        [ 0.7216],\n",
      "        [-0.9678],\n",
      "        [ 0.7216],\n",
      "        [ 0.7342],\n",
      "        [ 0.7342]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 30/10000,\n",
      " train_loss: 0.0926,\n",
      " train_mae: 0.2916,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.3370],\n",
      "        [-1.3370],\n",
      "        [ 0.7026],\n",
      "        [ 0.7008],\n",
      "        [ 0.2111],\n",
      "        [ 0.6956],\n",
      "        [ 0.7049],\n",
      "        [ 0.7050],\n",
      "        [ 0.7049],\n",
      "        [ 0.7045],\n",
      "        [-1.3373],\n",
      "        [-1.3349],\n",
      "        [ 0.6995],\n",
      "        [-0.5370],\n",
      "        [ 0.7049],\n",
      "        [-0.5370],\n",
      "        [ 0.6995],\n",
      "        [-1.3124],\n",
      "        [-0.8755],\n",
      "        [ 0.7048],\n",
      "        [ 0.7040],\n",
      "        [ 0.7043],\n",
      "        [ 0.7052],\n",
      "        [ 0.7052],\n",
      "        [ 0.7051],\n",
      "        [-0.8755],\n",
      "        [-1.3371],\n",
      "        [ 0.7051],\n",
      "        [-1.3372],\n",
      "        [ 0.7050],\n",
      "        [-1.3372],\n",
      "        [ 0.6995],\n",
      "        [-0.5370],\n",
      "        [ 0.7040],\n",
      "        [ 0.6978],\n",
      "        [ 0.7052],\n",
      "        [ 0.7040],\n",
      "        [-1.3259],\n",
      "        [-0.1385],\n",
      "        [-1.3366],\n",
      "        [-1.3369],\n",
      "        [ 0.7050],\n",
      "        [-1.3349],\n",
      "        [ 0.7018],\n",
      "        [ 0.7052],\n",
      "        [-1.3373],\n",
      "        [ 0.4470],\n",
      "        [-1.1486],\n",
      "        [-1.3372],\n",
      "        [ 0.7051],\n",
      "        [-1.1486],\n",
      "        [-1.2954],\n",
      "        [-1.3373],\n",
      "        [ 0.7018],\n",
      "        [-1.3372],\n",
      "        [ 0.6891],\n",
      "        [-1.1892],\n",
      "        [ 0.6891],\n",
      "        [ 0.7052],\n",
      "        [ 0.7052]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 31/10000,\n",
      " train_loss: 0.1282,\n",
      " train_mae: 0.3057,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.3947],\n",
      "        [-1.3947],\n",
      "        [ 0.7267],\n",
      "        [ 0.7246],\n",
      "        [ 0.1658],\n",
      "        [ 0.7186],\n",
      "        [ 0.7292],\n",
      "        [ 0.7294],\n",
      "        [ 0.7292],\n",
      "        [ 0.7288],\n",
      "        [-1.3950],\n",
      "        [-1.3929],\n",
      "        [ 0.7231],\n",
      "        [-0.6293],\n",
      "        [ 0.7293],\n",
      "        [-0.6293],\n",
      "        [ 0.7231],\n",
      "        [-1.3728],\n",
      "        [-0.9643],\n",
      "        [ 0.7291],\n",
      "        [ 0.7283],\n",
      "        [ 0.7286],\n",
      "        [ 0.7296],\n",
      "        [ 0.7296],\n",
      "        [ 0.7295],\n",
      "        [-0.9643],\n",
      "        [-1.3948],\n",
      "        [ 0.7295],\n",
      "        [-1.3950],\n",
      "        [ 0.7294],\n",
      "        [-1.3949],\n",
      "        [ 0.7231],\n",
      "        [-0.6293],\n",
      "        [ 0.7283],\n",
      "        [ 0.7212],\n",
      "        [ 0.7296],\n",
      "        [ 0.7283],\n",
      "        [-1.3849],\n",
      "        [-0.2151],\n",
      "        [-1.3945],\n",
      "        [-1.3947],\n",
      "        [ 0.7294],\n",
      "        [-1.3929],\n",
      "        [ 0.7258],\n",
      "        [ 0.7296],\n",
      "        [-1.3950],\n",
      "        [ 0.4318],\n",
      "        [-1.2232],\n",
      "        [-1.3949],\n",
      "        [ 0.7295],\n",
      "        [-1.2232],\n",
      "        [-1.3576],\n",
      "        [-1.3950],\n",
      "        [ 0.7258],\n",
      "        [-1.3950],\n",
      "        [ 0.7110],\n",
      "        [-1.2607],\n",
      "        [ 0.7110],\n",
      "        [ 0.7296],\n",
      "        [ 0.7296]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 32/10000,\n",
      " train_loss: 0.1445,\n",
      " train_mae: 0.2817,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2953],\n",
      "        [-1.2953],\n",
      "        [ 0.8047],\n",
      "        [ 0.8026],\n",
      "        [ 0.2254],\n",
      "        [ 0.7964],\n",
      "        [ 0.8073],\n",
      "        [ 0.8075],\n",
      "        [ 0.8073],\n",
      "        [ 0.8069],\n",
      "        [-1.2955],\n",
      "        [-1.2936],\n",
      "        [ 0.8010],\n",
      "        [-0.5683],\n",
      "        [ 0.8074],\n",
      "        [-0.5683],\n",
      "        [ 0.8010],\n",
      "        [-1.2752],\n",
      "        [-0.8915],\n",
      "        [ 0.8072],\n",
      "        [ 0.8063],\n",
      "        [ 0.8066],\n",
      "        [ 0.8077],\n",
      "        [ 0.8077],\n",
      "        [ 0.8076],\n",
      "        [-0.8915],\n",
      "        [-1.2953],\n",
      "        [ 0.8076],\n",
      "        [-1.2955],\n",
      "        [ 0.8075],\n",
      "        [-1.2954],\n",
      "        [ 0.8010],\n",
      "        [-0.5683],\n",
      "        [ 0.8063],\n",
      "        [ 0.7990],\n",
      "        [ 0.8077],\n",
      "        [ 0.8063],\n",
      "        [-1.2863],\n",
      "        [-0.1594],\n",
      "        [-1.2950],\n",
      "        [-1.2952],\n",
      "        [ 0.8075],\n",
      "        [-1.2936],\n",
      "        [ 0.8038],\n",
      "        [ 0.8077],\n",
      "        [-1.2955],\n",
      "        [ 0.4987],\n",
      "        [-1.1362],\n",
      "        [-1.2954],\n",
      "        [ 0.8076],\n",
      "        [-1.1362],\n",
      "        [-1.2612],\n",
      "        [-1.2955],\n",
      "        [ 0.8038],\n",
      "        [-1.2955],\n",
      "        [ 0.7885],\n",
      "        [-1.1713],\n",
      "        [ 0.7885],\n",
      "        [ 0.8077],\n",
      "        [ 0.8077]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 33/10000,\n",
      " train_loss: 0.1176,\n",
      " train_mae: 0.2666,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1020],\n",
      "        [-1.1020],\n",
      "        [ 0.9091],\n",
      "        [ 0.9071],\n",
      "        [ 0.3499],\n",
      "        [ 0.9011],\n",
      "        [ 0.9117],\n",
      "        [ 0.9118],\n",
      "        [ 0.9117],\n",
      "        [ 0.9112],\n",
      "        [-1.1022],\n",
      "        [-1.1005],\n",
      "        [ 0.9056],\n",
      "        [-0.4121],\n",
      "        [ 0.9117],\n",
      "        [-0.4121],\n",
      "        [ 0.9056],\n",
      "        [-1.0832],\n",
      "        [-0.7201],\n",
      "        [ 0.9116],\n",
      "        [ 0.9107],\n",
      "        [ 0.9110],\n",
      "        [ 0.9120],\n",
      "        [ 0.9120],\n",
      "        [ 0.9119],\n",
      "        [-0.7201],\n",
      "        [-1.1021],\n",
      "        [ 0.9119],\n",
      "        [-1.1022],\n",
      "        [ 0.9118],\n",
      "        [-1.1022],\n",
      "        [ 0.9056],\n",
      "        [-0.4121],\n",
      "        [ 0.9107],\n",
      "        [ 0.9037],\n",
      "        [ 0.9120],\n",
      "        [ 0.9107],\n",
      "        [-1.0937],\n",
      "        [-0.0203],\n",
      "        [-1.1018],\n",
      "        [-1.1020],\n",
      "        [ 0.9118],\n",
      "        [-1.1005],\n",
      "        [ 0.9083],\n",
      "        [ 0.9120],\n",
      "        [-1.1022],\n",
      "        [ 0.6137],\n",
      "        [-0.9521],\n",
      "        [-1.1022],\n",
      "        [ 0.9119],\n",
      "        [-0.9521],\n",
      "        [-1.0700],\n",
      "        [-1.1022],\n",
      "        [ 0.9083],\n",
      "        [-1.1022],\n",
      "        [ 0.8936],\n",
      "        [-0.9852],\n",
      "        [ 0.8936],\n",
      "        [ 0.9120],\n",
      "        [ 0.9120]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 34/10000,\n",
      " train_loss: 0.0974,\n",
      " train_mae: 0.2891,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-0.9162],\n",
      "        [-0.9162],\n",
      "        [ 0.9983],\n",
      "        [ 0.9964],\n",
      "        [ 0.4691],\n",
      "        [ 0.9907],\n",
      "        [ 1.0006],\n",
      "        [ 1.0008],\n",
      "        [ 1.0006],\n",
      "        [ 1.0003],\n",
      "        [-0.9164],\n",
      "        [-0.9148],\n",
      "        [ 0.9950],\n",
      "        [-0.2562],\n",
      "        [ 1.0007],\n",
      "        [-0.2562],\n",
      "        [ 0.9950],\n",
      "        [-0.8982],\n",
      "        [-0.5506],\n",
      "        [ 1.0005],\n",
      "        [ 0.9998],\n",
      "        [ 1.0000],\n",
      "        [ 1.0010],\n",
      "        [ 1.0010],\n",
      "        [ 1.0009],\n",
      "        [-0.5506],\n",
      "        [-0.9163],\n",
      "        [ 1.0009],\n",
      "        [-0.9164],\n",
      "        [ 1.0008],\n",
      "        [-0.9164],\n",
      "        [ 0.9950],\n",
      "        [-0.2562],\n",
      "        [ 0.9998],\n",
      "        [ 0.9931],\n",
      "        [ 1.0010],\n",
      "        [ 0.9998],\n",
      "        [-0.9082],\n",
      "        [ 0.1172],\n",
      "        [-0.9160],\n",
      "        [-0.9162],\n",
      "        [ 1.0008],\n",
      "        [-0.9148],\n",
      "        [ 0.9974],\n",
      "        [ 1.0010],\n",
      "        [-0.9164],\n",
      "        [ 0.7191],\n",
      "        [-0.7727],\n",
      "        [-0.9164],\n",
      "        [ 1.0009],\n",
      "        [-0.7727],\n",
      "        [-0.8856],\n",
      "        [-0.9164],\n",
      "        [ 0.9974],\n",
      "        [-0.9164],\n",
      "        [ 0.9836],\n",
      "        [-0.8044],\n",
      "        [ 0.9836],\n",
      "        [ 1.0010],\n",
      "        [ 1.0009]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 35/10000,\n",
      " train_loss: 0.1154,\n",
      " train_mae: 0.3078,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-0.8278],\n",
      "        [-0.8278],\n",
      "        [ 1.0361],\n",
      "        [ 1.0342],\n",
      "        [ 0.5225],\n",
      "        [ 1.0288],\n",
      "        [ 1.0384],\n",
      "        [ 1.0385],\n",
      "        [ 1.0384],\n",
      "        [ 1.0380],\n",
      "        [-0.8280],\n",
      "        [-0.8264],\n",
      "        [ 1.0328],\n",
      "        [-0.1834],\n",
      "        [ 1.0384],\n",
      "        [-0.1834],\n",
      "        [ 1.0328],\n",
      "        [-0.8102],\n",
      "        [-0.4705],\n",
      "        [ 1.0383],\n",
      "        [ 1.0375],\n",
      "        [ 1.0378],\n",
      "        [ 1.0387],\n",
      "        [ 1.0387],\n",
      "        [ 1.0386],\n",
      "        [-0.4705],\n",
      "        [-0.8278],\n",
      "        [ 1.0386],\n",
      "        [-0.8280],\n",
      "        [ 1.0385],\n",
      "        [-0.8279],\n",
      "        [ 1.0328],\n",
      "        [-0.1834],\n",
      "        [ 1.0375],\n",
      "        [ 1.0311],\n",
      "        [ 1.0387],\n",
      "        [ 1.0375],\n",
      "        [-0.8200],\n",
      "        [ 0.1804],\n",
      "        [-0.8276],\n",
      "        [-0.8277],\n",
      "        [ 1.0385],\n",
      "        [-0.8264],\n",
      "        [ 1.0353],\n",
      "        [ 1.0387],\n",
      "        [-0.8280],\n",
      "        [ 0.7652],\n",
      "        [-0.6874],\n",
      "        [-0.8279],\n",
      "        [ 1.0386],\n",
      "        [-0.6874],\n",
      "        [-0.7978],\n",
      "        [-0.8280],\n",
      "        [ 1.0353],\n",
      "        [-0.8280],\n",
      "        [ 1.0218],\n",
      "        [-0.7184],\n",
      "        [ 1.0218],\n",
      "        [ 1.0387],\n",
      "        [ 1.0387]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 36/10000,\n",
      " train_loss: 0.1358,\n",
      " train_mae: 0.2962,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-0.8711],\n",
      "        [-0.8711],\n",
      "        [ 1.0098],\n",
      "        [ 1.0079],\n",
      "        [ 0.4883],\n",
      "        [ 1.0024],\n",
      "        [ 1.0122],\n",
      "        [ 1.0123],\n",
      "        [ 1.0122],\n",
      "        [ 1.0118],\n",
      "        [-0.8712],\n",
      "        [-0.8696],\n",
      "        [ 1.0066],\n",
      "        [-0.2240],\n",
      "        [ 1.0123],\n",
      "        [-0.2240],\n",
      "        [ 1.0066],\n",
      "        [-0.8534],\n",
      "        [-0.5126],\n",
      "        [ 1.0121],\n",
      "        [ 1.0113],\n",
      "        [ 1.0116],\n",
      "        [ 1.0125],\n",
      "        [ 1.0125],\n",
      "        [ 1.0124],\n",
      "        [-0.5126],\n",
      "        [-0.8711],\n",
      "        [ 1.0124],\n",
      "        [-0.8712],\n",
      "        [ 1.0123],\n",
      "        [-0.8712],\n",
      "        [ 1.0066],\n",
      "        [-0.2240],\n",
      "        [ 1.0113],\n",
      "        [ 1.0047],\n",
      "        [ 1.0125],\n",
      "        [ 1.0113],\n",
      "        [-0.8632],\n",
      "        [ 0.1425],\n",
      "        [-0.8708],\n",
      "        [-0.8710],\n",
      "        [ 1.0123],\n",
      "        [-0.8696],\n",
      "        [ 1.0090],\n",
      "        [ 1.0125],\n",
      "        [-0.8712],\n",
      "        [ 0.7344],\n",
      "        [-0.7303],\n",
      "        [-0.8712],\n",
      "        [ 1.0124],\n",
      "        [-0.7303],\n",
      "        [-0.8410],\n",
      "        [-0.8712],\n",
      "        [ 1.0090],\n",
      "        [-0.8712],\n",
      "        [ 0.9953],\n",
      "        [-0.7614],\n",
      "        [ 0.9953],\n",
      "        [ 1.0125],\n",
      "        [ 1.0125]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 37/10000,\n",
      " train_loss: 0.1231,\n",
      " train_mae: 0.2718,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.0120],\n",
      "        [-1.0120],\n",
      "        [ 0.9349],\n",
      "        [ 0.9329],\n",
      "        [ 0.3890],\n",
      "        [ 0.9270],\n",
      "        [ 0.9374],\n",
      "        [ 0.9375],\n",
      "        [ 0.9374],\n",
      "        [ 0.9370],\n",
      "        [-1.0122],\n",
      "        [-1.0106],\n",
      "        [ 0.9314],\n",
      "        [-0.3483],\n",
      "        [ 0.9374],\n",
      "        [-0.3483],\n",
      "        [ 0.9314],\n",
      "        [-0.9939],\n",
      "        [-0.6449],\n",
      "        [ 0.9373],\n",
      "        [ 0.9364],\n",
      "        [ 0.9367],\n",
      "        [ 0.9377],\n",
      "        [ 0.9377],\n",
      "        [ 0.9376],\n",
      "        [-0.6449],\n",
      "        [-1.0121],\n",
      "        [ 0.9376],\n",
      "        [-1.0122],\n",
      "        [ 0.9375],\n",
      "        [-1.0122],\n",
      "        [ 0.9314],\n",
      "        [-0.3483],\n",
      "        [ 0.9364],\n",
      "        [ 0.9295],\n",
      "        [ 0.9377],\n",
      "        [ 0.9364],\n",
      "        [-1.0040],\n",
      "        [ 0.0300],\n",
      "        [-1.0118],\n",
      "        [-1.0120],\n",
      "        [ 0.9375],\n",
      "        [-1.0106],\n",
      "        [ 0.9340],\n",
      "        [ 0.9377],\n",
      "        [-1.0122],\n",
      "        [ 0.6458],\n",
      "        [-0.8679],\n",
      "        [-1.0122],\n",
      "        [ 0.9376],\n",
      "        [-0.8679],\n",
      "        [-0.9812],\n",
      "        [-1.0122],\n",
      "        [ 0.9340],\n",
      "        [-1.0122],\n",
      "        [ 0.9196],\n",
      "        [-0.8997],\n",
      "        [ 0.9196],\n",
      "        [ 0.9377],\n",
      "        [ 0.9377]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 38/10000,\n",
      " train_loss: 0.0992,\n",
      " train_mae: 0.2641,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1710],\n",
      "        [-1.1710],\n",
      "        [ 0.8453],\n",
      "        [ 0.8432],\n",
      "        [ 0.2781],\n",
      "        [ 0.8371],\n",
      "        [ 0.8479],\n",
      "        [ 0.8481],\n",
      "        [ 0.8479],\n",
      "        [ 0.8475],\n",
      "        [-1.1712],\n",
      "        [-1.1695],\n",
      "        [ 0.8417],\n",
      "        [-0.4845],\n",
      "        [ 0.8480],\n",
      "        [-0.4845],\n",
      "        [ 0.8417],\n",
      "        [-1.1522],\n",
      "        [-0.7909],\n",
      "        [ 0.8478],\n",
      "        [ 0.8469],\n",
      "        [ 0.8473],\n",
      "        [ 0.8483],\n",
      "        [ 0.8483],\n",
      "        [ 0.8482],\n",
      "        [-0.7909],\n",
      "        [-1.1711],\n",
      "        [ 0.8482],\n",
      "        [-1.1712],\n",
      "        [ 0.8481],\n",
      "        [-1.1712],\n",
      "        [ 0.8417],\n",
      "        [-0.4845],\n",
      "        [ 0.8469],\n",
      "        [ 0.8397],\n",
      "        [ 0.8483],\n",
      "        [ 0.8469],\n",
      "        [-1.1626],\n",
      "        [-0.0935],\n",
      "        [-1.1708],\n",
      "        [-1.1710],\n",
      "        [ 0.8481],\n",
      "        [-1.1695],\n",
      "        [ 0.8444],\n",
      "        [ 0.8483],\n",
      "        [-1.1713],\n",
      "        [ 0.5444],\n",
      "        [-1.0215],\n",
      "        [-1.1712],\n",
      "        [ 0.8482],\n",
      "        [-1.0215],\n",
      "        [-1.1390],\n",
      "        [-1.1712],\n",
      "        [ 0.8444],\n",
      "        [-1.1712],\n",
      "        [ 0.8293],\n",
      "        [-1.0545],\n",
      "        [ 0.8293],\n",
      "        [ 0.8483],\n",
      "        [ 0.8483]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 39/10000,\n",
      " train_loss: 0.0994,\n",
      " train_mae: 0.2772,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2640],\n",
      "        [-1.2640],\n",
      "        [ 0.7775],\n",
      "        [ 0.7754],\n",
      "        [ 0.2137],\n",
      "        [ 0.7692],\n",
      "        [ 0.7801],\n",
      "        [ 0.7802],\n",
      "        [ 0.7801],\n",
      "        [ 0.7796],\n",
      "        [-1.2642],\n",
      "        [-1.2623],\n",
      "        [ 0.7738],\n",
      "        [-0.5549],\n",
      "        [ 0.7802],\n",
      "        [-0.5549],\n",
      "        [ 0.7738],\n",
      "        [-1.2441],\n",
      "        [-0.8688],\n",
      "        [ 0.7800],\n",
      "        [ 0.7791],\n",
      "        [ 0.7794],\n",
      "        [ 0.7804],\n",
      "        [ 0.7804],\n",
      "        [ 0.7804],\n",
      "        [-0.8688],\n",
      "        [-1.2640],\n",
      "        [ 0.7804],\n",
      "        [-1.2642],\n",
      "        [ 0.7802],\n",
      "        [-1.2641],\n",
      "        [ 0.7738],\n",
      "        [-0.5549],\n",
      "        [ 0.7791],\n",
      "        [ 0.7718],\n",
      "        [ 0.7804],\n",
      "        [ 0.7791],\n",
      "        [-1.2551],\n",
      "        [-0.1588],\n",
      "        [-1.2637],\n",
      "        [-1.2639],\n",
      "        [ 0.7802],\n",
      "        [-1.2623],\n",
      "        [ 0.7766],\n",
      "        [ 0.7804],\n",
      "        [-1.2642],\n",
      "        [ 0.4788],\n",
      "        [-1.1075],\n",
      "        [-1.2641],\n",
      "        [ 0.7803],\n",
      "        [-1.1075],\n",
      "        [-1.2302],\n",
      "        [-1.2642],\n",
      "        [ 0.7766],\n",
      "        [-1.2642],\n",
      "        [ 0.7615],\n",
      "        [-1.1418],\n",
      "        [ 0.7615],\n",
      "        [ 0.7804],\n",
      "        [ 0.7804]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 40/10000,\n",
      " train_loss: 0.1130,\n",
      " train_mae: 0.2729,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2462],\n",
      "        [-1.2462],\n",
      "        [ 0.7526],\n",
      "        [ 0.7507],\n",
      "        [ 0.2266],\n",
      "        [ 0.7450],\n",
      "        [ 0.7551],\n",
      "        [ 0.7552],\n",
      "        [ 0.7551],\n",
      "        [ 0.7547],\n",
      "        [-1.2465],\n",
      "        [-1.2444],\n",
      "        [ 0.7492],\n",
      "        [-0.5184],\n",
      "        [ 0.7552],\n",
      "        [-0.5184],\n",
      "        [ 0.7492],\n",
      "        [-1.2249],\n",
      "        [-0.8348],\n",
      "        [ 0.7550],\n",
      "        [ 0.7542],\n",
      "        [ 0.7545],\n",
      "        [ 0.7554],\n",
      "        [ 0.7554],\n",
      "        [ 0.7554],\n",
      "        [-0.8348],\n",
      "        [-1.2463],\n",
      "        [ 0.7554],\n",
      "        [-1.2464],\n",
      "        [ 0.7552],\n",
      "        [-1.2464],\n",
      "        [ 0.7492],\n",
      "        [-0.5184],\n",
      "        [ 0.7542],\n",
      "        [ 0.7474],\n",
      "        [ 0.7554],\n",
      "        [ 0.7542],\n",
      "        [-1.2366],\n",
      "        [-0.1296],\n",
      "        [-1.2459],\n",
      "        [-1.2461],\n",
      "        [ 0.7552],\n",
      "        [-1.2444],\n",
      "        [ 0.7518],\n",
      "        [ 0.7554],\n",
      "        [-1.2465],\n",
      "        [ 0.4755],\n",
      "        [-1.0811],\n",
      "        [-1.2464],\n",
      "        [ 0.7553],\n",
      "        [-1.0811],\n",
      "        [-1.2101],\n",
      "        [-1.2465],\n",
      "        [ 0.7518],\n",
      "        [-1.2464],\n",
      "        [ 0.7377],\n",
      "        [-1.1170],\n",
      "        [ 0.7377],\n",
      "        [ 0.7554],\n",
      "        [ 0.7554]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 41/10000,\n",
      " train_loss: 0.1097,\n",
      " train_mae: 0.2578,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1334],\n",
      "        [-1.1334],\n",
      "        [ 0.7682],\n",
      "        [ 0.7665],\n",
      "        [ 0.3040],\n",
      "        [ 0.7614],\n",
      "        [ 0.7704],\n",
      "        [ 0.7705],\n",
      "        [ 0.7704],\n",
      "        [ 0.7700],\n",
      "        [-1.1337],\n",
      "        [-1.1315],\n",
      "        [ 0.7652],\n",
      "        [-0.3921],\n",
      "        [ 0.7704],\n",
      "        [-0.3921],\n",
      "        [ 0.7652],\n",
      "        [-1.1103],\n",
      "        [-0.7054],\n",
      "        [ 0.7703],\n",
      "        [ 0.7695],\n",
      "        [ 0.7698],\n",
      "        [ 0.7706],\n",
      "        [ 0.7706],\n",
      "        [ 0.7706],\n",
      "        [-0.7054],\n",
      "        [-1.1335],\n",
      "        [ 0.7706],\n",
      "        [-1.1337],\n",
      "        [ 0.7705],\n",
      "        [-1.1336],\n",
      "        [ 0.7652],\n",
      "        [-0.3921],\n",
      "        [ 0.7695],\n",
      "        [ 0.7636],\n",
      "        [ 0.7706],\n",
      "        [ 0.7695],\n",
      "        [-1.1230],\n",
      "        [-0.0223],\n",
      "        [-1.1331],\n",
      "        [-1.1333],\n",
      "        [ 0.7705],\n",
      "        [-1.1315],\n",
      "        [ 0.7674],\n",
      "        [ 0.7706],\n",
      "        [-1.1337],\n",
      "        [ 0.5256],\n",
      "        [-0.9581],\n",
      "        [-1.1336],\n",
      "        [ 0.7706],\n",
      "        [-0.9581],\n",
      "        [-1.0945],\n",
      "        [-1.1337],\n",
      "        [ 0.7674],\n",
      "        [-1.1337],\n",
      "        [ 0.7551],\n",
      "        [-0.9957],\n",
      "        [ 0.7551],\n",
      "        [ 0.7706],\n",
      "        [ 0.7706]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 42/10000,\n",
      " train_loss: 0.0934,\n",
      " train_mae: 0.2562,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-0.9885],\n",
      "        [-0.9885],\n",
      "        [ 0.8019],\n",
      "        [ 0.8005],\n",
      "        [ 0.4022],\n",
      "        [ 0.7962],\n",
      "        [ 0.8038],\n",
      "        [ 0.8039],\n",
      "        [ 0.8038],\n",
      "        [ 0.8035],\n",
      "        [-0.9888],\n",
      "        [-0.9863],\n",
      "        [ 0.7994],\n",
      "        [-0.2360],\n",
      "        [ 0.8039],\n",
      "        [-0.2360],\n",
      "        [ 0.7994],\n",
      "        [-0.9632],\n",
      "        [-0.5429],\n",
      "        [ 0.8038],\n",
      "        [ 0.8031],\n",
      "        [ 0.8033],\n",
      "        [ 0.8041],\n",
      "        [ 0.8041],\n",
      "        [ 0.8040],\n",
      "        [-0.5429],\n",
      "        [-0.9885],\n",
      "        [ 0.8040],\n",
      "        [-0.9887],\n",
      "        [ 0.8039],\n",
      "        [-0.9886],\n",
      "        [ 0.7994],\n",
      "        [-0.2360],\n",
      "        [ 0.8031],\n",
      "        [ 0.7980],\n",
      "        [ 0.8041],\n",
      "        [ 0.8031],\n",
      "        [-0.9770],\n",
      "        [ 0.1099],\n",
      "        [-0.9881],\n",
      "        [-0.9884],\n",
      "        [ 0.8039],\n",
      "        [-0.9863],\n",
      "        [ 0.8013],\n",
      "        [ 0.8041],\n",
      "        [-0.9888],\n",
      "        [ 0.5949],\n",
      "        [-0.8015],\n",
      "        [-0.9886],\n",
      "        [ 0.8040],\n",
      "        [-0.8015],\n",
      "        [-0.9461],\n",
      "        [-0.9887],\n",
      "        [ 0.8013],\n",
      "        [-0.9887],\n",
      "        [ 0.7908],\n",
      "        [-0.8410],\n",
      "        [ 0.7908],\n",
      "        [ 0.8041],\n",
      "        [ 0.8041]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 43/10000,\n",
      " train_loss: 0.0904,\n",
      " train_mae: 0.2660,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-0.8867],\n",
      "        [-0.8867],\n",
      "        [ 0.8259],\n",
      "        [ 0.8246],\n",
      "        [ 0.4756],\n",
      "        [ 0.8208],\n",
      "        [ 0.8276],\n",
      "        [ 0.8277],\n",
      "        [ 0.8276],\n",
      "        [ 0.8273],\n",
      "        [-0.8870],\n",
      "        [-0.8843],\n",
      "        [ 0.8237],\n",
      "        [-0.1158],\n",
      "        [ 0.8277],\n",
      "        [-0.1158],\n",
      "        [ 0.8237],\n",
      "        [-0.8588],\n",
      "        [-0.4187],\n",
      "        [ 0.8275],\n",
      "        [ 0.8270],\n",
      "        [ 0.8272],\n",
      "        [ 0.8278],\n",
      "        [ 0.8278],\n",
      "        [ 0.8278],\n",
      "        [-0.4187],\n",
      "        [-0.8868],\n",
      "        [ 0.8278],\n",
      "        [-0.8870],\n",
      "        [ 0.8277],\n",
      "        [-0.8869],\n",
      "        [ 0.8237],\n",
      "        [-0.1158],\n",
      "        [ 0.8270],\n",
      "        [ 0.8224],\n",
      "        [ 0.8278],\n",
      "        [ 0.8270],\n",
      "        [-0.8739],\n",
      "        [ 0.2106],\n",
      "        [-0.8863],\n",
      "        [-0.8866],\n",
      "        [ 0.8277],\n",
      "        [-0.8843],\n",
      "        [ 0.8254],\n",
      "        [ 0.8278],\n",
      "        [-0.8870],\n",
      "        [ 0.6457],\n",
      "        [-0.6853],\n",
      "        [-0.8869],\n",
      "        [ 0.8278],\n",
      "        [-0.6853],\n",
      "        [-0.8401],\n",
      "        [-0.8870],\n",
      "        [ 0.8254],\n",
      "        [-0.8870],\n",
      "        [ 0.8161],\n",
      "        [-0.7271],\n",
      "        [ 0.8161],\n",
      "        [ 0.8278],\n",
      "        [ 0.8278]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 44/10000,\n",
      " train_loss: 0.1007,\n",
      " train_mae: 0.2684,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-0.8770],\n",
      "        [-0.8770],\n",
      "        [ 0.8216],\n",
      "        [ 0.8204],\n",
      "        [ 0.4989],\n",
      "        [ 0.8169],\n",
      "        [ 0.8232],\n",
      "        [ 0.8232],\n",
      "        [ 0.8232],\n",
      "        [ 0.8229],\n",
      "        [-0.8774],\n",
      "        [-0.8742],\n",
      "        [ 0.8195],\n",
      "        [-0.0692],\n",
      "        [ 0.8232],\n",
      "        [-0.0692],\n",
      "        [ 0.8195],\n",
      "        [-0.8456],\n",
      "        [-0.3757],\n",
      "        [ 0.8231],\n",
      "        [ 0.8226],\n",
      "        [ 0.8227],\n",
      "        [ 0.8234],\n",
      "        [ 0.8234],\n",
      "        [ 0.8233],\n",
      "        [-0.3757],\n",
      "        [-0.8771],\n",
      "        [ 0.8233],\n",
      "        [-0.8773],\n",
      "        [ 0.8232],\n",
      "        [-0.8772],\n",
      "        [ 0.8195],\n",
      "        [-0.0692],\n",
      "        [ 0.8226],\n",
      "        [ 0.8184],\n",
      "        [ 0.8234],\n",
      "        [ 0.8226],\n",
      "        [-0.8625],\n",
      "        [ 0.2488],\n",
      "        [-0.8765],\n",
      "        [-0.8768],\n",
      "        [ 0.8232],\n",
      "        [-0.8742],\n",
      "        [ 0.8211],\n",
      "        [ 0.8234],\n",
      "        [-0.8774],\n",
      "        [ 0.6563],\n",
      "        [-0.6563],\n",
      "        [-0.8772],\n",
      "        [ 0.8233],\n",
      "        [-0.6563],\n",
      "        [-0.8248],\n",
      "        [-0.8774],\n",
      "        [ 0.8211],\n",
      "        [-0.8773],\n",
      "        [ 0.8125],\n",
      "        [-0.7013],\n",
      "        [ 0.8125],\n",
      "        [ 0.8234],\n",
      "        [ 0.8234]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 45/10000,\n",
      " train_loss: 0.1026,\n",
      " train_mae: 0.2570,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-0.9588],\n",
      "        [-0.9588],\n",
      "        [ 0.7890],\n",
      "        [ 0.7878],\n",
      "        [ 0.4757],\n",
      "        [ 0.7844],\n",
      "        [ 0.7906],\n",
      "        [ 0.7907],\n",
      "        [ 0.7906],\n",
      "        [ 0.7903],\n",
      "        [-0.9592],\n",
      "        [-0.9556],\n",
      "        [ 0.7870],\n",
      "        [-0.0924],\n",
      "        [ 0.7906],\n",
      "        [-0.0924],\n",
      "        [ 0.7870],\n",
      "        [-0.9230],\n",
      "        [-0.4115],\n",
      "        [ 0.7905],\n",
      "        [ 0.7900],\n",
      "        [ 0.7902],\n",
      "        [ 0.7908],\n",
      "        [ 0.7908],\n",
      "        [ 0.7908],\n",
      "        [-0.4115],\n",
      "        [-0.9589],\n",
      "        [ 0.7908],\n",
      "        [-0.9592],\n",
      "        [ 0.7907],\n",
      "        [-0.9591],\n",
      "        [ 0.7870],\n",
      "        [-0.0924],\n",
      "        [ 0.7900],\n",
      "        [ 0.7859],\n",
      "        [ 0.7908],\n",
      "        [ 0.7900],\n",
      "        [-0.9422],\n",
      "        [ 0.2288],\n",
      "        [-0.9582],\n",
      "        [-0.9586],\n",
      "        [ 0.7907],\n",
      "        [-0.9556],\n",
      "        [ 0.7885],\n",
      "        [ 0.7908],\n",
      "        [-0.9593],\n",
      "        [ 0.6289],\n",
      "        [-0.7130],\n",
      "        [-0.9591],\n",
      "        [ 0.7907],\n",
      "        [-0.7130],\n",
      "        [-0.8996],\n",
      "        [-0.9592],\n",
      "        [ 0.7885],\n",
      "        [-0.9592],\n",
      "        [ 0.7802],\n",
      "        [-0.7623],\n",
      "        [ 0.7802],\n",
      "        [ 0.7908],\n",
      "        [ 0.7908]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 46/10000,\n",
      " train_loss: 0.0923,\n",
      " train_mae: 0.2464,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.0877],\n",
      "        [-1.0877],\n",
      "        [ 0.7456],\n",
      "        [ 0.7443],\n",
      "        [ 0.4322],\n",
      "        [ 0.7409],\n",
      "        [ 0.7471],\n",
      "        [ 0.7472],\n",
      "        [ 0.7471],\n",
      "        [ 0.7469],\n",
      "        [-1.0882],\n",
      "        [-1.0839],\n",
      "        [ 0.7435],\n",
      "        [-0.1482],\n",
      "        [ 0.7472],\n",
      "        [-0.1482],\n",
      "        [ 0.7435],\n",
      "        [-1.0466],\n",
      "        [-0.4849],\n",
      "        [ 0.7471],\n",
      "        [ 0.7465],\n",
      "        [ 0.7467],\n",
      "        [ 0.7474],\n",
      "        [ 0.7474],\n",
      "        [ 0.7473],\n",
      "        [-0.4849],\n",
      "        [-1.0878],\n",
      "        [ 0.7473],\n",
      "        [-1.0882],\n",
      "        [ 0.7472],\n",
      "        [-1.0880],\n",
      "        [ 0.7435],\n",
      "        [-0.1482],\n",
      "        [ 0.7465],\n",
      "        [ 0.7423],\n",
      "        [ 0.7474],\n",
      "        [ 0.7465],\n",
      "        [-1.0686],\n",
      "        [ 0.1826],\n",
      "        [-1.0870],\n",
      "        [-1.0875],\n",
      "        [ 0.7472],\n",
      "        [-1.0839],\n",
      "        [ 0.7450],\n",
      "        [ 0.7474],\n",
      "        [-1.0882],\n",
      "        [ 0.5855],\n",
      "        [-0.8120],\n",
      "        [-1.0880],\n",
      "        [ 0.7473],\n",
      "        [-0.8120],\n",
      "        [-1.0201],\n",
      "        [-1.0882],\n",
      "        [ 0.7450],\n",
      "        [-1.0882],\n",
      "        [ 0.7366],\n",
      "        [-0.8665],\n",
      "        [ 0.7366],\n",
      "        [ 0.7474],\n",
      "        [ 0.7474]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 47/10000,\n",
      " train_loss: 0.0866,\n",
      " train_mae: 0.2467,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2010],\n",
      "        [-1.2010],\n",
      "        [ 0.7153],\n",
      "        [ 0.7141],\n",
      "        [ 0.4033],\n",
      "        [ 0.7106],\n",
      "        [ 0.7169],\n",
      "        [ 0.7170],\n",
      "        [ 0.7169],\n",
      "        [ 0.7166],\n",
      "        [-1.2017],\n",
      "        [-1.1967],\n",
      "        [ 0.7132],\n",
      "        [-0.1863],\n",
      "        [ 0.7170],\n",
      "        [-0.1863],\n",
      "        [ 0.7132],\n",
      "        [-1.1540],\n",
      "        [-0.5395],\n",
      "        [ 0.7169],\n",
      "        [ 0.7163],\n",
      "        [ 0.7165],\n",
      "        [ 0.7172],\n",
      "        [ 0.7172],\n",
      "        [ 0.7171],\n",
      "        [-0.5395],\n",
      "        [-1.2012],\n",
      "        [ 0.7171],\n",
      "        [-1.2016],\n",
      "        [ 0.7170],\n",
      "        [-1.2014],\n",
      "        [ 0.7132],\n",
      "        [-0.1863],\n",
      "        [ 0.7163],\n",
      "        [ 0.7121],\n",
      "        [ 0.7172],\n",
      "        [ 0.7163],\n",
      "        [-1.1790],\n",
      "        [ 0.1523],\n",
      "        [-1.2002],\n",
      "        [-1.2008],\n",
      "        [ 0.7170],\n",
      "        [-1.1967],\n",
      "        [ 0.7148],\n",
      "        [ 0.7172],\n",
      "        [-1.2017],\n",
      "        [ 0.5560],\n",
      "        [-0.8927],\n",
      "        [-1.2014],\n",
      "        [ 0.7171],\n",
      "        [-0.8927],\n",
      "        [-1.1240],\n",
      "        [-1.2016],\n",
      "        [ 0.7148],\n",
      "        [-1.2016],\n",
      "        [ 0.7063],\n",
      "        [-0.9527],\n",
      "        [ 0.7063],\n",
      "        [ 0.7172],\n",
      "        [ 0.7172]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 48/10000,\n",
      " train_loss: 0.0912,\n",
      " train_mae: 0.2507,\n",
      " epoch_time_duration: 0.0090\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2507],\n",
      "        [-1.2507],\n",
      "        [ 0.7159],\n",
      "        [ 0.7147],\n",
      "        [ 0.4140],\n",
      "        [ 0.7113],\n",
      "        [ 0.7175],\n",
      "        [ 0.7176],\n",
      "        [ 0.7175],\n",
      "        [ 0.7172],\n",
      "        [-1.2515],\n",
      "        [-1.2456],\n",
      "        [ 0.7138],\n",
      "        [-0.1698],\n",
      "        [ 0.7176],\n",
      "        [-0.1698],\n",
      "        [ 0.7138],\n",
      "        [-1.1971],\n",
      "        [-0.5329],\n",
      "        [ 0.7175],\n",
      "        [ 0.7169],\n",
      "        [ 0.7171],\n",
      "        [ 0.7178],\n",
      "        [ 0.7178],\n",
      "        [ 0.7177],\n",
      "        [-0.5329],\n",
      "        [-1.2509],\n",
      "        [ 0.7177],\n",
      "        [-1.2514],\n",
      "        [ 0.7176],\n",
      "        [-1.2512],\n",
      "        [ 0.7138],\n",
      "        [-0.1698],\n",
      "        [ 0.7169],\n",
      "        [ 0.7127],\n",
      "        [ 0.7178],\n",
      "        [ 0.7169],\n",
      "        [-1.2255],\n",
      "        [ 0.1685],\n",
      "        [-1.2498],\n",
      "        [-1.2505],\n",
      "        [ 0.7176],\n",
      "        [-1.2456],\n",
      "        [ 0.7154],\n",
      "        [ 0.7178],\n",
      "        [-1.2515],\n",
      "        [ 0.5619],\n",
      "        [-0.9086],\n",
      "        [-1.2512],\n",
      "        [ 0.7177],\n",
      "        [-0.9086],\n",
      "        [-1.1633],\n",
      "        [-1.2514],\n",
      "        [ 0.7154],\n",
      "        [-1.2514],\n",
      "        [ 0.7071],\n",
      "        [-0.9739],\n",
      "        [ 0.7071],\n",
      "        [ 0.7178],\n",
      "        [ 0.7178]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 49/10000,\n",
      " train_loss: 0.0934,\n",
      " train_mae: 0.2465,\n",
      " epoch_time_duration: 0.0107\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2268],\n",
      "        [-1.2268],\n",
      "        [ 0.7490],\n",
      "        [ 0.7479],\n",
      "        [ 0.4664],\n",
      "        [ 0.7446],\n",
      "        [ 0.7506],\n",
      "        [ 0.7507],\n",
      "        [ 0.7506],\n",
      "        [ 0.7503],\n",
      "        [-1.2277],\n",
      "        [-1.2209],\n",
      "        [ 0.7470],\n",
      "        [-0.0945],\n",
      "        [ 0.7506],\n",
      "        [-0.0945],\n",
      "        [ 0.7470],\n",
      "        [-1.1660],\n",
      "        [-0.4586],\n",
      "        [ 0.7505],\n",
      "        [ 0.7500],\n",
      "        [ 0.7501],\n",
      "        [ 0.7508],\n",
      "        [ 0.7508],\n",
      "        [ 0.7508],\n",
      "        [-0.4586],\n",
      "        [-1.2270],\n",
      "        [ 0.7508],\n",
      "        [-1.2276],\n",
      "        [ 0.7507],\n",
      "        [-1.2274],\n",
      "        [ 0.7470],\n",
      "        [-0.0945],\n",
      "        [ 0.7500],\n",
      "        [ 0.7460],\n",
      "        [ 0.7508],\n",
      "        [ 0.7500],\n",
      "        [-1.1979],\n",
      "        [ 0.2336],\n",
      "        [-1.2258],\n",
      "        [-1.2265],\n",
      "        [ 0.7507],\n",
      "        [-1.2209],\n",
      "        [ 0.7485],\n",
      "        [ 0.7508],\n",
      "        [-1.2277],\n",
      "        [ 0.6048],\n",
      "        [-0.8509],\n",
      "        [-1.2274],\n",
      "        [ 0.7507],\n",
      "        [-0.8509],\n",
      "        [-1.1282],\n",
      "        [-1.2277],\n",
      "        [ 0.7485],\n",
      "        [-1.2276],\n",
      "        [ 0.7406],\n",
      "        [-0.9209],\n",
      "        [ 0.7406],\n",
      "        [ 0.7508],\n",
      "        [ 0.7508]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 50/10000,\n",
      " train_loss: 0.0873,\n",
      " train_mae: 0.2435,\n",
      " epoch_time_duration: 0.0109\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1588],\n",
      "        [-1.1588],\n",
      "        [ 0.7997],\n",
      "        [ 0.7987],\n",
      "        [ 0.5393],\n",
      "        [ 0.7956],\n",
      "        [ 0.8012],\n",
      "        [ 0.8013],\n",
      "        [ 0.8012],\n",
      "        [ 0.8010],\n",
      "        [-1.1599],\n",
      "        [-1.1521],\n",
      "        [ 0.7979],\n",
      "        [ 0.0101],\n",
      "        [ 0.8013],\n",
      "        [ 0.0101],\n",
      "        [ 0.7979],\n",
      "        [-1.0904],\n",
      "        [-0.3482],\n",
      "        [ 0.8011],\n",
      "        [ 0.8006],\n",
      "        [ 0.8008],\n",
      "        [ 0.8014],\n",
      "        [ 0.8014],\n",
      "        [ 0.8014],\n",
      "        [-0.3482],\n",
      "        [-1.1591],\n",
      "        [ 0.8014],\n",
      "        [-1.1598],\n",
      "        [ 0.8013],\n",
      "        [-1.1595],\n",
      "        [ 0.7979],\n",
      "        [ 0.0101],\n",
      "        [ 0.8006],\n",
      "        [ 0.7969],\n",
      "        [ 0.8014],\n",
      "        [ 0.8006],\n",
      "        [-1.1261],\n",
      "        [ 0.3226],\n",
      "        [-1.1576],\n",
      "        [-1.1585],\n",
      "        [ 0.8013],\n",
      "        [-1.1521],\n",
      "        [ 0.7993],\n",
      "        [ 0.8014],\n",
      "        [-1.1599],\n",
      "        [ 0.6669],\n",
      "        [-0.7508],\n",
      "        [-1.1595],\n",
      "        [ 0.8014],\n",
      "        [-0.7508],\n",
      "        [-1.0487],\n",
      "        [-1.1599],\n",
      "        [ 0.7993],\n",
      "        [-1.1598],\n",
      "        [ 0.7919],\n",
      "        [-0.8249],\n",
      "        [ 0.7919],\n",
      "        [ 0.8014],\n",
      "        [ 0.8014]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 51/10000,\n",
      " train_loss: 0.0831,\n",
      " train_mae: 0.2499,\n",
      " epoch_time_duration: 0.0088\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.0962],\n",
      "        [-1.0962],\n",
      "        [ 0.8451],\n",
      "        [ 0.8440],\n",
      "        [ 0.6022],\n",
      "        [ 0.8411],\n",
      "        [ 0.8465],\n",
      "        [ 0.8466],\n",
      "        [ 0.8465],\n",
      "        [ 0.8462],\n",
      "        [-1.0974],\n",
      "        [-1.0886],\n",
      "        [ 0.8433],\n",
      "        [ 0.1009],\n",
      "        [ 0.8466],\n",
      "        [ 0.1009],\n",
      "        [ 0.8433],\n",
      "        [-1.0204],\n",
      "        [-0.2502],\n",
      "        [ 0.8464],\n",
      "        [ 0.8459],\n",
      "        [ 0.8461],\n",
      "        [ 0.8467],\n",
      "        [ 0.8467],\n",
      "        [ 0.8467],\n",
      "        [-0.2502],\n",
      "        [-1.0965],\n",
      "        [ 0.8467],\n",
      "        [-1.0973],\n",
      "        [ 0.8466],\n",
      "        [-1.0970],\n",
      "        [ 0.8433],\n",
      "        [ 0.1009],\n",
      "        [ 0.8459],\n",
      "        [ 0.8423],\n",
      "        [ 0.8467],\n",
      "        [ 0.8459],\n",
      "        [-1.0597],\n",
      "        [ 0.3989],\n",
      "        [-1.0948],\n",
      "        [-1.0959],\n",
      "        [ 0.8466],\n",
      "        [-1.0886],\n",
      "        [ 0.8446],\n",
      "        [ 0.8467],\n",
      "        [-1.0974],\n",
      "        [ 0.7210],\n",
      "        [-0.6593],\n",
      "        [-1.0970],\n",
      "        [ 0.8467],\n",
      "        [-0.6593],\n",
      "        [-0.9750],\n",
      "        [-1.0974],\n",
      "        [ 0.8446],\n",
      "        [-1.0973],\n",
      "        [ 0.8376],\n",
      "        [-0.7366],\n",
      "        [ 0.8376],\n",
      "        [ 0.8467],\n",
      "        [ 0.8467]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 52/10000,\n",
      " train_loss: 0.0868,\n",
      " train_mae: 0.2545,\n",
      " epoch_time_duration: 0.0078\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.0799],\n",
      "        [-1.0799],\n",
      "        [ 0.8659],\n",
      "        [ 0.8648],\n",
      "        [ 0.6311],\n",
      "        [ 0.8619],\n",
      "        [ 0.8673],\n",
      "        [ 0.8674],\n",
      "        [ 0.8673],\n",
      "        [ 0.8670],\n",
      "        [-1.0813],\n",
      "        [-1.0715],\n",
      "        [ 0.8641],\n",
      "        [ 0.1445],\n",
      "        [ 0.8673],\n",
      "        [ 0.1445],\n",
      "        [ 0.8641],\n",
      "        [-0.9975],\n",
      "        [-0.2031],\n",
      "        [ 0.8672],\n",
      "        [ 0.8667],\n",
      "        [ 0.8669],\n",
      "        [ 0.8675],\n",
      "        [ 0.8675],\n",
      "        [ 0.8675],\n",
      "        [-0.2031],\n",
      "        [-1.0802],\n",
      "        [ 0.8675],\n",
      "        [-1.0811],\n",
      "        [ 0.8674],\n",
      "        [-1.0808],\n",
      "        [ 0.8641],\n",
      "        [ 0.1445],\n",
      "        [ 0.8667],\n",
      "        [ 0.8631],\n",
      "        [ 0.8675],\n",
      "        [ 0.8667],\n",
      "        [-1.0399],\n",
      "        [ 0.4348],\n",
      "        [-1.0784],\n",
      "        [-1.0795],\n",
      "        [ 0.8674],\n",
      "        [-1.0715],\n",
      "        [ 0.8654],\n",
      "        [ 0.8675],\n",
      "        [-1.0813],\n",
      "        [ 0.7455],\n",
      "        [-0.6183],\n",
      "        [-1.0808],\n",
      "        [ 0.8674],\n",
      "        [-0.6183],\n",
      "        [-0.9488],\n",
      "        [-1.0813],\n",
      "        [ 0.8654],\n",
      "        [-1.0811],\n",
      "        [ 0.8585],\n",
      "        [-0.6983],\n",
      "        [ 0.8585],\n",
      "        [ 0.8675],\n",
      "        [ 0.8675]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 53/10000,\n",
      " train_loss: 0.0901,\n",
      " train_mae: 0.2498,\n",
      " epoch_time_duration: 0.0079\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1211],\n",
      "        [-1.1211],\n",
      "        [ 0.8562],\n",
      "        [ 0.8551],\n",
      "        [ 0.6193],\n",
      "        [ 0.8521],\n",
      "        [ 0.8577],\n",
      "        [ 0.8578],\n",
      "        [ 0.8577],\n",
      "        [ 0.8574],\n",
      "        [-1.1226],\n",
      "        [-1.1120],\n",
      "        [ 0.8543],\n",
      "        [ 0.1323],\n",
      "        [ 0.8577],\n",
      "        [ 0.1323],\n",
      "        [ 0.8543],\n",
      "        [-1.0333],\n",
      "        [-0.2174],\n",
      "        [ 0.8576],\n",
      "        [ 0.8571],\n",
      "        [ 0.8573],\n",
      "        [ 0.8579],\n",
      "        [ 0.8579],\n",
      "        [ 0.8579],\n",
      "        [-0.2174],\n",
      "        [-1.1215],\n",
      "        [ 0.8579],\n",
      "        [-1.1225],\n",
      "        [ 0.8578],\n",
      "        [-1.1221],\n",
      "        [ 0.8543],\n",
      "        [ 0.1323],\n",
      "        [ 0.8571],\n",
      "        [ 0.8533],\n",
      "        [ 0.8579],\n",
      "        [ 0.8571],\n",
      "        [-1.0782],\n",
      "        [ 0.4230],\n",
      "        [-1.1195],\n",
      "        [-1.1207],\n",
      "        [ 0.8578],\n",
      "        [-1.1120],\n",
      "        [ 0.8557],\n",
      "        [ 0.8579],\n",
      "        [-1.1227],\n",
      "        [ 0.7341],\n",
      "        [-0.6399],\n",
      "        [-1.1221],\n",
      "        [ 0.8578],\n",
      "        [-0.6399],\n",
      "        [-0.9822],\n",
      "        [-1.1226],\n",
      "        [ 0.8557],\n",
      "        [-1.1225],\n",
      "        [ 0.8485],\n",
      "        [-0.7221],\n",
      "        [ 0.8485],\n",
      "        [ 0.8579],\n",
      "        [ 0.8579]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 54/10000,\n",
      " train_loss: 0.0871,\n",
      " train_mae: 0.2433,\n",
      " epoch_time_duration: 0.0075\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1973],\n",
      "        [-1.1973],\n",
      "        [ 0.8252],\n",
      "        [ 0.8241],\n",
      "        [ 0.5790],\n",
      "        [ 0.8209],\n",
      "        [ 0.8269],\n",
      "        [ 0.8270],\n",
      "        [ 0.8269],\n",
      "        [ 0.8266],\n",
      "        [-1.1990],\n",
      "        [-1.1876],\n",
      "        [ 0.8233],\n",
      "        [ 0.0815],\n",
      "        [ 0.8269],\n",
      "        [ 0.0815],\n",
      "        [ 0.8233],\n",
      "        [-1.1057],\n",
      "        [-0.2736],\n",
      "        [ 0.8268],\n",
      "        [ 0.8262],\n",
      "        [ 0.8264],\n",
      "        [ 0.8271],\n",
      "        [ 0.8271],\n",
      "        [ 0.8271],\n",
      "        [-0.2736],\n",
      "        [-1.1977],\n",
      "        [ 0.8271],\n",
      "        [-1.1988],\n",
      "        [ 0.8270],\n",
      "        [-1.1984],\n",
      "        [ 0.8233],\n",
      "        [ 0.0815],\n",
      "        [ 0.8262],\n",
      "        [ 0.8222],\n",
      "        [ 0.8271],\n",
      "        [ 0.8262],\n",
      "        [-1.1522],\n",
      "        [ 0.3777],\n",
      "        [-1.1955],\n",
      "        [-1.1969],\n",
      "        [ 0.8270],\n",
      "        [-1.1876],\n",
      "        [ 0.8247],\n",
      "        [ 0.8271],\n",
      "        [-1.1990],\n",
      "        [ 0.6976],\n",
      "        [-0.7028],\n",
      "        [-1.1984],\n",
      "        [ 0.8270],\n",
      "        [-0.7028],\n",
      "        [-1.0529],\n",
      "        [-1.1989],\n",
      "        [ 0.8247],\n",
      "        [-1.1988],\n",
      "        [ 0.8171],\n",
      "        [-0.7865],\n",
      "        [ 0.8171],\n",
      "        [ 0.8271],\n",
      "        [ 0.8271]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 55/10000,\n",
      " train_loss: 0.0835,\n",
      " train_mae: 0.2460,\n",
      " epoch_time_duration: 0.0075\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2666],\n",
      "        [-1.2666],\n",
      "        [ 0.7914],\n",
      "        [ 0.7901],\n",
      "        [ 0.5339],\n",
      "        [ 0.7867],\n",
      "        [ 0.7931],\n",
      "        [ 0.7932],\n",
      "        [ 0.7931],\n",
      "        [ 0.7928],\n",
      "        [-1.2683],\n",
      "        [-1.2565],\n",
      "        [ 0.7892],\n",
      "        [ 0.0250],\n",
      "        [ 0.7932],\n",
      "        [ 0.0250],\n",
      "        [ 0.7892],\n",
      "        [-1.1726],\n",
      "        [-0.3340],\n",
      "        [ 0.7930],\n",
      "        [ 0.7924],\n",
      "        [ 0.7926],\n",
      "        [ 0.7934],\n",
      "        [ 0.7934],\n",
      "        [ 0.7934],\n",
      "        [-0.3340],\n",
      "        [-1.2670],\n",
      "        [ 0.7934],\n",
      "        [-1.2681],\n",
      "        [ 0.7932],\n",
      "        [-1.2677],\n",
      "        [ 0.7892],\n",
      "        [ 0.0250],\n",
      "        [ 0.7924],\n",
      "        [ 0.7881],\n",
      "        [ 0.7934],\n",
      "        [ 0.7924],\n",
      "        [-1.2201],\n",
      "        [ 0.3268],\n",
      "        [-1.2647],\n",
      "        [-1.2661],\n",
      "        [ 0.7932],\n",
      "        [-1.2565],\n",
      "        [ 0.7908],\n",
      "        [ 0.7934],\n",
      "        [-1.2684],\n",
      "        [ 0.6569],\n",
      "        [-0.7659],\n",
      "        [-1.2677],\n",
      "        [ 0.7933],\n",
      "        [-0.7659],\n",
      "        [-1.1190],\n",
      "        [-1.2683],\n",
      "        [ 0.7908],\n",
      "        [-1.2681],\n",
      "        [ 0.7826],\n",
      "        [-0.8502],\n",
      "        [ 0.7826],\n",
      "        [ 0.7934],\n",
      "        [ 0.7934]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 56/10000,\n",
      " train_loss: 0.0848,\n",
      " train_mae: 0.2471,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2915],\n",
      "        [-1.2915],\n",
      "        [ 0.7716],\n",
      "        [ 0.7702],\n",
      "        [ 0.5064],\n",
      "        [ 0.7666],\n",
      "        [ 0.7735],\n",
      "        [ 0.7736],\n",
      "        [ 0.7735],\n",
      "        [ 0.7731],\n",
      "        [-1.2933],\n",
      "        [-1.2811],\n",
      "        [ 0.7693],\n",
      "        [-0.0057],\n",
      "        [ 0.7736],\n",
      "        [-0.0057],\n",
      "        [ 0.7693],\n",
      "        [-1.1962],\n",
      "        [-0.3630],\n",
      "        [ 0.7734],\n",
      "        [ 0.7727],\n",
      "        [ 0.7729],\n",
      "        [ 0.7738],\n",
      "        [ 0.7738],\n",
      "        [ 0.7737],\n",
      "        [-0.3630],\n",
      "        [-1.2919],\n",
      "        [ 0.7737],\n",
      "        [-1.2931],\n",
      "        [ 0.7736],\n",
      "        [-1.2926],\n",
      "        [ 0.7693],\n",
      "        [-0.0057],\n",
      "        [ 0.7727],\n",
      "        [ 0.7681],\n",
      "        [ 0.7738],\n",
      "        [ 0.7727],\n",
      "        [-1.2441],\n",
      "        [ 0.2969],\n",
      "        [-1.2895],\n",
      "        [-1.2910],\n",
      "        [ 0.7736],\n",
      "        [-1.2811],\n",
      "        [ 0.7710],\n",
      "        [ 0.7738],\n",
      "        [-1.2933],\n",
      "        [ 0.6321],\n",
      "        [-0.7913],\n",
      "        [-1.2926],\n",
      "        [ 0.7737],\n",
      "        [-0.7913],\n",
      "        [-1.1425],\n",
      "        [-1.2933],\n",
      "        [ 0.7710],\n",
      "        [-1.2931],\n",
      "        [ 0.7623],\n",
      "        [-0.8748],\n",
      "        [ 0.7623],\n",
      "        [ 0.7738],\n",
      "        [ 0.7738]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 57/10000,\n",
      " train_loss: 0.0863,\n",
      " train_mae: 0.2439,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2597],\n",
      "        [-1.2597],\n",
      "        [ 0.7727],\n",
      "        [ 0.7713],\n",
      "        [ 0.5058],\n",
      "        [ 0.7676],\n",
      "        [ 0.7747],\n",
      "        [ 0.7749],\n",
      "        [ 0.7747],\n",
      "        [ 0.7744],\n",
      "        [-1.2616],\n",
      "        [-1.2490],\n",
      "        [ 0.7704],\n",
      "        [ 0.0015],\n",
      "        [ 0.7748],\n",
      "        [ 0.0015],\n",
      "        [ 0.7704],\n",
      "        [-1.1639],\n",
      "        [-0.3474],\n",
      "        [ 0.7747],\n",
      "        [ 0.7739],\n",
      "        [ 0.7742],\n",
      "        [ 0.7751],\n",
      "        [ 0.7751],\n",
      "        [ 0.7750],\n",
      "        [-0.3474],\n",
      "        [-1.2601],\n",
      "        [ 0.7750],\n",
      "        [-1.2614],\n",
      "        [ 0.7749],\n",
      "        [-1.2609],\n",
      "        [ 0.7704],\n",
      "        [ 0.0015],\n",
      "        [ 0.7739],\n",
      "        [ 0.7691],\n",
      "        [ 0.7751],\n",
      "        [ 0.7739],\n",
      "        [-1.2117],\n",
      "        [ 0.2986],\n",
      "        [-1.2577],\n",
      "        [-1.2592],\n",
      "        [ 0.7749],\n",
      "        [-1.2490],\n",
      "        [ 0.7721],\n",
      "        [ 0.7751],\n",
      "        [-1.2616],\n",
      "        [ 0.6313],\n",
      "        [-0.7653],\n",
      "        [-1.2609],\n",
      "        [ 0.7750],\n",
      "        [-0.7653],\n",
      "        [-1.1105],\n",
      "        [-1.2615],\n",
      "        [ 0.7721],\n",
      "        [-1.2614],\n",
      "        [ 0.7631],\n",
      "        [-0.8472],\n",
      "        [ 0.7631],\n",
      "        [ 0.7751],\n",
      "        [ 0.7751]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 58/10000,\n",
      " train_loss: 0.0835,\n",
      " train_mae: 0.2383,\n",
      " epoch_time_duration: 0.0073\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1897],\n",
      "        [-1.1897],\n",
      "        [ 0.7888],\n",
      "        [ 0.7874],\n",
      "        [ 0.5239],\n",
      "        [ 0.7835],\n",
      "        [ 0.7909],\n",
      "        [ 0.7911],\n",
      "        [ 0.7909],\n",
      "        [ 0.7905],\n",
      "        [-1.1917],\n",
      "        [-1.1788],\n",
      "        [ 0.7864],\n",
      "        [ 0.0342],\n",
      "        [ 0.7910],\n",
      "        [ 0.0342],\n",
      "        [ 0.7864],\n",
      "        [-1.0941],\n",
      "        [-0.3022],\n",
      "        [ 0.7908],\n",
      "        [ 0.7901],\n",
      "        [ 0.7903],\n",
      "        [ 0.7913],\n",
      "        [ 0.7913],\n",
      "        [ 0.7912],\n",
      "        [-0.3022],\n",
      "        [-1.1901],\n",
      "        [ 0.7912],\n",
      "        [-1.1914],\n",
      "        [ 0.7911],\n",
      "        [-1.1909],\n",
      "        [ 0.7864],\n",
      "        [ 0.0342],\n",
      "        [ 0.7901],\n",
      "        [ 0.7851],\n",
      "        [ 0.7913],\n",
      "        [ 0.7901],\n",
      "        [-1.1414],\n",
      "        [ 0.3218],\n",
      "        [-1.1876],\n",
      "        [-1.1891],\n",
      "        [ 0.7911],\n",
      "        [-1.1788],\n",
      "        [ 0.7882],\n",
      "        [ 0.7913],\n",
      "        [-1.1917],\n",
      "        [ 0.6473],\n",
      "        [-0.7055],\n",
      "        [-1.1909],\n",
      "        [ 0.7912],\n",
      "        [-0.7055],\n",
      "        [-1.0415],\n",
      "        [-1.1916],\n",
      "        [ 0.7882],\n",
      "        [-1.1914],\n",
      "        [ 0.7790],\n",
      "        [-0.7847],\n",
      "        [ 0.7790],\n",
      "        [ 0.7913],\n",
      "        [ 0.7913]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 59/10000,\n",
      " train_loss: 0.0802,\n",
      " train_mae: 0.2408,\n",
      " epoch_time_duration: 0.0096\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1189],\n",
      "        [-1.1189],\n",
      "        [ 0.8058],\n",
      "        [ 0.8042],\n",
      "        [ 0.5414],\n",
      "        [ 0.8002],\n",
      "        [ 0.8080],\n",
      "        [ 0.8081],\n",
      "        [ 0.8080],\n",
      "        [ 0.8075],\n",
      "        [-1.1209],\n",
      "        [-1.1079],\n",
      "        [ 0.8032],\n",
      "        [ 0.0646],\n",
      "        [ 0.8080],\n",
      "        [ 0.0646],\n",
      "        [ 0.8032],\n",
      "        [-1.0241],\n",
      "        [-0.2596],\n",
      "        [ 0.8078],\n",
      "        [ 0.8070],\n",
      "        [ 0.8073],\n",
      "        [ 0.8084],\n",
      "        [ 0.8084],\n",
      "        [ 0.8083],\n",
      "        [-0.2596],\n",
      "        [-1.1193],\n",
      "        [ 0.8083],\n",
      "        [-1.1207],\n",
      "        [ 0.8081],\n",
      "        [-1.1201],\n",
      "        [ 0.8032],\n",
      "        [ 0.0646],\n",
      "        [ 0.8070],\n",
      "        [ 0.8019],\n",
      "        [ 0.8084],\n",
      "        [ 0.8070],\n",
      "        [-1.0707],\n",
      "        [ 0.3435],\n",
      "        [-1.1168],\n",
      "        [-1.1183],\n",
      "        [ 0.8081],\n",
      "        [-1.1079],\n",
      "        [ 0.8051],\n",
      "        [ 0.8084],\n",
      "        [-1.1209],\n",
      "        [ 0.6634],\n",
      "        [-0.6476],\n",
      "        [-1.1201],\n",
      "        [ 0.8082],\n",
      "        [-0.6476],\n",
      "        [-0.9727],\n",
      "        [-1.1209],\n",
      "        [ 0.8051],\n",
      "        [-1.1207],\n",
      "        [ 0.7956],\n",
      "        [-0.7240],\n",
      "        [ 0.7956],\n",
      "        [ 0.8084],\n",
      "        [ 0.8083]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 60/10000,\n",
      " train_loss: 0.0807,\n",
      " train_mae: 0.2431,\n",
      " epoch_time_duration: 0.0110\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.0819],\n",
      "        [-1.0819],\n",
      "        [ 0.8104],\n",
      "        [ 0.8088],\n",
      "        [ 0.5400],\n",
      "        [ 0.8045],\n",
      "        [ 0.8128],\n",
      "        [ 0.8129],\n",
      "        [ 0.8128],\n",
      "        [ 0.8123],\n",
      "        [-1.0841],\n",
      "        [-1.0709],\n",
      "        [ 0.8077],\n",
      "        [ 0.0672],\n",
      "        [ 0.8129],\n",
      "        [ 0.0672],\n",
      "        [ 0.8077],\n",
      "        [-0.9887],\n",
      "        [-0.2489],\n",
      "        [ 0.8126],\n",
      "        [ 0.8118],\n",
      "        [ 0.8121],\n",
      "        [ 0.8132],\n",
      "        [ 0.8132],\n",
      "        [ 0.8131],\n",
      "        [-0.2489],\n",
      "        [-1.0824],\n",
      "        [ 0.8131],\n",
      "        [-1.0838],\n",
      "        [ 0.8129],\n",
      "        [-1.0833],\n",
      "        [ 0.8077],\n",
      "        [ 0.0672],\n",
      "        [ 0.8118],\n",
      "        [ 0.8063],\n",
      "        [ 0.8132],\n",
      "        [ 0.8118],\n",
      "        [-1.0343],\n",
      "        [ 0.3423],\n",
      "        [-1.0798],\n",
      "        [-1.0814],\n",
      "        [ 0.8129],\n",
      "        [-1.0709],\n",
      "        [ 0.8097],\n",
      "        [ 0.8132],\n",
      "        [-1.0841],\n",
      "        [ 0.6634],\n",
      "        [-0.6242],\n",
      "        [-1.0833],\n",
      "        [ 0.8130],\n",
      "        [-0.6242],\n",
      "        [-0.9386],\n",
      "        [-1.0840],\n",
      "        [ 0.8097],\n",
      "        [-1.0838],\n",
      "        [ 0.7996],\n",
      "        [-0.6979],\n",
      "        [ 0.7996],\n",
      "        [ 0.8132],\n",
      "        [ 0.8132]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 61/10000,\n",
      " train_loss: 0.0817,\n",
      " train_mae: 0.2392,\n",
      " epoch_time_duration: 0.0090\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.0921],\n",
      "        [-1.0921],\n",
      "        [ 0.7984],\n",
      "        [ 0.7966],\n",
      "        [ 0.5133],\n",
      "        [ 0.7920],\n",
      "        [ 0.8011],\n",
      "        [ 0.8012],\n",
      "        [ 0.8011],\n",
      "        [ 0.8005],\n",
      "        [-1.0942],\n",
      "        [-1.0811],\n",
      "        [ 0.7954],\n",
      "        [ 0.0331],\n",
      "        [ 0.8012],\n",
      "        [ 0.0331],\n",
      "        [ 0.7954],\n",
      "        [-1.0004],\n",
      "        [-0.2802],\n",
      "        [ 0.8009],\n",
      "        [ 0.8000],\n",
      "        [ 0.8003],\n",
      "        [ 0.8016],\n",
      "        [ 0.8016],\n",
      "        [ 0.8015],\n",
      "        [-0.2802],\n",
      "        [-1.0925],\n",
      "        [ 0.8015],\n",
      "        [-1.0940],\n",
      "        [ 0.8012],\n",
      "        [-1.0934],\n",
      "        [ 0.7954],\n",
      "        [ 0.0331],\n",
      "        [ 0.8000],\n",
      "        [ 0.7939],\n",
      "        [ 0.8016],\n",
      "        [ 0.8000],\n",
      "        [-1.0450],\n",
      "        [ 0.3104],\n",
      "        [-1.0900],\n",
      "        [-1.0915],\n",
      "        [ 0.8012],\n",
      "        [-1.0811],\n",
      "        [ 0.7976],\n",
      "        [ 0.8016],\n",
      "        [-1.0943],\n",
      "        [ 0.6419],\n",
      "        [-0.6468],\n",
      "        [-1.0934],\n",
      "        [ 0.8014],\n",
      "        [-0.6468],\n",
      "        [-0.9518],\n",
      "        [-1.0942],\n",
      "        [ 0.7976],\n",
      "        [-1.0940],\n",
      "        [ 0.7866],\n",
      "        [-0.7184],\n",
      "        [ 0.7866],\n",
      "        [ 0.8016],\n",
      "        [ 0.8015]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 62/10000,\n",
      " train_loss: 0.0799,\n",
      " train_mae: 0.2355,\n",
      " epoch_time_duration: 0.0094\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1353],\n",
      "        [-1.1353],\n",
      "        [ 0.7766],\n",
      "        [ 0.7746],\n",
      "        [ 0.4694],\n",
      "        [ 0.7694],\n",
      "        [ 0.7796],\n",
      "        [ 0.7798],\n",
      "        [ 0.7796],\n",
      "        [ 0.7790],\n",
      "        [-1.1374],\n",
      "        [-1.1243],\n",
      "        [ 0.7732],\n",
      "        [-0.0261],\n",
      "        [ 0.7797],\n",
      "        [-0.0261],\n",
      "        [ 0.7732],\n",
      "        [-1.0452],\n",
      "        [-0.3401],\n",
      "        [ 0.7794],\n",
      "        [ 0.7783],\n",
      "        [ 0.7787],\n",
      "        [ 0.7802],\n",
      "        [ 0.7802],\n",
      "        [ 0.7801],\n",
      "        [-0.3401],\n",
      "        [-1.1357],\n",
      "        [ 0.7801],\n",
      "        [-1.1371],\n",
      "        [ 0.7798],\n",
      "        [-1.1366],\n",
      "        [ 0.7732],\n",
      "        [-0.0261],\n",
      "        [ 0.7783],\n",
      "        [ 0.7715],\n",
      "        [ 0.7802],\n",
      "        [ 0.7783],\n",
      "        [-1.0887],\n",
      "        [ 0.2576],\n",
      "        [-1.1331],\n",
      "        [-1.1347],\n",
      "        [ 0.7798],\n",
      "        [-1.1243],\n",
      "        [ 0.7757],\n",
      "        [ 0.7802],\n",
      "        [-1.1375],\n",
      "        [ 0.6061],\n",
      "        [-0.7009],\n",
      "        [-1.1366],\n",
      "        [ 0.7800],\n",
      "        [-0.7009],\n",
      "        [-0.9978],\n",
      "        [-1.1374],\n",
      "        [ 0.7757],\n",
      "        [-1.1371],\n",
      "        [ 0.7634],\n",
      "        [-0.7707],\n",
      "        [ 0.7634],\n",
      "        [ 0.7802],\n",
      "        [ 0.7802]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 63/10000,\n",
      " train_loss: 0.0777,\n",
      " train_mae: 0.2337,\n",
      " epoch_time_duration: 0.0075\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1804],\n",
      "        [-1.1804],\n",
      "        [ 0.7583],\n",
      "        [ 0.7561],\n",
      "        [ 0.4269],\n",
      "        [ 0.7502],\n",
      "        [ 0.7617],\n",
      "        [ 0.7620],\n",
      "        [ 0.7617],\n",
      "        [ 0.7611],\n",
      "        [-1.1826],\n",
      "        [-1.1694],\n",
      "        [ 0.7545],\n",
      "        [-0.0847],\n",
      "        [ 0.7619],\n",
      "        [-0.0847],\n",
      "        [ 0.7545],\n",
      "        [-1.0915],\n",
      "        [-0.3997],\n",
      "        [ 0.7616],\n",
      "        [ 0.7603],\n",
      "        [ 0.7607],\n",
      "        [ 0.7624],\n",
      "        [ 0.7624],\n",
      "        [ 0.7623],\n",
      "        [-0.3997],\n",
      "        [-1.1808],\n",
      "        [ 0.7623],\n",
      "        [-1.1823],\n",
      "        [ 0.7620],\n",
      "        [-1.1818],\n",
      "        [ 0.7545],\n",
      "        [-0.0847],\n",
      "        [ 0.7603],\n",
      "        [ 0.7526],\n",
      "        [ 0.7624],\n",
      "        [ 0.7603],\n",
      "        [-1.1342],\n",
      "        [ 0.2056],\n",
      "        [-1.1782],\n",
      "        [-1.1798],\n",
      "        [ 0.7620],\n",
      "        [-1.1694],\n",
      "        [ 0.7573],\n",
      "        [ 0.7624],\n",
      "        [-1.1826],\n",
      "        [ 0.5723],\n",
      "        [-0.7553],\n",
      "        [-1.1818],\n",
      "        [ 0.7622],\n",
      "        [-0.7553],\n",
      "        [-1.0452],\n",
      "        [-1.1826],\n",
      "        [ 0.7573],\n",
      "        [-1.1823],\n",
      "        [ 0.7435],\n",
      "        [-0.8236],\n",
      "        [ 0.7435],\n",
      "        [ 0.7624],\n",
      "        [ 0.7624]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 64/10000,\n",
      " train_loss: 0.0779,\n",
      " train_mae: 0.2335,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1987],\n",
      "        [-1.1987],\n",
      "        [ 0.7555],\n",
      "        [ 0.7529],\n",
      "        [ 0.4036],\n",
      "        [ 0.7464],\n",
      "        [ 0.7593],\n",
      "        [ 0.7596],\n",
      "        [ 0.7593],\n",
      "        [ 0.7585],\n",
      "        [-1.2010],\n",
      "        [-1.1876],\n",
      "        [ 0.7512],\n",
      "        [-0.1179],\n",
      "        [ 0.7595],\n",
      "        [-0.1179],\n",
      "        [ 0.7512],\n",
      "        [-1.1104],\n",
      "        [-0.4315],\n",
      "        [ 0.7591],\n",
      "        [ 0.7577],\n",
      "        [ 0.7582],\n",
      "        [ 0.7601],\n",
      "        [ 0.7601],\n",
      "        [ 0.7599],\n",
      "        [-0.4315],\n",
      "        [-1.1992],\n",
      "        [ 0.7599],\n",
      "        [-1.2007],\n",
      "        [ 0.7596],\n",
      "        [-1.2001],\n",
      "        [ 0.7512],\n",
      "        [-0.1179],\n",
      "        [ 0.7577],\n",
      "        [ 0.7491],\n",
      "        [ 0.7601],\n",
      "        [ 0.7577],\n",
      "        [-1.1525],\n",
      "        [ 0.1758],\n",
      "        [-1.1965],\n",
      "        [-1.1981],\n",
      "        [ 0.7596],\n",
      "        [-1.1876],\n",
      "        [ 0.7543],\n",
      "        [ 0.7601],\n",
      "        [-1.2010],\n",
      "        [ 0.5558],\n",
      "        [-0.7811],\n",
      "        [-1.2001],\n",
      "        [ 0.7598],\n",
      "        [-0.7811],\n",
      "        [-1.0648],\n",
      "        [-1.2009],\n",
      "        [ 0.7543],\n",
      "        [-1.2007],\n",
      "        [ 0.7391],\n",
      "        [-0.8479],\n",
      "        [ 0.7391],\n",
      "        [ 0.7601],\n",
      "        [ 0.7601]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 65/10000,\n",
      " train_loss: 0.0782,\n",
      " train_mae: 0.2316,\n",
      " epoch_time_duration: 0.0093\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1807],\n",
      "        [-1.1807],\n",
      "        [ 0.7712],\n",
      "        [ 0.7685],\n",
      "        [ 0.4060],\n",
      "        [ 0.7614],\n",
      "        [ 0.7754],\n",
      "        [ 0.7758],\n",
      "        [ 0.7754],\n",
      "        [ 0.7746],\n",
      "        [-1.1831],\n",
      "        [-1.1693],\n",
      "        [ 0.7666],\n",
      "        [-0.1167],\n",
      "        [ 0.7756],\n",
      "        [-0.1167],\n",
      "        [ 0.7666],\n",
      "        [-1.0921],\n",
      "        [-0.4258],\n",
      "        [ 0.7752],\n",
      "        [ 0.7737],\n",
      "        [ 0.7742],\n",
      "        [ 0.7763],\n",
      "        [ 0.7763],\n",
      "        [ 0.7761],\n",
      "        [-0.4258],\n",
      "        [-1.1812],\n",
      "        [ 0.7761],\n",
      "        [-1.1828],\n",
      "        [ 0.7758],\n",
      "        [-1.1822],\n",
      "        [ 0.7666],\n",
      "        [-0.1167],\n",
      "        [ 0.7737],\n",
      "        [ 0.7643],\n",
      "        [ 0.7763],\n",
      "        [ 0.7737],\n",
      "        [-1.1341],\n",
      "        [ 0.1759],\n",
      "        [-1.1784],\n",
      "        [-1.1801],\n",
      "        [ 0.7758],\n",
      "        [-1.1693],\n",
      "        [ 0.7700],\n",
      "        [ 0.7763],\n",
      "        [-1.1832],\n",
      "        [ 0.5618],\n",
      "        [-0.7684],\n",
      "        [-1.1822],\n",
      "        [ 0.7760],\n",
      "        [-0.7684],\n",
      "        [-1.0470],\n",
      "        [-1.1831],\n",
      "        [ 0.7700],\n",
      "        [-1.1828],\n",
      "        [ 0.7536],\n",
      "        [-0.8338],\n",
      "        [ 0.7536],\n",
      "        [ 0.7763],\n",
      "        [ 0.7763]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 66/10000,\n",
      " train_loss: 0.0763,\n",
      " train_mae: 0.2315,\n",
      " epoch_time_duration: 0.0078\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1407],\n",
      "        [-1.1407],\n",
      "        [ 0.7984],\n",
      "        [ 0.7955],\n",
      "        [ 0.4252],\n",
      "        [ 0.7880],\n",
      "        [ 0.8030],\n",
      "        [ 0.8034],\n",
      "        [ 0.8030],\n",
      "        [ 0.8021],\n",
      "        [-1.1433],\n",
      "        [-1.1289],\n",
      "        [ 0.7935],\n",
      "        [-0.0929],\n",
      "        [ 0.8032],\n",
      "        [-0.0929],\n",
      "        [ 0.7935],\n",
      "        [-1.0509],\n",
      "        [-0.3959],\n",
      "        [ 0.8028],\n",
      "        [ 0.8010],\n",
      "        [ 0.8016],\n",
      "        [ 0.8040],\n",
      "        [ 0.8040],\n",
      "        [ 0.8038],\n",
      "        [-0.3959],\n",
      "        [-1.1412],\n",
      "        [ 0.8038],\n",
      "        [-1.1429],\n",
      "        [ 0.8034],\n",
      "        [-1.1422],\n",
      "        [ 0.7935],\n",
      "        [-0.0929],\n",
      "        [ 0.8010],\n",
      "        [ 0.7910],\n",
      "        [ 0.8040],\n",
      "        [ 0.8010],\n",
      "        [-1.0931],\n",
      "        [ 0.1958],\n",
      "        [-1.1383],\n",
      "        [-1.1401],\n",
      "        [ 0.8034],\n",
      "        [-1.1289],\n",
      "        [ 0.7971],\n",
      "        [ 0.8040],\n",
      "        [-1.1433],\n",
      "        [ 0.5823],\n",
      "        [-0.7313],\n",
      "        [-1.1422],\n",
      "        [ 0.8036],\n",
      "        [-0.7313],\n",
      "        [-1.0060],\n",
      "        [-1.1432],\n",
      "        [ 0.7971],\n",
      "        [-1.1429],\n",
      "        [ 0.7797],\n",
      "        [-0.7955],\n",
      "        [ 0.7797],\n",
      "        [ 0.8040],\n",
      "        [ 0.8039]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 67/10000,\n",
      " train_loss: 0.0746,\n",
      " train_mae: 0.2335,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1062],\n",
      "        [-1.1062],\n",
      "        [ 0.8241],\n",
      "        [ 0.8210],\n",
      "        [ 0.4431],\n",
      "        [ 0.8130],\n",
      "        [ 0.8291],\n",
      "        [ 0.8295],\n",
      "        [ 0.8291],\n",
      "        [ 0.8281],\n",
      "        [-1.1090],\n",
      "        [-1.0939],\n",
      "        [ 0.8188],\n",
      "        [-0.0704],\n",
      "        [ 0.8293],\n",
      "        [-0.0704],\n",
      "        [ 0.8188],\n",
      "        [-1.0147],\n",
      "        [-0.3680],\n",
      "        [ 0.8289],\n",
      "        [ 0.8270],\n",
      "        [ 0.8276],\n",
      "        [ 0.8302],\n",
      "        [ 0.8302],\n",
      "        [ 0.8300],\n",
      "        [-0.3680],\n",
      "        [-1.1068],\n",
      "        [ 0.8300],\n",
      "        [-1.1086],\n",
      "        [ 0.8295],\n",
      "        [-1.1079],\n",
      "        [ 0.8188],\n",
      "        [-0.0704],\n",
      "        [ 0.8270],\n",
      "        [ 0.8162],\n",
      "        [ 0.8302],\n",
      "        [ 0.8270],\n",
      "        [-1.0573],\n",
      "        [ 0.2147],\n",
      "        [-1.1037],\n",
      "        [-1.1056],\n",
      "        [ 0.8295],\n",
      "        [-1.0939],\n",
      "        [ 0.8227],\n",
      "        [ 0.8302],\n",
      "        [-1.1090],\n",
      "        [ 0.6015],\n",
      "        [-0.6974],\n",
      "        [-1.1079],\n",
      "        [ 0.8298],\n",
      "        [-0.6974],\n",
      "        [-0.9696],\n",
      "        [-1.1089],\n",
      "        [ 0.8227],\n",
      "        [-1.1086],\n",
      "        [ 0.8042],\n",
      "        [-0.7607],\n",
      "        [ 0.8042],\n",
      "        [ 0.8302],\n",
      "        [ 0.8302]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 68/10000,\n",
      " train_loss: 0.0747,\n",
      " train_mae: 0.2332,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1000],\n",
      "        [-1.1000],\n",
      "        [ 0.8375],\n",
      "        [ 0.8341],\n",
      "        [ 0.4442],\n",
      "        [ 0.8255],\n",
      "        [ 0.8430],\n",
      "        [ 0.8434],\n",
      "        [ 0.8430],\n",
      "        [ 0.8419],\n",
      "        [-1.1029],\n",
      "        [-1.0871],\n",
      "        [ 0.8318],\n",
      "        [-0.0693],\n",
      "        [ 0.8432],\n",
      "        [-0.0693],\n",
      "        [ 0.8318],\n",
      "        [-1.0061],\n",
      "        [-0.3637],\n",
      "        [ 0.8427],\n",
      "        [ 0.8406],\n",
      "        [ 0.8413],\n",
      "        [ 0.8442],\n",
      "        [ 0.8442],\n",
      "        [ 0.8439],\n",
      "        [-0.3637],\n",
      "        [-1.1005],\n",
      "        [ 0.8439],\n",
      "        [-1.1025],\n",
      "        [ 0.8434],\n",
      "        [-1.1017],\n",
      "        [ 0.8318],\n",
      "        [-0.0693],\n",
      "        [ 0.8406],\n",
      "        [ 0.8289],\n",
      "        [ 0.8442],\n",
      "        [ 0.8406],\n",
      "        [-1.0494],\n",
      "        [ 0.2145],\n",
      "        [-1.0973],\n",
      "        [-1.0992],\n",
      "        [ 0.8434],\n",
      "        [-1.0871],\n",
      "        [ 0.8360],\n",
      "        [ 0.8442],\n",
      "        [-1.1029],\n",
      "        [ 0.6054],\n",
      "        [-0.6894],\n",
      "        [-1.1017],\n",
      "        [ 0.8437],\n",
      "        [-0.6894],\n",
      "        [-0.9606],\n",
      "        [-1.1028],\n",
      "        [ 0.8360],\n",
      "        [-1.1025],\n",
      "        [ 0.8161],\n",
      "        [-0.7522],\n",
      "        [ 0.8161],\n",
      "        [ 0.8442],\n",
      "        [ 0.8441]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 69/10000,\n",
      " train_loss: 0.0745,\n",
      " train_mae: 0.2303,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1257],\n",
      "        [-1.1257],\n",
      "        [ 0.8357],\n",
      "        [ 0.8319],\n",
      "        [ 0.4248],\n",
      "        [ 0.8226],\n",
      "        [ 0.8418],\n",
      "        [ 0.8423],\n",
      "        [ 0.8418],\n",
      "        [ 0.8405],\n",
      "        [-1.1289],\n",
      "        [-1.1122],\n",
      "        [ 0.8294],\n",
      "        [-0.0937],\n",
      "        [ 0.8421],\n",
      "        [-0.0937],\n",
      "        [ 0.8294],\n",
      "        [-1.0292],\n",
      "        [-0.3873],\n",
      "        [ 0.8415],\n",
      "        [ 0.8392],\n",
      "        [ 0.8399],\n",
      "        [ 0.8432],\n",
      "        [ 0.8432],\n",
      "        [ 0.8429],\n",
      "        [-0.3873],\n",
      "        [-1.1263],\n",
      "        [ 0.8429],\n",
      "        [-1.1284],\n",
      "        [ 0.8423],\n",
      "        [-1.1276],\n",
      "        [ 0.8294],\n",
      "        [-0.0937],\n",
      "        [ 0.8392],\n",
      "        [ 0.8263],\n",
      "        [ 0.8432],\n",
      "        [ 0.8392],\n",
      "        [-1.0733],\n",
      "        [ 0.1914],\n",
      "        [-1.1229],\n",
      "        [-1.1250],\n",
      "        [ 0.8423],\n",
      "        [-1.1122],\n",
      "        [ 0.8340],\n",
      "        [ 0.8432],\n",
      "        [-1.1290],\n",
      "        [ 0.5906],\n",
      "        [-0.7114],\n",
      "        [-1.1276],\n",
      "        [ 0.8426],\n",
      "        [-0.7114],\n",
      "        [-0.9831],\n",
      "        [-1.1288],\n",
      "        [ 0.8340],\n",
      "        [-1.1284],\n",
      "        [ 0.8124],\n",
      "        [-0.7740],\n",
      "        [ 0.8124],\n",
      "        [ 0.8432],\n",
      "        [ 0.8431]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 70/10000,\n",
      " train_loss: 0.0730,\n",
      " train_mae: 0.2268,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1671],\n",
      "        [-1.1671],\n",
      "        [ 0.8249],\n",
      "        [ 0.8207],\n",
      "        [ 0.3944],\n",
      "        [ 0.8104],\n",
      "        [ 0.8317],\n",
      "        [ 0.8322],\n",
      "        [ 0.8317],\n",
      "        [ 0.8302],\n",
      "        [-1.1706],\n",
      "        [-1.1527],\n",
      "        [ 0.8179],\n",
      "        [-0.1302],\n",
      "        [ 0.8320],\n",
      "        [-0.1302],\n",
      "        [ 0.8179],\n",
      "        [-1.0672],\n",
      "        [-0.4237],\n",
      "        [ 0.8313],\n",
      "        [ 0.8287],\n",
      "        [ 0.8295],\n",
      "        [ 0.8333],\n",
      "        [ 0.8333],\n",
      "        [ 0.8329],\n",
      "        [-0.4237],\n",
      "        [-1.1678],\n",
      "        [ 0.8329],\n",
      "        [-1.1700],\n",
      "        [ 0.8322],\n",
      "        [-1.1691],\n",
      "        [ 0.8179],\n",
      "        [-0.1302],\n",
      "        [ 0.8287],\n",
      "        [ 0.8145],\n",
      "        [ 0.8333],\n",
      "        [ 0.8287],\n",
      "        [-1.1124],\n",
      "        [ 0.1568],\n",
      "        [-1.1641],\n",
      "        [-1.1663],\n",
      "        [ 0.8322],\n",
      "        [-1.1527],\n",
      "        [ 0.8230],\n",
      "        [ 0.8333],\n",
      "        [-1.1706],\n",
      "        [ 0.5654],\n",
      "        [-0.7472],\n",
      "        [-1.1691],\n",
      "        [ 0.8326],\n",
      "        [-0.7472],\n",
      "        [-1.0203],\n",
      "        [-1.1705],\n",
      "        [ 0.8230],\n",
      "        [-1.1700],\n",
      "        [ 0.7994],\n",
      "        [-0.8099],\n",
      "        [ 0.7994],\n",
      "        [ 0.8333],\n",
      "        [ 0.8332]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 71/10000,\n",
      " train_loss: 0.0718,\n",
      " train_mae: 0.2259,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1988],\n",
      "        [-1.1988],\n",
      "        [ 0.8151],\n",
      "        [ 0.8106],\n",
      "        [ 0.3695],\n",
      "        [ 0.7995],\n",
      "        [ 0.8227],\n",
      "        [ 0.8233],\n",
      "        [ 0.8227],\n",
      "        [ 0.8211],\n",
      "        [-1.2027],\n",
      "        [-1.1835],\n",
      "        [ 0.8076],\n",
      "        [-0.1568],\n",
      "        [ 0.8230],\n",
      "        [-0.1568],\n",
      "        [ 0.8076],\n",
      "        [-1.0947],\n",
      "        [-0.4489],\n",
      "        [ 0.8222],\n",
      "        [ 0.8193],\n",
      "        [ 0.8203],\n",
      "        [ 0.8245],\n",
      "        [ 0.8245],\n",
      "        [ 0.8240],\n",
      "        [-0.4489],\n",
      "        [-1.1996],\n",
      "        [ 0.8240],\n",
      "        [-1.2021],\n",
      "        [ 0.8233],\n",
      "        [-1.2011],\n",
      "        [ 0.8076],\n",
      "        [-0.1568],\n",
      "        [ 0.8193],\n",
      "        [ 0.8039],\n",
      "        [ 0.8245],\n",
      "        [ 0.8193],\n",
      "        [-1.1414],\n",
      "        [ 0.1300],\n",
      "        [-1.1956],\n",
      "        [-1.1980],\n",
      "        [ 0.8233],\n",
      "        [-1.1835],\n",
      "        [ 0.8131],\n",
      "        [ 0.8245],\n",
      "        [-1.2027],\n",
      "        [ 0.5438],\n",
      "        [-0.7715],\n",
      "        [-1.2011],\n",
      "        [ 0.8237],\n",
      "        [-0.7715],\n",
      "        [-1.0468],\n",
      "        [-1.2025],\n",
      "        [ 0.8131],\n",
      "        [-1.2021],\n",
      "        [ 0.7876],\n",
      "        [-0.8343],\n",
      "        [ 0.7876],\n",
      "        [ 0.8245],\n",
      "        [ 0.8244]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 72/10000,\n",
      " train_loss: 0.0717,\n",
      " train_mae: 0.2246,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2035],\n",
      "        [-1.2035],\n",
      "        [ 0.8139],\n",
      "        [ 0.8090],\n",
      "        [ 0.3622],\n",
      "        [ 0.7972],\n",
      "        [ 0.8220],\n",
      "        [ 0.8227],\n",
      "        [ 0.8220],\n",
      "        [ 0.8203],\n",
      "        [-1.2077],\n",
      "        [-1.1870],\n",
      "        [ 0.8058],\n",
      "        [-0.1579],\n",
      "        [ 0.8224],\n",
      "        [-0.1579],\n",
      "        [ 0.8058],\n",
      "        [-1.0941],\n",
      "        [-0.4461],\n",
      "        [ 0.8216],\n",
      "        [ 0.8184],\n",
      "        [ 0.8194],\n",
      "        [ 0.8240],\n",
      "        [ 0.8240],\n",
      "        [ 0.8236],\n",
      "        [-0.4461],\n",
      "        [-1.2043],\n",
      "        [ 0.8236],\n",
      "        [-1.2071],\n",
      "        [ 0.8227],\n",
      "        [-1.2059],\n",
      "        [ 0.8058],\n",
      "        [-0.1579],\n",
      "        [ 0.8184],\n",
      "        [ 0.8019],\n",
      "        [ 0.8240],\n",
      "        [ 0.8184],\n",
      "        [-1.1425],\n",
      "        [ 0.1249],\n",
      "        [-1.1999],\n",
      "        [-1.2025],\n",
      "        [ 0.8227],\n",
      "        [-1.1870],\n",
      "        [ 0.8117],\n",
      "        [ 0.8240],\n",
      "        [-1.2078],\n",
      "        [ 0.5363],\n",
      "        [-0.7666],\n",
      "        [-1.2059],\n",
      "        [ 0.8232],\n",
      "        [-0.7666],\n",
      "        [-1.0448],\n",
      "        [-1.2076],\n",
      "        [ 0.8117],\n",
      "        [-1.2071],\n",
      "        [ 0.7848],\n",
      "        [-0.8295],\n",
      "        [ 0.7848],\n",
      "        [ 0.8240],\n",
      "        [ 0.8240]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 73/10000,\n",
      " train_loss: 0.0709,\n",
      " train_mae: 0.2227,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1821],\n",
      "        [-1.1821],\n",
      "        [ 0.8211],\n",
      "        [ 0.8160],\n",
      "        [ 0.3722],\n",
      "        [ 0.8038],\n",
      "        [ 0.8297],\n",
      "        [ 0.8305],\n",
      "        [ 0.8297],\n",
      "        [ 0.8279],\n",
      "        [-1.1869],\n",
      "        [-1.1640],\n",
      "        [ 0.8126],\n",
      "        [-0.1344],\n",
      "        [ 0.8302],\n",
      "        [-0.1344],\n",
      "        [ 0.8126],\n",
      "        [-1.0662],\n",
      "        [-0.4163],\n",
      "        [ 0.8292],\n",
      "        [ 0.8259],\n",
      "        [ 0.8270],\n",
      "        [ 0.8320],\n",
      "        [ 0.8320],\n",
      "        [ 0.8314],\n",
      "        [-0.4163],\n",
      "        [-1.1830],\n",
      "        [ 0.8314],\n",
      "        [-1.1861],\n",
      "        [ 0.8305],\n",
      "        [-1.1848],\n",
      "        [ 0.8126],\n",
      "        [-0.1344],\n",
      "        [ 0.8259],\n",
      "        [ 0.8086],\n",
      "        [ 0.8320],\n",
      "        [ 0.8259],\n",
      "        [-1.1168],\n",
      "        [ 0.1410],\n",
      "        [-1.1782],\n",
      "        [-1.1810],\n",
      "        [ 0.8305],\n",
      "        [-1.1640],\n",
      "        [ 0.8188],\n",
      "        [ 0.8320],\n",
      "        [-1.1869],\n",
      "        [ 0.5429],\n",
      "        [-0.7336],\n",
      "        [-1.1848],\n",
      "        [ 0.8311],\n",
      "        [-0.7336],\n",
      "        [-1.0152],\n",
      "        [-1.1867],\n",
      "        [ 0.8188],\n",
      "        [-1.1861],\n",
      "        [ 0.7909],\n",
      "        [-0.7967],\n",
      "        [ 0.7909],\n",
      "        [ 0.8320],\n",
      "        [ 0.8319]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 74/10000,\n",
      " train_loss: 0.0692,\n",
      " train_mae: 0.2218,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1521],\n",
      "        [-1.1521],\n",
      "        [ 0.8300],\n",
      "        [ 0.8247],\n",
      "        [ 0.3878],\n",
      "        [ 0.8121],\n",
      "        [ 0.8391],\n",
      "        [ 0.8400],\n",
      "        [ 0.8391],\n",
      "        [ 0.8371],\n",
      "        [-1.1575],\n",
      "        [-1.1323],\n",
      "        [ 0.8212],\n",
      "        [-0.1023],\n",
      "        [ 0.8396],\n",
      "        [-0.1023],\n",
      "        [ 0.8212],\n",
      "        [-1.0287],\n",
      "        [-0.3769],\n",
      "        [ 0.8386],\n",
      "        [ 0.8350],\n",
      "        [ 0.8362],\n",
      "        [ 0.8415],\n",
      "        [ 0.8415],\n",
      "        [ 0.8409],\n",
      "        [-0.3769],\n",
      "        [-1.1531],\n",
      "        [ 0.8409],\n",
      "        [-1.1566],\n",
      "        [ 0.8400],\n",
      "        [-1.1552],\n",
      "        [ 0.8212],\n",
      "        [-0.1023],\n",
      "        [ 0.8350],\n",
      "        [ 0.8171],\n",
      "        [ 0.8415],\n",
      "        [ 0.8350],\n",
      "        [-1.0818],\n",
      "        [ 0.1642],\n",
      "        [-1.1477],\n",
      "        [-1.1509],\n",
      "        [ 0.8400],\n",
      "        [-1.1323],\n",
      "        [ 0.8276],\n",
      "        [ 0.8415],\n",
      "        [-1.1576],\n",
      "        [ 0.5537],\n",
      "        [-0.6905],\n",
      "        [-1.1552],\n",
      "        [ 0.8405],\n",
      "        [-0.6905],\n",
      "        [-0.9759],\n",
      "        [-1.1573],\n",
      "        [ 0.8276],\n",
      "        [-1.1566],\n",
      "        [ 0.7990],\n",
      "        [-0.7537],\n",
      "        [ 0.7990],\n",
      "        [ 0.8415],\n",
      "        [ 0.8414]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 75/10000,\n",
      " train_loss: 0.0681,\n",
      " train_mae: 0.2211,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1350],\n",
      "        [-1.1350],\n",
      "        [ 0.8323],\n",
      "        [ 0.8268],\n",
      "        [ 0.3947],\n",
      "        [ 0.8138],\n",
      "        [ 0.8420],\n",
      "        [ 0.8429],\n",
      "        [ 0.8420],\n",
      "        [ 0.8399],\n",
      "        [-1.1412],\n",
      "        [-1.1132],\n",
      "        [ 0.8232],\n",
      "        [-0.0809],\n",
      "        [ 0.8425],\n",
      "        [-0.0809],\n",
      "        [ 0.8232],\n",
      "        [-1.0033],\n",
      "        [-0.3489],\n",
      "        [ 0.8414],\n",
      "        [ 0.8376],\n",
      "        [ 0.8388],\n",
      "        [ 0.8447],\n",
      "        [ 0.8447],\n",
      "        [ 0.8440],\n",
      "        [-0.3489],\n",
      "        [-1.1361],\n",
      "        [ 0.8440],\n",
      "        [-1.1401],\n",
      "        [ 0.8429],\n",
      "        [-1.1384],\n",
      "        [ 0.8232],\n",
      "        [-0.0809],\n",
      "        [ 0.8376],\n",
      "        [ 0.8189],\n",
      "        [ 0.8447],\n",
      "        [ 0.8376],\n",
      "        [-1.0591],\n",
      "        [ 0.1777],\n",
      "        [-1.1301],\n",
      "        [-1.1336],\n",
      "        [ 0.8429],\n",
      "        [-1.1132],\n",
      "        [ 0.8298],\n",
      "        [ 0.8447],\n",
      "        [-1.1413],\n",
      "        [ 0.5565],\n",
      "        [-0.6592],\n",
      "        [-1.1384],\n",
      "        [ 0.8436],\n",
      "        [-0.6592],\n",
      "        [-0.9485],\n",
      "        [-1.1409],\n",
      "        [ 0.8298],\n",
      "        [-1.1401],\n",
      "        [ 0.8004],\n",
      "        [-0.7225],\n",
      "        [ 0.8004],\n",
      "        [ 0.8447],\n",
      "        [ 0.8445]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 76/10000,\n",
      " train_loss: 0.0675,\n",
      " train_mae: 0.2186,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1419],\n",
      "        [-1.1419],\n",
      "        [ 0.8241],\n",
      "        [ 0.8183],\n",
      "        [ 0.3855],\n",
      "        [ 0.8046],\n",
      "        [ 0.8346],\n",
      "        [ 0.8356],\n",
      "        [ 0.8346],\n",
      "        [ 0.8323],\n",
      "        [-1.1489],\n",
      "        [-1.1179],\n",
      "        [ 0.8145],\n",
      "        [-0.0800],\n",
      "        [ 0.8352],\n",
      "        [-0.0800],\n",
      "        [ 0.8145],\n",
      "        [-1.0014],\n",
      "        [-0.3431],\n",
      "        [ 0.8340],\n",
      "        [ 0.8298],\n",
      "        [ 0.8312],\n",
      "        [ 0.8376],\n",
      "        [ 0.8376],\n",
      "        [ 0.8368],\n",
      "        [-0.3431],\n",
      "        [-1.1431],\n",
      "        [ 0.8368],\n",
      "        [-1.1477],\n",
      "        [ 0.8356],\n",
      "        [-1.1458],\n",
      "        [ 0.8145],\n",
      "        [-0.0800],\n",
      "        [ 0.8298],\n",
      "        [ 0.8100],\n",
      "        [ 0.8376],\n",
      "        [ 0.8298],\n",
      "        [-1.0600],\n",
      "        [ 0.1728],\n",
      "        [-1.1364],\n",
      "        [-1.1404],\n",
      "        [ 0.8356],\n",
      "        [-1.1179],\n",
      "        [ 0.8215],\n",
      "        [ 0.8376],\n",
      "        [-1.1490],\n",
      "        [ 0.5452],\n",
      "        [-0.6509],\n",
      "        [-1.1458],\n",
      "        [ 0.8363],\n",
      "        [-0.6509],\n",
      "        [-0.9445],\n",
      "        [-1.1487],\n",
      "        [ 0.8215],\n",
      "        [-1.1477],\n",
      "        [ 0.7907],\n",
      "        [-0.7145],\n",
      "        [ 0.7907],\n",
      "        [ 0.8376],\n",
      "        [ 0.8374]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 77/10000,\n",
      " train_loss: 0.0662,\n",
      " train_mae: 0.2153,\n",
      " epoch_time_duration: 0.0151\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1674],\n",
      "        [-1.1674],\n",
      "        [ 0.8086],\n",
      "        [ 0.8024],\n",
      "        [ 0.3642],\n",
      "        [ 0.7878],\n",
      "        [ 0.8201],\n",
      "        [ 0.8212],\n",
      "        [ 0.8201],\n",
      "        [ 0.8175],\n",
      "        [-1.1755],\n",
      "        [-1.1410],\n",
      "        [ 0.7983],\n",
      "        [-0.0948],\n",
      "        [ 0.8207],\n",
      "        [-0.0948],\n",
      "        [ 0.7983],\n",
      "        [-1.0176],\n",
      "        [-0.3542],\n",
      "        [ 0.8193],\n",
      "        [ 0.8148],\n",
      "        [ 0.8163],\n",
      "        [ 0.8234],\n",
      "        [ 0.8234],\n",
      "        [ 0.8225],\n",
      "        [-0.3542],\n",
      "        [-1.1688],\n",
      "        [ 0.8225],\n",
      "        [-1.1741],\n",
      "        [ 0.8212],\n",
      "        [-1.1718],\n",
      "        [ 0.7983],\n",
      "        [-0.0948],\n",
      "        [ 0.8148],\n",
      "        [ 0.7935],\n",
      "        [ 0.8234],\n",
      "        [ 0.8148],\n",
      "        [-1.0791],\n",
      "        [ 0.1541],\n",
      "        [-1.1614],\n",
      "        [-1.1657],\n",
      "        [ 0.8212],\n",
      "        [-1.1410],\n",
      "        [ 0.8058],\n",
      "        [ 0.8234],\n",
      "        [-1.1756],\n",
      "        [ 0.5232],\n",
      "        [-0.6605],\n",
      "        [-1.1718],\n",
      "        [ 0.8220],\n",
      "        [-0.6605],\n",
      "        [-0.9587],\n",
      "        [-1.1752],\n",
      "        [ 0.8058],\n",
      "        [-1.1741],\n",
      "        [ 0.7732],\n",
      "        [-0.7244],\n",
      "        [ 0.7732],\n",
      "        [ 0.8234],\n",
      "        [ 0.8232]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 78/10000,\n",
      " train_loss: 0.0646,\n",
      " train_mae: 0.2133,\n",
      " epoch_time_duration: 0.0100\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1947],\n",
      "        [-1.1947],\n",
      "        [ 0.7937],\n",
      "        [ 0.7869],\n",
      "        [ 0.3425],\n",
      "        [ 0.7715],\n",
      "        [ 0.8062],\n",
      "        [ 0.8075],\n",
      "        [ 0.8062],\n",
      "        [ 0.8033],\n",
      "        [-1.2040],\n",
      "        [-1.1657],\n",
      "        [ 0.7826],\n",
      "        [-0.1103],\n",
      "        [ 0.8069],\n",
      "        [-0.1103],\n",
      "        [ 0.7826],\n",
      "        [-1.0348],\n",
      "        [-0.3661],\n",
      "        [ 0.8054],\n",
      "        [ 0.8004],\n",
      "        [ 0.8020],\n",
      "        [ 0.8100],\n",
      "        [ 0.8100],\n",
      "        [ 0.8090],\n",
      "        [-0.3661],\n",
      "        [-1.1963],\n",
      "        [ 0.8090],\n",
      "        [-1.2023],\n",
      "        [ 0.8075],\n",
      "        [-1.1998],\n",
      "        [ 0.7826],\n",
      "        [-0.1103],\n",
      "        [ 0.8004],\n",
      "        [ 0.7775],\n",
      "        [ 0.8100],\n",
      "        [ 0.8004],\n",
      "        [-1.0994],\n",
      "        [ 0.1348],\n",
      "        [-1.1880],\n",
      "        [-1.1929],\n",
      "        [ 0.8075],\n",
      "        [-1.1657],\n",
      "        [ 0.7906],\n",
      "        [ 0.8100],\n",
      "        [-1.2041],\n",
      "        [ 0.5011],\n",
      "        [-0.6709],\n",
      "        [-1.1998],\n",
      "        [ 0.8084],\n",
      "        [-0.6709],\n",
      "        [-0.9737],\n",
      "        [-1.2036],\n",
      "        [ 0.7906],\n",
      "        [-1.2023],\n",
      "        [ 0.7560],\n",
      "        [-0.7351],\n",
      "        [ 0.7560],\n",
      "        [ 0.8100],\n",
      "        [ 0.8098]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 79/10000,\n",
      " train_loss: 0.0636,\n",
      " train_mae: 0.2116,\n",
      " epoch_time_duration: 0.0092\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2077],\n",
      "        [-1.2077],\n",
      "        [ 0.7868],\n",
      "        [ 0.7796],\n",
      "        [ 0.3318],\n",
      "        [ 0.7632],\n",
      "        [ 0.8005],\n",
      "        [ 0.8019],\n",
      "        [ 0.8005],\n",
      "        [ 0.7973],\n",
      "        [-1.2183],\n",
      "        [-1.1755],\n",
      "        [ 0.7750],\n",
      "        [-0.1123],\n",
      "        [ 0.8012],\n",
      "        [-0.1123],\n",
      "        [ 0.7750],\n",
      "        [-1.0365],\n",
      "        [-0.3636],\n",
      "        [ 0.7996],\n",
      "        [ 0.7941],\n",
      "        [ 0.7958],\n",
      "        [ 0.8047],\n",
      "        [ 0.8047],\n",
      "        [ 0.8036],\n",
      "        [-0.3636],\n",
      "        [-1.2095],\n",
      "        [ 0.8036],\n",
      "        [-1.2163],\n",
      "        [ 0.8019],\n",
      "        [-1.2134],\n",
      "        [ 0.7750],\n",
      "        [-0.1123],\n",
      "        [ 0.7941],\n",
      "        [ 0.7696],\n",
      "        [ 0.8047],\n",
      "        [ 0.7941],\n",
      "        [-1.1044],\n",
      "        [ 0.1277],\n",
      "        [-1.2001],\n",
      "        [-1.2055],\n",
      "        [ 0.8019],\n",
      "        [-1.1755],\n",
      "        [ 0.7835],\n",
      "        [ 0.8047],\n",
      "        [-1.2185],\n",
      "        [ 0.4889],\n",
      "        [-0.6659],\n",
      "        [-1.2134],\n",
      "        [ 0.8029],\n",
      "        [-0.6659],\n",
      "        [-0.9731],\n",
      "        [-1.2179],\n",
      "        [ 0.7835],\n",
      "        [-1.2163],\n",
      "        [ 0.7470],\n",
      "        [-0.7303],\n",
      "        [ 0.7470],\n",
      "        [ 0.8047],\n",
      "        [ 0.8045]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 80/10000,\n",
      " train_loss: 0.0627,\n",
      " train_mae: 0.2089,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2017],\n",
      "        [-1.2017],\n",
      "        [ 0.7902],\n",
      "        [ 0.7825],\n",
      "        [ 0.3353],\n",
      "        [ 0.7653],\n",
      "        [ 0.8049],\n",
      "        [ 0.8064],\n",
      "        [ 0.8049],\n",
      "        [ 0.8014],\n",
      "        [-1.2140],\n",
      "        [-1.1659],\n",
      "        [ 0.7776],\n",
      "        [-0.0971],\n",
      "        [ 0.8057],\n",
      "        [-0.0971],\n",
      "        [ 0.7776],\n",
      "        [-1.0181],\n",
      "        [-0.3425],\n",
      "        [ 0.8039],\n",
      "        [ 0.7980],\n",
      "        [ 0.7998],\n",
      "        [ 0.8097],\n",
      "        [ 0.8097],\n",
      "        [ 0.8084],\n",
      "        [-0.3425],\n",
      "        [-1.2037],\n",
      "        [ 0.8084],\n",
      "        [-1.2116],\n",
      "        [ 0.8064],\n",
      "        [-1.2082],\n",
      "        [ 0.7776],\n",
      "        [-0.0971],\n",
      "        [ 0.7980],\n",
      "        [ 0.7719],\n",
      "        [ 0.8097],\n",
      "        [ 0.7980],\n",
      "        [-1.0894],\n",
      "        [ 0.1364],\n",
      "        [-1.1931],\n",
      "        [-1.1992],\n",
      "        [ 0.8064],\n",
      "        [-1.1659],\n",
      "        [ 0.7866],\n",
      "        [ 0.8097],\n",
      "        [-1.2142],\n",
      "        [ 0.4895],\n",
      "        [-0.6412],\n",
      "        [-1.2082],\n",
      "        [ 0.8076],\n",
      "        [-0.6412],\n",
      "        [-0.9522],\n",
      "        [-1.2134],\n",
      "        [ 0.7866],\n",
      "        [-1.2116],\n",
      "        [ 0.7485],\n",
      "        [-0.7056],\n",
      "        [ 0.7485],\n",
      "        [ 0.8097],\n",
      "        [ 0.8094]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 81/10000,\n",
      " train_loss: 0.0611,\n",
      " train_mae: 0.2062,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1859],\n",
      "        [-1.1859],\n",
      "        [ 0.7996],\n",
      "        [ 0.7915],\n",
      "        [ 0.3460],\n",
      "        [ 0.7735],\n",
      "        [ 0.8155],\n",
      "        [ 0.8173],\n",
      "        [ 0.8155],\n",
      "        [ 0.8117],\n",
      "        [-1.2002],\n",
      "        [-1.1461],\n",
      "        [ 0.7864],\n",
      "        [-0.0737],\n",
      "        [ 0.8165],\n",
      "        [-0.0737],\n",
      "        [ 0.7864],\n",
      "        [-0.9890],\n",
      "        [-0.3125],\n",
      "        [ 0.8145],\n",
      "        [ 0.8080],\n",
      "        [ 0.8100],\n",
      "        [ 0.8209],\n",
      "        [ 0.8209],\n",
      "        [ 0.8194],\n",
      "        [-0.3125],\n",
      "        [-1.1882],\n",
      "        [ 0.8194],\n",
      "        [-1.1974],\n",
      "        [ 0.8173],\n",
      "        [-1.1934],\n",
      "        [ 0.7864],\n",
      "        [-0.0737],\n",
      "        [ 0.8080],\n",
      "        [ 0.7804],\n",
      "        [ 0.8209],\n",
      "        [ 0.8080],\n",
      "        [-1.0639],\n",
      "        [ 0.1527],\n",
      "        [-1.1762],\n",
      "        [-1.1831],\n",
      "        [ 0.8173],\n",
      "        [-1.1461],\n",
      "        [ 0.7959],\n",
      "        [ 0.8209],\n",
      "        [-1.2004],\n",
      "        [ 0.4969],\n",
      "        [-0.6068],\n",
      "        [-1.1934],\n",
      "        [ 0.8185],\n",
      "        [-0.6068],\n",
      "        [-0.9208],\n",
      "        [-1.1995],\n",
      "        [ 0.7959],\n",
      "        [-1.1974],\n",
      "        [ 0.7561],\n",
      "        [-0.6710],\n",
      "        [ 0.7561],\n",
      "        [ 0.8209],\n",
      "        [ 0.8206]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 82/10000,\n",
      " train_loss: 0.0596,\n",
      " train_mae: 0.2034,\n",
      " epoch_time_duration: 0.0072\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1758],\n",
      "        [-1.1758],\n",
      "        [ 0.8080],\n",
      "        [ 0.7993],\n",
      "        [ 0.3521],\n",
      "        [ 0.7803],\n",
      "        [ 0.8254],\n",
      "        [ 0.8273],\n",
      "        [ 0.8254],\n",
      "        [ 0.8212],\n",
      "        [-1.1923],\n",
      "        [-1.1318],\n",
      "        [ 0.7939],\n",
      "        [-0.0567],\n",
      "        [ 0.8264],\n",
      "        [-0.0567],\n",
      "        [ 0.7939],\n",
      "        [-0.9654],\n",
      "        [-0.2895],\n",
      "        [ 0.8242],\n",
      "        [ 0.8171],\n",
      "        [ 0.8193],\n",
      "        [ 0.8315],\n",
      "        [ 0.8315],\n",
      "        [ 0.8298],\n",
      "        [-0.2895],\n",
      "        [-1.1784],\n",
      "        [ 0.8298],\n",
      "        [-1.1890],\n",
      "        [ 0.8273],\n",
      "        [-1.1844],\n",
      "        [ 0.7939],\n",
      "        [-0.0567],\n",
      "        [ 0.8171],\n",
      "        [ 0.7876],\n",
      "        [ 0.8315],\n",
      "        [ 0.8171],\n",
      "        [-1.0438],\n",
      "        [ 0.1635],\n",
      "        [-1.1650],\n",
      "        [-1.1727],\n",
      "        [ 0.8273],\n",
      "        [-1.1318],\n",
      "        [ 0.8040],\n",
      "        [ 0.8315],\n",
      "        [-1.1926],\n",
      "        [ 0.5006],\n",
      "        [-0.5790],\n",
      "        [-1.1844],\n",
      "        [ 0.8287],\n",
      "        [-0.5790],\n",
      "        [-0.8951],\n",
      "        [-1.1915],\n",
      "        [ 0.8040],\n",
      "        [-1.1890],\n",
      "        [ 0.7621],\n",
      "        [-0.6429],\n",
      "        [ 0.7621],\n",
      "        [ 0.8315],\n",
      "        [ 0.8312]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 83/10000,\n",
      " train_loss: 0.0583,\n",
      " train_mae: 0.2007,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1819],\n",
      "        [-1.1819],\n",
      "        [ 0.8102],\n",
      "        [ 0.8007],\n",
      "        [ 0.3453],\n",
      "        [ 0.7802],\n",
      "        [ 0.8295],\n",
      "        [ 0.8316],\n",
      "        [ 0.8295],\n",
      "        [ 0.8247],\n",
      "        [-1.2009],\n",
      "        [-1.1336],\n",
      "        [ 0.7948],\n",
      "        [-0.0564],\n",
      "        [ 0.8306],\n",
      "        [-0.0564],\n",
      "        [ 0.7948],\n",
      "        [-0.9587],\n",
      "        [-0.2843],\n",
      "        [ 0.8281],\n",
      "        [ 0.8201],\n",
      "        [ 0.8226],\n",
      "        [ 0.8365],\n",
      "        [ 0.8365],\n",
      "        [ 0.8345],\n",
      "        [-0.2843],\n",
      "        [-1.1849],\n",
      "        [ 0.8345],\n",
      "        [-1.1970],\n",
      "        [ 0.8316],\n",
      "        [-1.1916],\n",
      "        [ 0.7948],\n",
      "        [-0.0564],\n",
      "        [ 0.8201],\n",
      "        [ 0.7880],\n",
      "        [ 0.8365],\n",
      "        [ 0.8201],\n",
      "        [-1.0401],\n",
      "        [ 0.1593],\n",
      "        [-1.1698],\n",
      "        [-1.1784],\n",
      "        [ 0.8316],\n",
      "        [-1.1336],\n",
      "        [ 0.8058],\n",
      "        [ 0.8365],\n",
      "        [-1.2012],\n",
      "        [ 0.4933],\n",
      "        [-0.5695],\n",
      "        [-1.1916],\n",
      "        [ 0.8333],\n",
      "        [-0.5695],\n",
      "        [-0.8866],\n",
      "        [-1.1999],\n",
      "        [ 0.8058],\n",
      "        [-1.1970],\n",
      "        [ 0.7608],\n",
      "        [-0.6329],\n",
      "        [ 0.7608],\n",
      "        [ 0.8365],\n",
      "        [ 0.8361]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 84/10000,\n",
      " train_loss: 0.0568,\n",
      " train_mae: 0.1981,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2025],\n",
      "        [-1.2025],\n",
      "        [ 0.8060],\n",
      "        [ 0.7954],\n",
      "        [ 0.3257],\n",
      "        [ 0.7731],\n",
      "        [ 0.8277],\n",
      "        [ 0.8302],\n",
      "        [ 0.8277],\n",
      "        [ 0.8223],\n",
      "        [-1.2242],\n",
      "        [-1.1499],\n",
      "        [ 0.7890],\n",
      "        [-0.0721],\n",
      "        [ 0.8291],\n",
      "        [-0.0721],\n",
      "        [ 0.7890],\n",
      "        [-0.9674],\n",
      "        [-0.2959],\n",
      "        [ 0.8262],\n",
      "        [ 0.8171],\n",
      "        [ 0.8199],\n",
      "        [ 0.8360],\n",
      "        [ 0.8360],\n",
      "        [ 0.8336],\n",
      "        [-0.2959],\n",
      "        [-1.2059],\n",
      "        [ 0.8336],\n",
      "        [-1.2196],\n",
      "        [ 0.8302],\n",
      "        [-1.2134],\n",
      "        [ 0.7890],\n",
      "        [-0.0721],\n",
      "        [ 0.8171],\n",
      "        [ 0.7816],\n",
      "        [ 0.8360],\n",
      "        [ 0.8171],\n",
      "        [-1.0514],\n",
      "        [ 0.1406],\n",
      "        [-1.1892],\n",
      "        [-1.1987],\n",
      "        [ 0.8302],\n",
      "        [-1.1499],\n",
      "        [ 0.8011],\n",
      "        [ 0.8360],\n",
      "        [-1.2246],\n",
      "        [ 0.4746],\n",
      "        [-0.5769],\n",
      "        [-1.2134],\n",
      "        [ 0.8321],\n",
      "        [-0.5769],\n",
      "        [-0.8940],\n",
      "        [-1.2230],\n",
      "        [ 0.8011],\n",
      "        [-1.2196],\n",
      "        [ 0.7522],\n",
      "        [-0.6397],\n",
      "        [ 0.7522],\n",
      "        [ 0.8360],\n",
      "        [ 0.8355]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 85/10000,\n",
      " train_loss: 0.0549,\n",
      " train_mae: 0.1959,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2258],\n",
      "        [-1.2258],\n",
      "        [ 0.8000],\n",
      "        [ 0.7882],\n",
      "        [ 0.3011],\n",
      "        [ 0.7636],\n",
      "        [ 0.8247],\n",
      "        [ 0.8277],\n",
      "        [ 0.8247],\n",
      "        [ 0.8184],\n",
      "        [-1.2503],\n",
      "        [-1.1687],\n",
      "        [ 0.7811],\n",
      "        [-0.0931],\n",
      "        [ 0.8263],\n",
      "        [-0.0931],\n",
      "        [ 0.7811],\n",
      "        [-0.9793],\n",
      "        [-0.3129],\n",
      "        [ 0.8229],\n",
      "        [ 0.8125],\n",
      "        [ 0.8157],\n",
      "        [ 0.8346],\n",
      "        [ 0.8346],\n",
      "        [ 0.8316],\n",
      "        [-0.3129],\n",
      "        [-1.2295],\n",
      "        [ 0.8316],\n",
      "        [-1.2450],\n",
      "        [ 0.8277],\n",
      "        [-1.2380],\n",
      "        [ 0.7811],\n",
      "        [-0.0931],\n",
      "        [ 0.8125],\n",
      "        [ 0.7729],\n",
      "        [ 0.8346],\n",
      "        [ 0.8125],\n",
      "        [-1.0654],\n",
      "        [ 0.1168],\n",
      "        [-1.2111],\n",
      "        [-1.2215],\n",
      "        [ 0.8277],\n",
      "        [-1.1687],\n",
      "        [ 0.7945],\n",
      "        [ 0.8346],\n",
      "        [-1.2508],\n",
      "        [ 0.4516],\n",
      "        [-0.5891],\n",
      "        [-1.2380],\n",
      "        [ 0.8299],\n",
      "        [-0.5891],\n",
      "        [-0.9049],\n",
      "        [-1.2490],\n",
      "        [ 0.7945],\n",
      "        [-1.2450],\n",
      "        [ 0.7409],\n",
      "        [-0.6512],\n",
      "        [ 0.7409],\n",
      "        [ 0.8346],\n",
      "        [ 0.8340]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 86/10000,\n",
      " train_loss: 0.0531,\n",
      " train_mae: 0.1928,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2381],\n",
      "        [-1.2381],\n",
      "        [ 0.7976],\n",
      "        [ 0.7846],\n",
      "        [ 0.2816],\n",
      "        [ 0.7576],\n",
      "        [ 0.8258],\n",
      "        [ 0.8293],\n",
      "        [ 0.8258],\n",
      "        [ 0.8185],\n",
      "        [-1.2660],\n",
      "        [-1.1763],\n",
      "        [ 0.7767],\n",
      "        [-0.1070],\n",
      "        [ 0.8276],\n",
      "        [-0.1070],\n",
      "        [ 0.7767],\n",
      "        [-0.9802],\n",
      "        [-0.3217],\n",
      "        [ 0.8237],\n",
      "        [ 0.8118],\n",
      "        [ 0.8154],\n",
      "        [ 0.8375],\n",
      "        [ 0.8375],\n",
      "        [ 0.8339],\n",
      "        [-0.3217],\n",
      "        [-1.2422],\n",
      "        [ 0.8339],\n",
      "        [-1.2598],\n",
      "        [ 0.8293],\n",
      "        [-1.2518],\n",
      "        [ 0.7767],\n",
      "        [-0.1070],\n",
      "        [ 0.8118],\n",
      "        [ 0.7677],\n",
      "        [ 0.8375],\n",
      "        [ 0.8118],\n",
      "        [-1.0683],\n",
      "        [ 0.0989],\n",
      "        [-1.2219],\n",
      "        [-1.2334],\n",
      "        [ 0.8293],\n",
      "        [-1.1763],\n",
      "        [ 0.7915],\n",
      "        [ 0.8375],\n",
      "        [-1.2666],\n",
      "        [ 0.4327],\n",
      "        [-0.5919],\n",
      "        [-1.2518],\n",
      "        [ 0.8319],\n",
      "        [-0.5919],\n",
      "        [-0.9051],\n",
      "        [-1.2645],\n",
      "        [ 0.7915],\n",
      "        [-1.2598],\n",
      "        [ 0.7330],\n",
      "        [-0.6530],\n",
      "        [ 0.7330],\n",
      "        [ 0.8375],\n",
      "        [ 0.8368]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 87/10000,\n",
      " train_loss: 0.0513,\n",
      " train_mae: 0.1886,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2341],\n",
      "        [-1.2341],\n",
      "        [ 0.8010],\n",
      "        [ 0.7866],\n",
      "        [ 0.2715],\n",
      "        [ 0.7571],\n",
      "        [ 0.8328],\n",
      "        [ 0.8369],\n",
      "        [ 0.8328],\n",
      "        [ 0.8245],\n",
      "        [-1.2660],\n",
      "        [-1.1670],\n",
      "        [ 0.7779],\n",
      "        [-0.1084],\n",
      "        [ 0.8350],\n",
      "        [-0.1084],\n",
      "        [ 0.7779],\n",
      "        [-0.9642],\n",
      "        [-0.3166],\n",
      "        [ 0.8304],\n",
      "        [ 0.8168],\n",
      "        [ 0.8209],\n",
      "        [ 0.8468],\n",
      "        [ 0.8468],\n",
      "        [ 0.8424],\n",
      "        [-0.3166],\n",
      "        [-1.2387],\n",
      "        [ 0.8424],\n",
      "        [-1.2586],\n",
      "        [ 0.8369],\n",
      "        [-1.2494],\n",
      "        [ 0.7779],\n",
      "        [-0.1084],\n",
      "        [ 0.8168],\n",
      "        [ 0.7681],\n",
      "        [ 0.8468],\n",
      "        [ 0.8168],\n",
      "        [-1.0541],\n",
      "        [ 0.0921],\n",
      "        [-1.2162],\n",
      "        [-1.2289],\n",
      "        [ 0.8369],\n",
      "        [-1.1670],\n",
      "        [ 0.7942],\n",
      "        [ 0.8468],\n",
      "        [-1.2667],\n",
      "        [ 0.4218],\n",
      "        [-0.5793],\n",
      "        [-1.2494],\n",
      "        [ 0.8400],\n",
      "        [-0.5793],\n",
      "        [-0.8886],\n",
      "        [-1.2641],\n",
      "        [ 0.7942],\n",
      "        [-1.2586],\n",
      "        [ 0.7307],\n",
      "        [-0.6391],\n",
      "        [ 0.7307],\n",
      "        [ 0.8468],\n",
      "        [ 0.8458]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 88/10000,\n",
      " train_loss: 0.0492,\n",
      " train_mae: 0.1834,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2196],\n",
      "        [-1.2196],\n",
      "        [ 0.8071],\n",
      "        [ 0.7912],\n",
      "        [ 0.2665],\n",
      "        [ 0.7593],\n",
      "        [ 0.8431],\n",
      "        [ 0.8478],\n",
      "        [ 0.8431],\n",
      "        [ 0.8335],\n",
      "        [-1.2561],\n",
      "        [-1.1467],\n",
      "        [ 0.7818],\n",
      "        [-0.1026],\n",
      "        [ 0.8456],\n",
      "        [-0.1026],\n",
      "        [ 0.7818],\n",
      "        [-0.9375],\n",
      "        [-0.3033],\n",
      "        [ 0.8403],\n",
      "        [ 0.8248],\n",
      "        [ 0.8294],\n",
      "        [ 0.8596],\n",
      "        [ 0.8596],\n",
      "        [ 0.8543],\n",
      "        [-0.3033],\n",
      "        [-1.2247],\n",
      "        [ 0.8543],\n",
      "        [-1.2474],\n",
      "        [ 0.8478],\n",
      "        [-1.2369],\n",
      "        [ 0.7818],\n",
      "        [-0.1026],\n",
      "        [ 0.8248],\n",
      "        [ 0.7712],\n",
      "        [ 0.8596],\n",
      "        [ 0.8248],\n",
      "        [-1.0290],\n",
      "        [ 0.0914],\n",
      "        [-1.1998],\n",
      "        [-1.2138],\n",
      "        [ 0.8478],\n",
      "        [-1.1467],\n",
      "        [ 0.7996],\n",
      "        [ 0.8596],\n",
      "        [-1.2570],\n",
      "        [ 0.4151],\n",
      "        [-0.5576],\n",
      "        [-1.2369],\n",
      "        [ 0.8515],\n",
      "        [-0.5576],\n",
      "        [-0.8616],\n",
      "        [-1.2539],\n",
      "        [ 0.7996],\n",
      "        [-1.2474],\n",
      "        [ 0.7310],\n",
      "        [-0.6158],\n",
      "        [ 0.7310],\n",
      "        [ 0.8596],\n",
      "        [ 0.8585]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 89/10000,\n",
      " train_loss: 0.0469,\n",
      " train_mae: 0.1778,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2066],\n",
      "        [-1.2066],\n",
      "        [ 0.8103],\n",
      "        [ 0.7927],\n",
      "        [ 0.2570],\n",
      "        [ 0.7579],\n",
      "        [ 0.8511],\n",
      "        [ 0.8567],\n",
      "        [ 0.8511],\n",
      "        [ 0.8400],\n",
      "        [-1.2484],\n",
      "        [-1.1279],\n",
      "        [ 0.7823],\n",
      "        [-0.1011],\n",
      "        [ 0.8541],\n",
      "        [-0.1011],\n",
      "        [ 0.7823],\n",
      "        [-0.9129],\n",
      "        [-0.2942],\n",
      "        [ 0.8479],\n",
      "        [ 0.8301],\n",
      "        [ 0.8354],\n",
      "        [ 0.8709],\n",
      "        [ 0.8709],\n",
      "        [ 0.8644],\n",
      "        [-0.2942],\n",
      "        [-1.2124],\n",
      "        [ 0.8644],\n",
      "        [-1.2382],\n",
      "        [ 0.8567],\n",
      "        [-1.2261],\n",
      "        [ 0.7823],\n",
      "        [-0.1011],\n",
      "        [ 0.8301],\n",
      "        [ 0.7708],\n",
      "        [ 0.8709],\n",
      "        [ 0.8301],\n",
      "        [-1.0055],\n",
      "        [ 0.0863],\n",
      "        [-1.1849],\n",
      "        [-1.2002],\n",
      "        [ 0.8567],\n",
      "        [-1.1279],\n",
      "        [ 0.8020],\n",
      "        [ 0.8709],\n",
      "        [-1.2495],\n",
      "        [ 0.4039],\n",
      "        [-0.5394],\n",
      "        [-1.2261],\n",
      "        [ 0.8610],\n",
      "        [-0.5394],\n",
      "        [-0.8371],\n",
      "        [-1.2458],\n",
      "        [ 0.8020],\n",
      "        [-1.2382],\n",
      "        [ 0.7276],\n",
      "        [-0.5959],\n",
      "        [ 0.7276],\n",
      "        [ 0.8709],\n",
      "        [ 0.8695]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 90/10000,\n",
      " train_loss: 0.0446,\n",
      " train_mae: 0.1721,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2043],\n",
      "        [-1.2043],\n",
      "        [ 0.8060],\n",
      "        [ 0.7864],\n",
      "        [ 0.2358],\n",
      "        [ 0.7481],\n",
      "        [ 0.8530],\n",
      "        [ 0.8596],\n",
      "        [ 0.8530],\n",
      "        [ 0.8400],\n",
      "        [-1.2518],\n",
      "        [-1.1196],\n",
      "        [ 0.7749],\n",
      "        [-0.1124],\n",
      "        [ 0.8565],\n",
      "        [-0.1124],\n",
      "        [ 0.7749],\n",
      "        [-0.9001],\n",
      "        [-0.2983],\n",
      "        [ 0.8492],\n",
      "        [ 0.8286],\n",
      "        [ 0.8346],\n",
      "        [ 0.8770],\n",
      "        [ 0.8770],\n",
      "        [ 0.8689],\n",
      "        [-0.2983],\n",
      "        [-1.2106],\n",
      "        [ 0.8689],\n",
      "        [-1.2399],\n",
      "        [ 0.8596],\n",
      "        [-1.2260],\n",
      "        [ 0.7749],\n",
      "        [-0.1124],\n",
      "        [ 0.8286],\n",
      "        [ 0.7622],\n",
      "        [ 0.8770],\n",
      "        [ 0.8286],\n",
      "        [-0.9933],\n",
      "        [ 0.0689],\n",
      "        [-1.1805],\n",
      "        [-1.1972],\n",
      "        [ 0.8596],\n",
      "        [-1.1196],\n",
      "        [ 0.7967],\n",
      "        [ 0.8770],\n",
      "        [-1.2531],\n",
      "        [ 0.3815],\n",
      "        [-0.5344],\n",
      "        [-1.2260],\n",
      "        [ 0.8648],\n",
      "        [-0.5344],\n",
      "        [-0.8248],\n",
      "        [-1.2487],\n",
      "        [ 0.7967],\n",
      "        [-1.2399],\n",
      "        [ 0.7153],\n",
      "        [-0.5891],\n",
      "        [ 0.7153],\n",
      "        [ 0.8770],\n",
      "        [ 0.8752]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 91/10000,\n",
      " train_loss: 0.0420,\n",
      " train_mae: 0.1662,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2117],\n",
      "        [-1.2117],\n",
      "        [ 0.7945],\n",
      "        [ 0.7724],\n",
      "        [ 0.2036],\n",
      "        [ 0.7300],\n",
      "        [ 0.8490],\n",
      "        [ 0.8569],\n",
      "        [ 0.8490],\n",
      "        [ 0.8337],\n",
      "        [-1.2657],\n",
      "        [-1.1212],\n",
      "        [ 0.7596],\n",
      "        [-0.1355],\n",
      "        [ 0.8532],\n",
      "        [-0.1355],\n",
      "        [ 0.7596],\n",
      "        [-0.8981],\n",
      "        [-0.3142],\n",
      "        [ 0.8445],\n",
      "        [ 0.8204],\n",
      "        [ 0.8274],\n",
      "        [ 0.8785],\n",
      "        [ 0.8785],\n",
      "        [ 0.8684],\n",
      "        [-0.3142],\n",
      "        [-1.2187],\n",
      "        [ 0.8684],\n",
      "        [-1.2517],\n",
      "        [ 0.8569],\n",
      "        [-1.2358],\n",
      "        [ 0.7596],\n",
      "        [-0.1355],\n",
      "        [ 0.8204],\n",
      "        [ 0.7455],\n",
      "        [ 0.8785],\n",
      "        [ 0.8204],\n",
      "        [-0.9915],\n",
      "        [ 0.0401],\n",
      "        [-1.1858],\n",
      "        [-1.2040],\n",
      "        [ 0.8569],\n",
      "        [-1.1212],\n",
      "        [ 0.7840],\n",
      "        [ 0.8785],\n",
      "        [-1.2673],\n",
      "        [ 0.3485],\n",
      "        [-0.5413],\n",
      "        [-1.2358],\n",
      "        [ 0.8632],\n",
      "        [-0.5413],\n",
      "        [-0.8237],\n",
      "        [-1.2620],\n",
      "        [ 0.7840],\n",
      "        [-1.2517],\n",
      "        [ 0.6942],\n",
      "        [-0.5941],\n",
      "        [ 0.6942],\n",
      "        [ 0.8785],\n",
      "        [ 0.8762]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 92/10000,\n",
      " train_loss: 0.0392,\n",
      " train_mae: 0.1595,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2196],\n",
      "        [-1.2196],\n",
      "        [ 0.7801],\n",
      "        [ 0.7552],\n",
      "        [ 0.1690],\n",
      "        [ 0.7083],\n",
      "        [ 0.8434],\n",
      "        [ 0.8528],\n",
      "        [ 0.8434],\n",
      "        [ 0.8252],\n",
      "        [-1.2810],\n",
      "        [-1.1228],\n",
      "        [ 0.7410],\n",
      "        [-0.1599],\n",
      "        [ 0.8484],\n",
      "        [-0.1599],\n",
      "        [ 0.7410],\n",
      "        [-0.8966],\n",
      "        [-0.3314],\n",
      "        [ 0.8379],\n",
      "        [ 0.8097],\n",
      "        [ 0.8178],\n",
      "        [ 0.8796],\n",
      "        [ 0.8796],\n",
      "        [ 0.8669],\n",
      "        [-0.3314],\n",
      "        [-1.2273],\n",
      "        [ 0.8669],\n",
      "        [-1.2645],\n",
      "        [ 0.8528],\n",
      "        [-1.2465],\n",
      "        [ 0.7410],\n",
      "        [-0.1599],\n",
      "        [ 0.8097],\n",
      "        [ 0.7253],\n",
      "        [ 0.8796],\n",
      "        [ 0.8097],\n",
      "        [-0.9899],\n",
      "        [ 0.0095],\n",
      "        [-1.1914],\n",
      "        [-1.2111],\n",
      "        [ 0.8528],\n",
      "        [-1.1228],\n",
      "        [ 0.7682],\n",
      "        [ 0.8796],\n",
      "        [-1.2829],\n",
      "        [ 0.3125],\n",
      "        [-0.5492],\n",
      "        [-1.2465],\n",
      "        [ 0.8606],\n",
      "        [-0.5492],\n",
      "        [-0.8231],\n",
      "        [-1.2766],\n",
      "        [ 0.7682],\n",
      "        [-1.2645],\n",
      "        [ 0.6694],\n",
      "        [-0.6000],\n",
      "        [ 0.6694],\n",
      "        [ 0.8796],\n",
      "        [ 0.8767]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 93/10000,\n",
      " train_loss: 0.0365,\n",
      " train_mae: 0.1518,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2177],\n",
      "        [-1.2177],\n",
      "        [ 0.7680],\n",
      "        [ 0.7403],\n",
      "        [ 0.1422],\n",
      "        [ 0.6890],\n",
      "        [ 0.8407],\n",
      "        [ 0.8520],\n",
      "        [ 0.8407],\n",
      "        [ 0.8194],\n",
      "        [-1.2881],\n",
      "        [-1.1137],\n",
      "        [ 0.7246],\n",
      "        [-0.1744],\n",
      "        [ 0.8466],\n",
      "        [-0.1744],\n",
      "        [ 0.7246],\n",
      "        [-0.8839],\n",
      "        [-0.3380],\n",
      "        [ 0.8343],\n",
      "        [ 0.8015],\n",
      "        [ 0.8109],\n",
      "        [ 0.8850],\n",
      "        [ 0.8850],\n",
      "        [ 0.8691],\n",
      "        [-0.3380],\n",
      "        [-1.2263],\n",
      "        [ 0.8691],\n",
      "        [-1.2686],\n",
      "        [ 0.8520],\n",
      "        [-1.2478],\n",
      "        [ 0.7246],\n",
      "        [-0.1744],\n",
      "        [ 0.8015],\n",
      "        [ 0.7075],\n",
      "        [ 0.8850],\n",
      "        [ 0.8015],\n",
      "        [-0.9772],\n",
      "        [-0.0121],\n",
      "        [-1.1869],\n",
      "        [-1.2083],\n",
      "        [ 0.8520],\n",
      "        [-1.1137],\n",
      "        [ 0.7547],\n",
      "        [ 0.8850],\n",
      "        [-1.2903],\n",
      "        [ 0.2830],\n",
      "        [-0.5462],\n",
      "        [-1.2478],\n",
      "        [ 0.8613],\n",
      "        [-0.5462],\n",
      "        [-0.8114],\n",
      "        [-1.2828],\n",
      "        [ 0.7547],\n",
      "        [-1.2686],\n",
      "        [ 0.6473],\n",
      "        [-0.5949],\n",
      "        [ 0.6473],\n",
      "        [ 0.8850],\n",
      "        [ 0.8814]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 94/10000,\n",
      " train_loss: 0.0337,\n",
      " train_mae: 0.1431,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2034],\n",
      "        [-1.2034],\n",
      "        [ 0.7601],\n",
      "        [ 0.7297],\n",
      "        [ 0.1269],\n",
      "        [ 0.6744],\n",
      "        [ 0.8425],\n",
      "        [ 0.8557],\n",
      "        [ 0.8425],\n",
      "        [ 0.8178],\n",
      "        [-1.2847],\n",
      "        [-1.0912],\n",
      "        [ 0.7126],\n",
      "        [-0.1754],\n",
      "        [ 0.8494],\n",
      "        [-0.1754],\n",
      "        [ 0.7126],\n",
      "        [-0.8571],\n",
      "        [-0.3307],\n",
      "        [ 0.8349],\n",
      "        [ 0.7975],\n",
      "        [ 0.8081],\n",
      "        [ 0.8960],\n",
      "        [ 0.8960],\n",
      "        [ 0.8762],\n",
      "        [-0.3307],\n",
      "        [-1.2130],\n",
      "        [ 0.8762],\n",
      "        [-1.2613],\n",
      "        [ 0.8557],\n",
      "        [-1.2373],\n",
      "        [ 0.7126],\n",
      "        [-0.1754],\n",
      "        [ 0.7975],\n",
      "        [ 0.6942],\n",
      "        [ 0.8960],\n",
      "        [ 0.7975],\n",
      "        [-0.9505],\n",
      "        [-0.0210],\n",
      "        [-1.1694],\n",
      "        [-1.1930],\n",
      "        [ 0.8557],\n",
      "        [-1.0912],\n",
      "        [ 0.7455],\n",
      "        [ 0.8960],\n",
      "        [-1.2875],\n",
      "        [ 0.2635],\n",
      "        [-0.5289],\n",
      "        [-1.2373],\n",
      "        [ 0.8669],\n",
      "        [-0.5289],\n",
      "        [-0.7856],\n",
      "        [-1.2782],\n",
      "        [ 0.7455],\n",
      "        [-1.2613],\n",
      "        [ 0.6303],\n",
      "        [-0.5757],\n",
      "        [ 0.6303],\n",
      "        [ 0.8960],\n",
      "        [ 0.8914]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 95/10000,\n",
      " train_loss: 0.0306,\n",
      " train_mae: 0.1341,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1837],\n",
      "        [-1.1837],\n",
      "        [ 0.7532],\n",
      "        [ 0.7202],\n",
      "        [ 0.1173],\n",
      "        [ 0.6614],\n",
      "        [ 0.8456],\n",
      "        [ 0.8610],\n",
      "        [ 0.8456],\n",
      "        [ 0.8173],\n",
      "        [-1.2778],\n",
      "        [-1.0625],\n",
      "        [ 0.7019],\n",
      "        [-0.1702],\n",
      "        [ 0.8536],\n",
      "        [-0.1702],\n",
      "        [ 0.7019],\n",
      "        [-0.8243],\n",
      "        [-0.3172],\n",
      "        [ 0.8369],\n",
      "        [ 0.7945],\n",
      "        [ 0.8063],\n",
      "        [ 0.9097],\n",
      "        [ 0.9097],\n",
      "        [ 0.8854],\n",
      "        [-0.3172],\n",
      "        [-1.1944],\n",
      "        [ 0.8854],\n",
      "        [-1.2498],\n",
      "        [ 0.8610],\n",
      "        [-1.2220],\n",
      "        [ 0.7019],\n",
      "        [-0.1702],\n",
      "        [ 0.7945],\n",
      "        [ 0.6823],\n",
      "        [ 0.9097],\n",
      "        [ 0.7945],\n",
      "        [-0.9178],\n",
      "        [-0.0238],\n",
      "        [-1.1463],\n",
      "        [-1.1721],\n",
      "        [ 0.8610],\n",
      "        [-1.0625],\n",
      "        [ 0.7373],\n",
      "        [ 0.9097],\n",
      "        [-1.2812],\n",
      "        [ 0.2489],\n",
      "        [-0.5057],\n",
      "        [-1.2220],\n",
      "        [ 0.8742],\n",
      "        [-0.5057],\n",
      "        [-0.7537],\n",
      "        [-1.2700],\n",
      "        [ 0.7373],\n",
      "        [-1.2498],\n",
      "        [ 0.6153],\n",
      "        [-0.5505],\n",
      "        [ 0.6153],\n",
      "        [ 0.9097],\n",
      "        [ 0.9039]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 96/10000,\n",
      " train_loss: 0.0278,\n",
      " train_mae: 0.1253,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1696],\n",
      "        [-1.1696],\n",
      "        [ 0.7418],\n",
      "        [ 0.7062],\n",
      "        [ 0.1032],\n",
      "        [ 0.6438],\n",
      "        [ 0.8450],\n",
      "        [ 0.8629],\n",
      "        [ 0.8450],\n",
      "        [ 0.8128],\n",
      "        [-1.2777],\n",
      "        [-1.0397],\n",
      "        [ 0.6867],\n",
      "        [-0.1706],\n",
      "        [ 0.8543],\n",
      "        [-0.1706],\n",
      "        [ 0.6867],\n",
      "        [-0.7982],\n",
      "        [-0.3100],\n",
      "        [ 0.8350],\n",
      "        [ 0.7873],\n",
      "        [ 0.8005],\n",
      "        [ 0.9215],\n",
      "        [ 0.9215],\n",
      "        [ 0.8917],\n",
      "        [-0.3100],\n",
      "        [-1.1815],\n",
      "        [ 0.8917],\n",
      "        [-1.2445],\n",
      "        [ 0.8629],\n",
      "        [-1.2124],\n",
      "        [ 0.6867],\n",
      "        [-0.1706],\n",
      "        [ 0.7873],\n",
      "        [ 0.6659],\n",
      "        [ 0.9215],\n",
      "        [ 0.7873],\n",
      "        [-0.8915],\n",
      "        [-0.0316],\n",
      "        [-1.1287],\n",
      "        [-1.1569],\n",
      "        [ 0.8629],\n",
      "        [-1.0397],\n",
      "        [ 0.7246],\n",
      "        [ 0.9215],\n",
      "        [-1.2819],\n",
      "        [ 0.2301],\n",
      "        [-0.4894],\n",
      "        [-1.2124],\n",
      "        [ 0.8784],\n",
      "        [-0.4894],\n",
      "        [-0.7289],\n",
      "        [-1.2683],\n",
      "        [ 0.7246],\n",
      "        [-1.2445],\n",
      "        [ 0.5957],\n",
      "        [-0.5322],\n",
      "        [ 0.5957],\n",
      "        [ 0.9215],\n",
      "        [ 0.9143]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 97/10000,\n",
      " train_loss: 0.0251,\n",
      " train_mae: 0.1164,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1666],\n",
      "        [-1.1666],\n",
      "        [ 0.7238],\n",
      "        [ 0.6854],\n",
      "        [ 0.0803],\n",
      "        [ 0.6193],\n",
      "        [ 0.8387],\n",
      "        [ 0.8593],\n",
      "        [ 0.8387],\n",
      "        [ 0.8021],\n",
      "        [-1.2889],\n",
      "        [-1.0288],\n",
      "        [ 0.6646],\n",
      "        [-0.1818],\n",
      "        [ 0.8493],\n",
      "        [-0.1818],\n",
      "        [ 0.6646],\n",
      "        [-0.7854],\n",
      "        [-0.3146],\n",
      "        [ 0.8273],\n",
      "        [ 0.7736],\n",
      "        [ 0.7883],\n",
      "        [ 0.9293],\n",
      "        [ 0.9293],\n",
      "        [ 0.8932],\n",
      "        [-0.3146],\n",
      "        [-1.1795],\n",
      "        [ 0.8932],\n",
      "        [-1.2501],\n",
      "        [ 0.8593],\n",
      "        [-1.2138],\n",
      "        [ 0.6646],\n",
      "        [-0.1818],\n",
      "        [ 0.7736],\n",
      "        [ 0.6425],\n",
      "        [ 0.9293],\n",
      "        [ 0.7736],\n",
      "        [-0.8780],\n",
      "        [-0.0491],\n",
      "        [-1.1224],\n",
      "        [-1.1527],\n",
      "        [ 0.8593],\n",
      "        [-1.0288],\n",
      "        [ 0.7052],\n",
      "        [ 0.9293],\n",
      "        [-1.2939],\n",
      "        [ 0.2033],\n",
      "        [-0.4859],\n",
      "        [-1.2138],\n",
      "        [ 0.8774],\n",
      "        [-0.4859],\n",
      "        [-0.7173],\n",
      "        [-1.2777],\n",
      "        [ 0.7052],\n",
      "        [-1.2501],\n",
      "        [ 0.5691],\n",
      "        [-0.5269],\n",
      "        [ 0.5691],\n",
      "        [ 0.9293],\n",
      "        [ 0.9204]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 98/10000,\n",
      " train_loss: 0.0226,\n",
      " train_mae: 0.1083,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1697],\n",
      "        [-1.1697],\n",
      "        [ 0.7041],\n",
      "        [ 0.6630],\n",
      "        [ 0.0556],\n",
      "        [ 0.5934],\n",
      "        [ 0.8304],\n",
      "        [ 0.8539],\n",
      "        [ 0.8304],\n",
      "        [ 0.7896],\n",
      "        [-1.3063],\n",
      "        [-1.0247],\n",
      "        [ 0.6409],\n",
      "        [-0.1966],\n",
      "        [ 0.8425],\n",
      "        [-0.1966],\n",
      "        [ 0.6409],\n",
      "        [-0.7798],\n",
      "        [-0.3239],\n",
      "        [ 0.8176],\n",
      "        [ 0.7581],\n",
      "        [ 0.7743],\n",
      "        [ 0.9362],\n",
      "        [ 0.9362],\n",
      "        [ 0.8932],\n",
      "        [-0.3239],\n",
      "        [-1.1838],\n",
      "        [ 0.8932],\n",
      "        [-1.2618],\n",
      "        [ 0.8539],\n",
      "        [-1.2213],\n",
      "        [ 0.6409],\n",
      "        [-0.1966],\n",
      "        [ 0.7581],\n",
      "        [ 0.6177],\n",
      "        [ 0.9362],\n",
      "        [ 0.7581],\n",
      "        [-0.8717],\n",
      "        [-0.0692],\n",
      "        [-1.1225],\n",
      "        [-1.1548],\n",
      "        [ 0.8539],\n",
      "        [-1.0247],\n",
      "        [ 0.6841],\n",
      "        [ 0.9362],\n",
      "        [-1.3122],\n",
      "        [ 0.1753],\n",
      "        [-0.4883],\n",
      "        [-1.2213],\n",
      "        [ 0.8748],\n",
      "        [-0.4883],\n",
      "        [-0.7129],\n",
      "        [-1.2934],\n",
      "        [ 0.6841],\n",
      "        [-1.2618],\n",
      "        [ 0.5414],\n",
      "        [-0.5279],\n",
      "        [ 0.5414],\n",
      "        [ 0.9362],\n",
      "        [ 0.9255]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 99/10000,\n",
      " train_loss: 0.0204,\n",
      " train_mae: 0.1013,\n",
      " epoch_time_duration: 0.0133\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1694],\n",
      "        [-1.1694],\n",
      "        [ 0.6925],\n",
      "        [ 0.6494],\n",
      "        [ 0.0427],\n",
      "        [ 0.5772],\n",
      "        [ 0.8286],\n",
      "        [ 0.8546],\n",
      "        [ 0.8286],\n",
      "        [ 0.7839],\n",
      "        [-1.3208],\n",
      "        [-1.0170],\n",
      "        [ 0.6264],\n",
      "        [-0.2015],\n",
      "        [ 0.8419],\n",
      "        [-0.2015],\n",
      "        [ 0.6264],\n",
      "        [-0.7694],\n",
      "        [-0.3243],\n",
      "        [ 0.8145],\n",
      "        [ 0.7500],\n",
      "        [ 0.7674],\n",
      "        [ 0.9484],\n",
      "        [ 0.9484],\n",
      "        [ 0.8987],\n",
      "        [-0.3243],\n",
      "        [-1.1845],\n",
      "        [ 0.8987],\n",
      "        [-1.2702],\n",
      "        [ 0.8546],\n",
      "        [-1.2254],\n",
      "        [ 0.6264],\n",
      "        [-0.2015],\n",
      "        [ 0.7500],\n",
      "        [ 0.6023],\n",
      "        [ 0.9484],\n",
      "        [ 0.7500],\n",
      "        [-0.8613],\n",
      "        [-0.0783],\n",
      "        [-1.1191],\n",
      "        [-1.1535],\n",
      "        [ 0.8546],\n",
      "        [-1.0170],\n",
      "        [ 0.6715],\n",
      "        [ 0.9484],\n",
      "        [-1.3277],\n",
      "        [ 0.1593],\n",
      "        [-0.4836],\n",
      "        [-1.2254],\n",
      "        [ 0.8779],\n",
      "        [-0.4836],\n",
      "        [-0.7032],\n",
      "        [-1.3059],\n",
      "        [ 0.6715],\n",
      "        [-1.2702],\n",
      "        [ 0.5239],\n",
      "        [-0.5221],\n",
      "        [ 0.5239],\n",
      "        [ 0.9484],\n",
      "        [ 0.9358]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 100/10000,\n",
      " train_loss: 0.0183,\n",
      " train_mae: 0.0942,\n",
      " epoch_time_duration: 0.0123\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1635],\n",
      "        [-1.1635],\n",
      "        [ 0.6952],\n",
      "        [ 0.6507],\n",
      "        [ 0.0466],\n",
      "        [ 0.5769],\n",
      "        [ 0.8383],\n",
      "        [ 0.8662],\n",
      "        [ 0.8383],\n",
      "        [ 0.7908],\n",
      "        [-1.3302],\n",
      "        [-1.0030],\n",
      "        [ 0.6271],\n",
      "        [-0.1915],\n",
      "        [ 0.8526],\n",
      "        [-0.1915],\n",
      "        [ 0.6271],\n",
      "        [-0.7509],\n",
      "        [-0.3114],\n",
      "        [ 0.8232],\n",
      "        [ 0.7551],\n",
      "        [ 0.7734],\n",
      "        [ 0.9693],\n",
      "        [ 0.9693],\n",
      "        [ 0.9142],\n",
      "        [-0.3114],\n",
      "        [-1.1798],\n",
      "        [ 0.9142],\n",
      "        [-1.2734],\n",
      "        [ 0.8662],\n",
      "        [-1.2240],\n",
      "        [ 0.6271],\n",
      "        [-0.1915],\n",
      "        [ 0.7551],\n",
      "        [ 0.6025],\n",
      "        [ 0.9693],\n",
      "        [ 0.7551],\n",
      "        [-0.8435],\n",
      "        [-0.0715],\n",
      "        [-1.1099],\n",
      "        [-1.1465],\n",
      "        [ 0.8662],\n",
      "        [-1.0030],\n",
      "        [ 0.6734],\n",
      "        [ 0.9693],\n",
      "        [-1.3380],\n",
      "        [ 0.1609],\n",
      "        [-0.4675],\n",
      "        [-1.2240],\n",
      "        [ 0.8915],\n",
      "        [-0.4675],\n",
      "        [-0.6847],\n",
      "        [-1.3133],\n",
      "        [ 0.6734],\n",
      "        [-1.2734],\n",
      "        [ 0.5229],\n",
      "        [-0.5053],\n",
      "        [ 0.5229],\n",
      "        [ 0.9693],\n",
      "        [ 0.9553]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 101/10000,\n",
      " train_loss: 0.0161,\n",
      " train_mae: 0.0888,\n",
      " epoch_time_duration: 0.0206\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1631],\n",
      "        [-1.1631],\n",
      "        [ 0.7068],\n",
      "        [ 0.6611],\n",
      "        [ 0.0561],\n",
      "        [ 0.5859],\n",
      "        [ 0.8556],\n",
      "        [ 0.8851],\n",
      "        [ 0.8556],\n",
      "        [ 0.8058],\n",
      "        [-1.3440],\n",
      "        [-0.9947],\n",
      "        [ 0.6369],\n",
      "        [-0.1793],\n",
      "        [ 0.8707],\n",
      "        [-0.1793],\n",
      "        [ 0.6369],\n",
      "        [-0.7369],\n",
      "        [-0.2979],\n",
      "        [ 0.8397],\n",
      "        [ 0.7687],\n",
      "        [ 0.7876],\n",
      "        [ 0.9959],\n",
      "        [ 0.9959],\n",
      "        [ 0.9363],\n",
      "        [-0.2979],\n",
      "        [-1.1804],\n",
      "        [ 0.9363],\n",
      "        [-1.2814],\n",
      "        [ 0.8851],\n",
      "        [-1.2278],\n",
      "        [ 0.6369],\n",
      "        [-0.1793],\n",
      "        [ 0.7687],\n",
      "        [ 0.6119],\n",
      "        [ 0.9959],\n",
      "        [ 0.7687],\n",
      "        [-0.8308],\n",
      "        [-0.0607],\n",
      "        [-1.1064],\n",
      "        [-1.1450],\n",
      "        [ 0.8851],\n",
      "        [-0.9947],\n",
      "        [ 0.6844],\n",
      "        [ 0.9959],\n",
      "        [-1.3527],\n",
      "        [ 0.1693],\n",
      "        [-0.4528],\n",
      "        [-1.2278],\n",
      "        [ 0.9120],\n",
      "        [-0.4528],\n",
      "        [-0.6701],\n",
      "        [-1.3253],\n",
      "        [ 0.6844],\n",
      "        [-1.2814],\n",
      "        [ 0.5311],\n",
      "        [-0.4906],\n",
      "        [ 0.5311],\n",
      "        [ 0.9959],\n",
      "        [ 0.9806]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 102/10000,\n",
      " train_loss: 0.0138,\n",
      " train_mae: 0.0836,\n",
      " epoch_time_duration: 0.0116\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1806],\n",
      "        [-1.1806],\n",
      "        [ 0.7162],\n",
      "        [ 0.6690],\n",
      "        [ 0.0550],\n",
      "        [ 0.5916],\n",
      "        [ 0.8716],\n",
      "        [ 0.9028],\n",
      "        [ 0.8716],\n",
      "        [ 0.8193],\n",
      "        [-1.3727],\n",
      "        [-1.0057],\n",
      "        [ 0.6440],\n",
      "        [-0.1812],\n",
      "        [ 0.8876],\n",
      "        [-0.1812],\n",
      "        [ 0.6440],\n",
      "        [-0.7424],\n",
      "        [-0.3002],\n",
      "        [ 0.8549],\n",
      "        [ 0.7805],\n",
      "        [ 0.8003],\n",
      "        [ 1.0213],\n",
      "        [ 1.0213],\n",
      "        [ 0.9572],\n",
      "        [-0.3002],\n",
      "        [-1.1988],\n",
      "        [ 0.9572],\n",
      "        [-1.3055],\n",
      "        [ 0.9028],\n",
      "        [-1.2487],\n",
      "        [ 0.6440],\n",
      "        [-0.1812],\n",
      "        [ 0.7805],\n",
      "        [ 0.6182],\n",
      "        [ 1.0213],\n",
      "        [ 0.7805],\n",
      "        [-0.8379],\n",
      "        [-0.0622],\n",
      "        [-1.1213],\n",
      "        [-1.1617],\n",
      "        [ 0.9028],\n",
      "        [-1.0057],\n",
      "        [ 0.6930],\n",
      "        [ 1.0213],\n",
      "        [-1.3821],\n",
      "        [ 0.1688],\n",
      "        [-0.4557],\n",
      "        [-1.2487],\n",
      "        [ 0.9313],\n",
      "        [-0.4557],\n",
      "        [-0.6747],\n",
      "        [-1.3525],\n",
      "        [ 0.6930],\n",
      "        [-1.3055],\n",
      "        [ 0.5355],\n",
      "        [-0.4936],\n",
      "        [ 0.5355],\n",
      "        [ 1.0213],\n",
      "        [ 1.0048]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 103/10000,\n",
      " train_loss: 0.0117,\n",
      " train_mae: 0.0810,\n",
      " epoch_time_duration: 0.0109\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2102],\n",
      "        [-1.2102],\n",
      "        [ 0.7202],\n",
      "        [ 0.6711],\n",
      "        [ 0.0432],\n",
      "        [ 0.5910],\n",
      "        [ 0.8832],\n",
      "        [ 0.9162],\n",
      "        [ 0.8832],\n",
      "        [ 0.8281],\n",
      "        [-1.4112],\n",
      "        [-1.0303],\n",
      "        [ 0.6453],\n",
      "        [-0.1954],\n",
      "        [ 0.9001],\n",
      "        [-0.1954],\n",
      "        [ 0.6453],\n",
      "        [-0.7622],\n",
      "        [-0.3155],\n",
      "        [ 0.8656],\n",
      "        [ 0.7874],\n",
      "        [ 0.8082],\n",
      "        [ 1.0430],\n",
      "        [ 1.0430],\n",
      "        [ 0.9741],\n",
      "        [-0.3155],\n",
      "        [-1.2291],\n",
      "        [ 0.9741],\n",
      "        [-1.3404],\n",
      "        [ 0.9162],\n",
      "        [-1.2810],\n",
      "        [ 0.6453],\n",
      "        [-0.1954],\n",
      "        [ 0.7874],\n",
      "        [ 0.6186],\n",
      "        [ 1.0430],\n",
      "        [ 0.7874],\n",
      "        [-0.8591],\n",
      "        [-0.0753],\n",
      "        [-1.1490],\n",
      "        [-1.1907],\n",
      "        [ 0.9162],\n",
      "        [-1.0303],\n",
      "        [ 0.6961],\n",
      "        [ 1.0430],\n",
      "        [-1.4211],\n",
      "        [ 0.1585],\n",
      "        [-0.4724],\n",
      "        [-1.2810],\n",
      "        [ 0.9465],\n",
      "        [-0.4724],\n",
      "        [-0.6936],\n",
      "        [-1.3898],\n",
      "        [ 0.6961],\n",
      "        [-1.3404],\n",
      "        [ 0.5332],\n",
      "        [-0.5106],\n",
      "        [ 0.5332],\n",
      "        [ 1.0430],\n",
      "        [ 1.0251]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 104/10000,\n",
      " train_loss: 0.0101,\n",
      " train_mae: 0.0795,\n",
      " epoch_time_duration: 0.0123\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2297],\n",
      "        [-1.2297],\n",
      "        [ 0.7250],\n",
      "        [ 0.6744],\n",
      "        [ 0.0373],\n",
      "        [ 0.5921],\n",
      "        [ 0.8944],\n",
      "        [ 0.9291],\n",
      "        [ 0.8944],\n",
      "        [ 0.8369],\n",
      "        [-1.4401],\n",
      "        [-1.0445],\n",
      "        [ 0.6478],\n",
      "        [-0.2023],\n",
      "        [ 0.9121],\n",
      "        [-0.2023],\n",
      "        [ 0.6478],\n",
      "        [-0.7721],\n",
      "        [-0.3227],\n",
      "        [ 0.8760],\n",
      "        [ 0.7945],\n",
      "        [ 0.8161],\n",
      "        [ 1.0635],\n",
      "        [ 1.0635],\n",
      "        [ 0.9902],\n",
      "        [-0.3227],\n",
      "        [-1.2492],\n",
      "        [ 0.9902],\n",
      "        [-1.3653],\n",
      "        [ 0.9291],\n",
      "        [-1.3032],\n",
      "        [ 0.6478],\n",
      "        [-0.2023],\n",
      "        [ 0.7945],\n",
      "        [ 0.6204],\n",
      "        [ 1.0635],\n",
      "        [ 0.7945],\n",
      "        [-0.8703],\n",
      "        [-0.0818],\n",
      "        [-1.1664],\n",
      "        [-1.2094],\n",
      "        [ 0.9291],\n",
      "        [-1.0445],\n",
      "        [ 0.7001],\n",
      "        [ 1.0635],\n",
      "        [-1.4506],\n",
      "        [ 0.1534],\n",
      "        [-0.4802],\n",
      "        [-1.3032],\n",
      "        [ 0.9610],\n",
      "        [-0.4802],\n",
      "        [-0.7029],\n",
      "        [-1.4174],\n",
      "        [ 0.7001],\n",
      "        [-1.3653],\n",
      "        [ 0.5330],\n",
      "        [-0.5187],\n",
      "        [ 0.5330],\n",
      "        [ 1.0635],\n",
      "        [ 1.0445]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 105/10000,\n",
      " train_loss: 0.0092,\n",
      " train_mae: 0.0755,\n",
      " epoch_time_duration: 0.0090\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2254],\n",
      "        [-1.2254],\n",
      "        [ 0.7311],\n",
      "        [ 0.6797],\n",
      "        [ 0.0443],\n",
      "        [ 0.5966],\n",
      "        [ 0.9048],\n",
      "        [ 0.9407],\n",
      "        [ 0.9048],\n",
      "        [ 0.8454],\n",
      "        [-1.4479],\n",
      "        [-1.0345],\n",
      "        [ 0.6529],\n",
      "        [-0.1925],\n",
      "        [ 0.9231],\n",
      "        [-0.1925],\n",
      "        [ 0.6529],\n",
      "        [-0.7589],\n",
      "        [-0.3116],\n",
      "        [ 0.8857],\n",
      "        [ 0.8020],\n",
      "        [ 0.8241],\n",
      "        [ 1.0819],\n",
      "        [ 1.0819],\n",
      "        [ 1.0044],\n",
      "        [-0.3116],\n",
      "        [-1.2457],\n",
      "        [ 1.0044],\n",
      "        [-1.3679],\n",
      "        [ 0.9407],\n",
      "        [-1.3023],\n",
      "        [ 0.6529],\n",
      "        [-0.1925],\n",
      "        [ 0.8020],\n",
      "        [ 0.6252],\n",
      "        [ 1.0819],\n",
      "        [ 0.8020],\n",
      "        [-0.8577],\n",
      "        [-0.0734],\n",
      "        [-1.1598],\n",
      "        [-1.2043],\n",
      "        [ 0.9407],\n",
      "        [-1.0345],\n",
      "        [ 0.7058],\n",
      "        [ 1.0819],\n",
      "        [-1.4593],\n",
      "        [ 0.1592],\n",
      "        [-0.4677],\n",
      "        [-1.3023],\n",
      "        [ 0.9739],\n",
      "        [-0.4677],\n",
      "        [-0.6895],\n",
      "        [-1.4235],\n",
      "        [ 0.7058],\n",
      "        [-1.3679],\n",
      "        [ 0.5372],\n",
      "        [-0.5059],\n",
      "        [ 0.5372],\n",
      "        [ 1.0819],\n",
      "        [ 1.0617]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 106/10000,\n",
      " train_loss: 0.0082,\n",
      " train_mae: 0.0698,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2086],\n",
      "        [-1.2086],\n",
      "        [ 0.7260],\n",
      "        [ 0.6743],\n",
      "        [ 0.0476],\n",
      "        [ 0.5911],\n",
      "        [ 0.9033],\n",
      "        [ 0.9406],\n",
      "        [ 0.9033],\n",
      "        [ 0.8423],\n",
      "        [-1.4432],\n",
      "        [-1.0136],\n",
      "        [ 0.6473],\n",
      "        [-0.1831],\n",
      "        [ 0.9223],\n",
      "        [-0.1831],\n",
      "        [ 0.6473],\n",
      "        [-0.7379],\n",
      "        [-0.2991],\n",
      "        [ 0.8837],\n",
      "        [ 0.7980],\n",
      "        [ 0.8205],\n",
      "        [ 1.0892],\n",
      "        [ 1.0892],\n",
      "        [ 1.0071],\n",
      "        [-0.2991],\n",
      "        [-1.2297],\n",
      "        [ 1.0071],\n",
      "        [-1.3577],\n",
      "        [ 0.9406],\n",
      "        [-1.2886],\n",
      "        [ 0.6473],\n",
      "        [-0.1831],\n",
      "        [ 0.7980],\n",
      "        [ 0.6196],\n",
      "        [ 1.0892],\n",
      "        [ 0.7980],\n",
      "        [-0.8361],\n",
      "        [-0.0671],\n",
      "        [-1.1411],\n",
      "        [-1.1868],\n",
      "        [ 0.9406],\n",
      "        [-1.0136],\n",
      "        [ 0.7006],\n",
      "        [ 1.0892],\n",
      "        [-1.4555],\n",
      "        [ 0.1597],\n",
      "        [-0.4515],\n",
      "        [-1.2886],\n",
      "        [ 0.9751],\n",
      "        [-0.4515],\n",
      "        [-0.6694],\n",
      "        [-1.4170],\n",
      "        [ 0.7006],\n",
      "        [-1.3577],\n",
      "        [ 0.5319],\n",
      "        [-0.4889],\n",
      "        [ 0.5319],\n",
      "        [ 1.0892],\n",
      "        [ 1.0676]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 107/10000,\n",
      " train_loss: 0.0072,\n",
      " train_mae: 0.0632,\n",
      " epoch_time_duration: 0.0076\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1970],\n",
      "        [-1.1970],\n",
      "        [ 0.7030],\n",
      "        [ 0.6507],\n",
      "        [ 0.0332],\n",
      "        [ 0.5671],\n",
      "        [ 0.8848],\n",
      "        [ 0.9236],\n",
      "        [ 0.8848],\n",
      "        [ 0.8217],\n",
      "        [-1.4405],\n",
      "        [-1.0007],\n",
      "        [ 0.6235],\n",
      "        [-0.1903],\n",
      "        [ 0.9045],\n",
      "        [-0.1903],\n",
      "        [ 0.6235],\n",
      "        [-0.7287],\n",
      "        [-0.3025],\n",
      "        [ 0.8644],\n",
      "        [ 0.7763],\n",
      "        [ 0.7993],\n",
      "        [ 1.0814],\n",
      "        [ 1.0814],\n",
      "        [ 0.9936],\n",
      "        [-0.3025],\n",
      "        [-1.2185],\n",
      "        [ 0.9936],\n",
      "        [-1.3506],\n",
      "        [ 0.9236],\n",
      "        [-1.2789],\n",
      "        [ 0.6235],\n",
      "        [-0.1903],\n",
      "        [ 0.7763],\n",
      "        [ 0.5957],\n",
      "        [ 1.0814],\n",
      "        [ 0.7763],\n",
      "        [-0.8250],\n",
      "        [-0.0780],\n",
      "        [-1.1285],\n",
      "        [-1.1749],\n",
      "        [ 0.9236],\n",
      "        [-1.0007],\n",
      "        [ 0.6772],\n",
      "        [ 1.0814],\n",
      "        [-1.4536],\n",
      "        [ 0.1422],\n",
      "        [-0.4500],\n",
      "        [-1.2789],\n",
      "        [ 0.9598],\n",
      "        [-0.4500],\n",
      "        [-0.6618],\n",
      "        [-1.4127],\n",
      "        [ 0.6772],\n",
      "        [-1.3506],\n",
      "        [ 0.5081],\n",
      "        [-0.4862],\n",
      "        [ 0.5081],\n",
      "        [ 1.0814],\n",
      "        [ 1.0581]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 108/10000,\n",
      " train_loss: 0.0065,\n",
      " train_mae: 0.0599,\n",
      " epoch_time_duration: 0.0078\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1892],\n",
      "        [-1.1892],\n",
      "        [ 0.6819],\n",
      "        [ 0.6291],\n",
      "        [ 0.0190],\n",
      "        [ 0.5453],\n",
      "        [ 0.8675],\n",
      "        [ 0.9076],\n",
      "        [ 0.8675],\n",
      "        [ 0.8027],\n",
      "        [-1.4396],\n",
      "        [-0.9923],\n",
      "        [ 0.6018],\n",
      "        [-0.1987],\n",
      "        [ 0.8878],\n",
      "        [-0.1987],\n",
      "        [ 0.6018],\n",
      "        [-0.7237],\n",
      "        [-0.3078],\n",
      "        [ 0.8465],\n",
      "        [ 0.7563],\n",
      "        [ 0.7798],\n",
      "        [ 1.0733],\n",
      "        [ 1.0733],\n",
      "        [ 0.9806],\n",
      "        [-0.3078],\n",
      "        [-1.2110],\n",
      "        [ 0.9806],\n",
      "        [-1.3461],\n",
      "        [ 0.9076],\n",
      "        [-1.2725],\n",
      "        [ 0.6018],\n",
      "        [-0.1987],\n",
      "        [ 0.7563],\n",
      "        [ 0.5739],\n",
      "        [ 1.0733],\n",
      "        [ 0.7563],\n",
      "        [-0.8184],\n",
      "        [-0.0894],\n",
      "        [-1.1202],\n",
      "        [-1.1668],\n",
      "        [ 0.9076],\n",
      "        [-0.9923],\n",
      "        [ 0.6558],\n",
      "        [ 1.0733],\n",
      "        [-1.4533],\n",
      "        [ 0.1255],\n",
      "        [-0.4514],\n",
      "        [-1.2725],\n",
      "        [ 0.9453],\n",
      "        [-0.4514],\n",
      "        [-0.6581],\n",
      "        [-1.4105],\n",
      "        [ 0.6558],\n",
      "        [-1.3461],\n",
      "        [ 0.4864],\n",
      "        [-0.4867],\n",
      "        [ 0.4864],\n",
      "        [ 1.0733],\n",
      "        [ 1.0486]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 109/10000,\n",
      " train_loss: 0.0062,\n",
      " train_mae: 0.0565,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1809],\n",
      "        [-1.1809],\n",
      "        [ 0.6870],\n",
      "        [ 0.6340],\n",
      "        [ 0.0264],\n",
      "        [ 0.5500],\n",
      "        [ 0.8745],\n",
      "        [ 0.9153],\n",
      "        [ 0.8745],\n",
      "        [ 0.8088],\n",
      "        [-1.4388],\n",
      "        [-0.9810],\n",
      "        [ 0.6066],\n",
      "        [-0.1893],\n",
      "        [ 0.8952],\n",
      "        [-0.1893],\n",
      "        [ 0.6066],\n",
      "        [-0.7113],\n",
      "        [-0.2975],\n",
      "        [ 0.8532],\n",
      "        [ 0.7619],\n",
      "        [ 0.7857],\n",
      "        [ 1.0851],\n",
      "        [ 1.0851],\n",
      "        [ 0.9898],\n",
      "        [-0.2975],\n",
      "        [-1.2031],\n",
      "        [ 0.9898],\n",
      "        [-1.3418],\n",
      "        [ 0.9153],\n",
      "        [-1.2661],\n",
      "        [ 0.6066],\n",
      "        [-0.1893],\n",
      "        [ 0.7619],\n",
      "        [ 0.5786],\n",
      "        [ 1.0851],\n",
      "        [ 0.7619],\n",
      "        [-0.8061],\n",
      "        [-0.0810],\n",
      "        [-1.1105],\n",
      "        [-1.1580],\n",
      "        [ 0.9153],\n",
      "        [-0.9810],\n",
      "        [ 0.6608],\n",
      "        [ 1.0851],\n",
      "        [-1.4531],\n",
      "        [ 0.1321],\n",
      "        [-0.4401],\n",
      "        [-1.2661],\n",
      "        [ 0.9537],\n",
      "        [-0.4401],\n",
      "        [-0.6458],\n",
      "        [-1.4086],\n",
      "        [ 0.6608],\n",
      "        [-1.3418],\n",
      "        [ 0.4911],\n",
      "        [-0.4751],\n",
      "        [ 0.4911],\n",
      "        [ 1.0851],\n",
      "        [ 1.0597]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 110/10000,\n",
      " train_loss: 0.0057,\n",
      " train_mae: 0.0561,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1903],\n",
      "        [-1.1903],\n",
      "        [ 0.7041],\n",
      "        [ 0.6503],\n",
      "        [ 0.0349],\n",
      "        [ 0.5651],\n",
      "        [ 0.8945],\n",
      "        [ 0.9360],\n",
      "        [ 0.8945],\n",
      "        [ 0.8278],\n",
      "        [-1.4541],\n",
      "        [-0.9867],\n",
      "        [ 0.6225],\n",
      "        [-0.1834],\n",
      "        [ 0.9156],\n",
      "        [-0.1834],\n",
      "        [ 0.6225],\n",
      "        [-0.7126],\n",
      "        [-0.2930],\n",
      "        [ 0.8729],\n",
      "        [ 0.7802],\n",
      "        [ 0.8043],\n",
      "        [ 1.1090],\n",
      "        [ 1.1090],\n",
      "        [ 1.0119],\n",
      "        [-0.2930],\n",
      "        [-1.2130],\n",
      "        [ 1.0119],\n",
      "        [-1.3548],\n",
      "        [ 0.9360],\n",
      "        [-1.2773],\n",
      "        [ 0.6225],\n",
      "        [-0.1834],\n",
      "        [ 0.7802],\n",
      "        [ 0.5941],\n",
      "        [ 1.1090],\n",
      "        [ 0.7802],\n",
      "        [-0.8088],\n",
      "        [-0.0738],\n",
      "        [-1.1186],\n",
      "        [-1.1670],\n",
      "        [ 0.9360],\n",
      "        [-0.9867],\n",
      "        [ 0.6775],\n",
      "        [ 1.1090],\n",
      "        [-1.4688],\n",
      "        [ 0.1419],\n",
      "        [-0.4375],\n",
      "        [-1.2773],\n",
      "        [ 0.9751],\n",
      "        [-0.4375],\n",
      "        [-0.6461],\n",
      "        [-1.4231],\n",
      "        [ 0.6775],\n",
      "        [-1.3548],\n",
      "        [ 0.5055],\n",
      "        [-0.4730],\n",
      "        [ 0.5055],\n",
      "        [ 1.1090],\n",
      "        [ 1.0831]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 111/10000,\n",
      " train_loss: 0.0050,\n",
      " train_mae: 0.0579,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2195],\n",
      "        [-1.2195],\n",
      "        [ 0.7089],\n",
      "        [ 0.6537],\n",
      "        [ 0.0240],\n",
      "        [ 0.5663],\n",
      "        [ 0.9046],\n",
      "        [ 0.9473],\n",
      "        [ 0.9046],\n",
      "        [ 0.8359],\n",
      "        [-1.4856],\n",
      "        [-1.0137],\n",
      "        [ 0.6251],\n",
      "        [-0.1986],\n",
      "        [ 0.9262],\n",
      "        [-0.1986],\n",
      "        [ 0.6251],\n",
      "        [-0.7362],\n",
      "        [-0.3101],\n",
      "        [ 0.8823],\n",
      "        [ 0.7870],\n",
      "        [ 0.8118],\n",
      "        [ 1.1255],\n",
      "        [ 1.1255],\n",
      "        [ 1.0254],\n",
      "        [-0.3101],\n",
      "        [-1.2424],\n",
      "        [ 1.0254],\n",
      "        [-1.3855],\n",
      "        [ 0.9473],\n",
      "        [-1.3073],\n",
      "        [ 0.6251],\n",
      "        [-0.1986],\n",
      "        [ 0.7870],\n",
      "        [ 0.5960],\n",
      "        [ 1.1255],\n",
      "        [ 0.7870],\n",
      "        [-0.8336],\n",
      "        [-0.0869],\n",
      "        [-1.1470],\n",
      "        [-1.1959],\n",
      "        [ 0.9473],\n",
      "        [-1.0137],\n",
      "        [ 0.6816],\n",
      "        [ 1.1255],\n",
      "        [-1.5004],\n",
      "        [ 0.1331],\n",
      "        [-0.4570],\n",
      "        [-1.3073],\n",
      "        [ 0.9875],\n",
      "        [-0.4570],\n",
      "        [-0.6688],\n",
      "        [-1.4544],\n",
      "        [ 0.6816],\n",
      "        [-1.3855],\n",
      "        [ 0.5051],\n",
      "        [-0.4931],\n",
      "        [ 0.5051],\n",
      "        [ 1.1255],\n",
      "        [ 1.0987]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 112/10000,\n",
      " train_loss: 0.0047,\n",
      " train_mae: 0.0581,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2292],\n",
      "        [-1.2292],\n",
      "        [ 0.7077],\n",
      "        [ 0.6517],\n",
      "        [ 0.0176],\n",
      "        [ 0.5633],\n",
      "        [ 0.9069],\n",
      "        [ 0.9505],\n",
      "        [ 0.9069],\n",
      "        [ 0.8368],\n",
      "        [-1.4989],\n",
      "        [-1.0219],\n",
      "        [ 0.6228],\n",
      "        [-0.2054],\n",
      "        [ 0.9290],\n",
      "        [-0.2054],\n",
      "        [ 0.6228],\n",
      "        [-0.7435],\n",
      "        [-0.3170],\n",
      "        [ 0.8841],\n",
      "        [ 0.7870],\n",
      "        [ 0.8122],\n",
      "        [ 1.1335],\n",
      "        [ 1.1335],\n",
      "        [ 1.0305],\n",
      "        [-0.3170],\n",
      "        [-1.2523],\n",
      "        [ 1.0305],\n",
      "        [-1.3971],\n",
      "        [ 0.9505],\n",
      "        [-1.3179],\n",
      "        [ 0.6228],\n",
      "        [-0.2054],\n",
      "        [ 0.7870],\n",
      "        [ 0.5933],\n",
      "        [ 1.1335],\n",
      "        [ 0.7870],\n",
      "        [-0.8412],\n",
      "        [-0.0935],\n",
      "        [-1.1561],\n",
      "        [-1.2054],\n",
      "        [ 0.9505],\n",
      "        [-1.0219],\n",
      "        [ 0.6800],\n",
      "        [ 1.1335],\n",
      "        [-1.5140],\n",
      "        [ 0.1271],\n",
      "        [-0.4639],\n",
      "        [-1.3179],\n",
      "        [ 0.9917],\n",
      "        [-0.4639],\n",
      "        [-0.6759],\n",
      "        [-1.4671],\n",
      "        [ 0.6800],\n",
      "        [-1.3971],\n",
      "        [ 0.5015],\n",
      "        [-0.5001],\n",
      "        [ 0.5015],\n",
      "        [ 1.1335],\n",
      "        [ 1.1059]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 113/10000,\n",
      " train_loss: 0.0046,\n",
      " train_mae: 0.0523,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2045],\n",
      "        [-1.2045],\n",
      "        [ 0.7054],\n",
      "        [ 0.6499],\n",
      "        [ 0.0270],\n",
      "        [ 0.5624],\n",
      "        [ 0.9043],\n",
      "        [ 0.9482],\n",
      "        [ 0.9043],\n",
      "        [ 0.8341],\n",
      "        [-1.4808],\n",
      "        [-0.9960],\n",
      "        [ 0.6213],\n",
      "        [-0.1911],\n",
      "        [ 0.9265],\n",
      "        [-0.1911],\n",
      "        [ 0.6213],\n",
      "        [-0.7194],\n",
      "        [-0.3003],\n",
      "        [ 0.8815],\n",
      "        [ 0.7844],\n",
      "        [ 0.8096],\n",
      "        [ 1.1339],\n",
      "        [ 1.1339],\n",
      "        [ 1.0290],\n",
      "        [-0.3003],\n",
      "        [-1.2279],\n",
      "        [ 1.0290],\n",
      "        [-1.3757],\n",
      "        [ 0.9482],\n",
      "        [-1.2947],\n",
      "        [ 0.6213],\n",
      "        [-0.1911],\n",
      "        [ 0.7844],\n",
      "        [ 0.5921],\n",
      "        [ 1.1339],\n",
      "        [ 0.7844],\n",
      "        [-0.8161],\n",
      "        [-0.0817],\n",
      "        [-1.1307],\n",
      "        [-1.1805],\n",
      "        [ 0.9482],\n",
      "        [-0.9960],\n",
      "        [ 0.6780],\n",
      "        [ 1.1339],\n",
      "        [-1.4965],\n",
      "        [ 0.1341],\n",
      "        [-0.4443],\n",
      "        [-1.2947],\n",
      "        [ 0.9897],\n",
      "        [-0.4443],\n",
      "        [-0.6528],\n",
      "        [-1.4478],\n",
      "        [ 0.6780],\n",
      "        [-1.3757],\n",
      "        [ 0.5015],\n",
      "        [-0.4798],\n",
      "        [ 0.5015],\n",
      "        [ 1.1339],\n",
      "        [ 1.1057]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 114/10000,\n",
      " train_loss: 0.0040,\n",
      " train_mae: 0.0480,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1839],\n",
      "        [-1.1839],\n",
      "        [ 0.6887],\n",
      "        [ 0.6335],\n",
      "        [ 0.0228],\n",
      "        [ 0.5470],\n",
      "        [ 0.8877],\n",
      "        [ 0.9320],\n",
      "        [ 0.8877],\n",
      "        [ 0.8172],\n",
      "        [-1.4640],\n",
      "        [-0.9764],\n",
      "        [ 0.6052],\n",
      "        [-0.1893],\n",
      "        [ 0.9101],\n",
      "        [-0.1893],\n",
      "        [ 0.6052],\n",
      "        [-0.7042],\n",
      "        [-0.2955],\n",
      "        [ 0.8647],\n",
      "        [ 0.7674],\n",
      "        [ 0.7926],\n",
      "        [ 1.1216],\n",
      "        [ 1.1216],\n",
      "        [ 1.0141],\n",
      "        [-0.2955],\n",
      "        [-1.2074],\n",
      "        [ 1.0141],\n",
      "        [-1.3566],\n",
      "        [ 0.9320],\n",
      "        [-1.2746],\n",
      "        [ 0.6052],\n",
      "        [-0.1893],\n",
      "        [ 0.7674],\n",
      "        [ 0.5763],\n",
      "        [ 1.1216],\n",
      "        [ 0.7674],\n",
      "        [-0.7990],\n",
      "        [-0.0829],\n",
      "        [-1.1101],\n",
      "        [-1.1599],\n",
      "        [ 0.9320],\n",
      "        [-0.9764],\n",
      "        [ 0.6614],\n",
      "        [ 1.1216],\n",
      "        [-1.4802],\n",
      "        [ 0.1271],\n",
      "        [-0.4356],\n",
      "        [-1.2746],\n",
      "        [ 0.9741],\n",
      "        [-0.4356],\n",
      "        [-0.6390],\n",
      "        [-1.4302],\n",
      "        [ 0.6614],\n",
      "        [-1.3566],\n",
      "        [ 0.4868],\n",
      "        [-0.4701],\n",
      "        [ 0.4868],\n",
      "        [ 1.1216],\n",
      "        [ 1.0925]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 115/10000,\n",
      " train_loss: 0.0039,\n",
      " train_mae: 0.0471,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1871],\n",
      "        [-1.1871],\n",
      "        [ 0.6755],\n",
      "        [ 0.6200],\n",
      "        [ 0.0105],\n",
      "        [ 0.5331],\n",
      "        [ 0.8768],\n",
      "        [ 0.9218],\n",
      "        [ 0.8768],\n",
      "        [ 0.8053],\n",
      "        [-1.4687],\n",
      "        [-0.9800],\n",
      "        [ 0.5915],\n",
      "        [-0.1998],\n",
      "        [ 0.8996],\n",
      "        [-0.1998],\n",
      "        [ 0.5915],\n",
      "        [-0.7097],\n",
      "        [-0.3050],\n",
      "        [ 0.8535],\n",
      "        [ 0.7550],\n",
      "        [ 0.7804],\n",
      "        [ 1.1158],\n",
      "        [ 1.1158],\n",
      "        [ 1.0056],\n",
      "        [-0.3050],\n",
      "        [-1.2106],\n",
      "        [ 1.0056],\n",
      "        [-1.3603],\n",
      "        [ 0.9218],\n",
      "        [-1.2779],\n",
      "        [ 0.5915],\n",
      "        [-0.1998],\n",
      "        [ 0.7550],\n",
      "        [ 0.5626],\n",
      "        [ 1.1158],\n",
      "        [ 0.7550],\n",
      "        [-0.8038],\n",
      "        [-0.0944],\n",
      "        [-1.1134],\n",
      "        [-1.1630],\n",
      "        [ 0.9218],\n",
      "        [-0.9800],\n",
      "        [ 0.6480],\n",
      "        [ 1.1158],\n",
      "        [-1.4851],\n",
      "        [ 0.1141],\n",
      "        [-0.4437],\n",
      "        [-1.2779],\n",
      "        [ 0.9648],\n",
      "        [-0.4437],\n",
      "        [-0.6451],\n",
      "        [-1.4345],\n",
      "        [ 0.6480],\n",
      "        [-1.3603],\n",
      "        [ 0.4728],\n",
      "        [-0.4779],\n",
      "        [ 0.4728],\n",
      "        [ 1.1158],\n",
      "        [ 1.0859]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 116/10000,\n",
      " train_loss: 0.0038,\n",
      " train_mae: 0.0470,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1954],\n",
      "        [-1.1954],\n",
      "        [ 0.6934],\n",
      "        [ 0.6373],\n",
      "        [ 0.0199],\n",
      "        [ 0.5493],\n",
      "        [ 0.8968],\n",
      "        [ 0.9423],\n",
      "        [ 0.8968],\n",
      "        [ 0.8246],\n",
      "        [-1.4811],\n",
      "        [-0.9853],\n",
      "        [ 0.6084],\n",
      "        [-0.1934],\n",
      "        [ 0.9198],\n",
      "        [-0.1934],\n",
      "        [ 0.6084],\n",
      "        [-0.7109],\n",
      "        [-0.3001],\n",
      "        [ 0.8733],\n",
      "        [ 0.7737],\n",
      "        [ 0.7994],\n",
      "        [ 1.1380],\n",
      "        [ 1.1380],\n",
      "        [ 1.0268],\n",
      "        [-0.3001],\n",
      "        [-1.2193],\n",
      "        [ 1.0268],\n",
      "        [-1.3712],\n",
      "        [ 0.9423],\n",
      "        [-1.2876],\n",
      "        [ 0.6084],\n",
      "        [-0.1934],\n",
      "        [ 0.7737],\n",
      "        [ 0.5791],\n",
      "        [ 1.1380],\n",
      "        [ 0.7737],\n",
      "        [-0.8064],\n",
      "        [-0.0865],\n",
      "        [-1.1206],\n",
      "        [-1.1710],\n",
      "        [ 0.9423],\n",
      "        [-0.9853],\n",
      "        [ 0.6656],\n",
      "        [ 1.1380],\n",
      "        [-1.4977],\n",
      "        [ 0.1249],\n",
      "        [-0.4408],\n",
      "        [-1.2876],\n",
      "        [ 0.9856],\n",
      "        [-0.4408],\n",
      "        [-0.6453],\n",
      "        [-1.4464],\n",
      "        [ 0.6656],\n",
      "        [-1.3712],\n",
      "        [ 0.4883],\n",
      "        [-0.4756],\n",
      "        [ 0.4883],\n",
      "        [ 1.1380],\n",
      "        [ 1.1079]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 117/10000,\n",
      " train_loss: 0.0034,\n",
      " train_mae: 0.0503,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2117],\n",
      "        [-1.2117],\n",
      "        [ 0.7063],\n",
      "        [ 0.6492],\n",
      "        [ 0.0217],\n",
      "        [ 0.5599],\n",
      "        [ 0.9127],\n",
      "        [ 0.9588],\n",
      "        [ 0.9127],\n",
      "        [ 0.8394],\n",
      "        [-1.5001],\n",
      "        [-0.9990],\n",
      "        [ 0.6200],\n",
      "        [-0.1951],\n",
      "        [ 0.9360],\n",
      "        [-0.1951],\n",
      "        [ 0.6200],\n",
      "        [-0.7208],\n",
      "        [-0.3036],\n",
      "        [ 0.8888],\n",
      "        [ 0.7878],\n",
      "        [ 0.8139],\n",
      "        [ 1.1571],\n",
      "        [ 1.1571],\n",
      "        [ 1.0445],\n",
      "        [-0.3036],\n",
      "        [-1.2358],\n",
      "        [ 1.0445],\n",
      "        [-1.3893],\n",
      "        [ 0.9588],\n",
      "        [-1.3049],\n",
      "        [ 0.6200],\n",
      "        [-0.1951],\n",
      "        [ 0.7878],\n",
      "        [ 0.5902],\n",
      "        [ 1.1571],\n",
      "        [ 0.7878],\n",
      "        [-0.8177],\n",
      "        [-0.0864],\n",
      "        [-1.1360],\n",
      "        [-1.1870],\n",
      "        [ 0.9588],\n",
      "        [-0.9990],\n",
      "        [ 0.6780],\n",
      "        [ 1.1571],\n",
      "        [-1.5168],\n",
      "        [ 0.1285],\n",
      "        [-0.4466],\n",
      "        [-1.3049],\n",
      "        [ 1.0027],\n",
      "        [-0.4466],\n",
      "        [-0.6542],\n",
      "        [-1.4652],\n",
      "        [ 0.6780],\n",
      "        [-1.3893],\n",
      "        [ 0.4979],\n",
      "        [-0.4819],\n",
      "        [ 0.4979],\n",
      "        [ 1.1571],\n",
      "        [ 1.1266]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 118/10000,\n",
      " train_loss: 0.0034,\n",
      " train_mae: 0.0488,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2187],\n",
      "        [-1.2187],\n",
      "        [ 0.6951],\n",
      "        [ 0.6376],\n",
      "        [ 0.0093],\n",
      "        [ 0.5478],\n",
      "        [ 0.9038],\n",
      "        [ 0.9506],\n",
      "        [ 0.9038],\n",
      "        [ 0.8296],\n",
      "        [-1.5080],\n",
      "        [-1.0064],\n",
      "        [ 0.6082],\n",
      "        [-0.2066],\n",
      "        [ 0.9275],\n",
      "        [-0.2066],\n",
      "        [ 0.6082],\n",
      "        [-0.7295],\n",
      "        [-0.3146],\n",
      "        [ 0.8796],\n",
      "        [ 0.7774],\n",
      "        [ 0.8037],\n",
      "        [ 1.1527],\n",
      "        [ 1.1527],\n",
      "        [ 1.0377],\n",
      "        [-0.3146],\n",
      "        [-1.2429],\n",
      "        [ 1.0377],\n",
      "        [-1.3966],\n",
      "        [ 0.9506],\n",
      "        [-1.3120],\n",
      "        [ 0.6082],\n",
      "        [-0.2066],\n",
      "        [ 0.7774],\n",
      "        [ 0.5782],\n",
      "        [ 1.1527],\n",
      "        [ 0.7774],\n",
      "        [-0.8258],\n",
      "        [-0.0984],\n",
      "        [-1.1431],\n",
      "        [-1.1941],\n",
      "        [ 0.9506],\n",
      "        [-1.0064],\n",
      "        [ 0.6666],\n",
      "        [ 1.1527],\n",
      "        [-1.5248],\n",
      "        [ 0.1158],\n",
      "        [-0.4568],\n",
      "        [-1.3120],\n",
      "        [ 0.9953],\n",
      "        [-0.4568],\n",
      "        [-0.6632],\n",
      "        [-1.4729],\n",
      "        [ 0.6666],\n",
      "        [-1.3966],\n",
      "        [ 0.4855],\n",
      "        [-0.4919],\n",
      "        [ 0.4855],\n",
      "        [ 1.1527],\n",
      "        [ 1.1215]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 119/10000,\n",
      " train_loss: 0.0032,\n",
      " train_mae: 0.0438,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1923],\n",
      "        [-1.1923],\n",
      "        [ 0.6880],\n",
      "        [ 0.6313],\n",
      "        [ 0.0159],\n",
      "        [ 0.5428],\n",
      "        [ 0.8950],\n",
      "        [ 0.9416],\n",
      "        [ 0.8950],\n",
      "        [ 0.8211],\n",
      "        [-1.4850],\n",
      "        [-0.9805],\n",
      "        [ 0.6023],\n",
      "        [-0.1949],\n",
      "        [ 0.9185],\n",
      "        [-0.1949],\n",
      "        [ 0.6023],\n",
      "        [-0.7068],\n",
      "        [-0.3003],\n",
      "        [ 0.8709],\n",
      "        [ 0.7694],\n",
      "        [ 0.7955],\n",
      "        [ 1.1446],\n",
      "        [ 1.1446],\n",
      "        [ 1.0288],\n",
      "        [-0.3003],\n",
      "        [-1.2165],\n",
      "        [ 1.0288],\n",
      "        [-1.3716],\n",
      "        [ 0.9416],\n",
      "        [-1.2860],\n",
      "        [ 0.6023],\n",
      "        [-0.1949],\n",
      "        [ 0.7694],\n",
      "        [ 0.5728],\n",
      "        [ 1.1446],\n",
      "        [ 0.7694],\n",
      "        [-0.8018],\n",
      "        [-0.0892],\n",
      "        [-1.1166],\n",
      "        [-1.1676],\n",
      "        [ 0.9416],\n",
      "        [-0.9805],\n",
      "        [ 0.6599],\n",
      "        [ 1.1446],\n",
      "        [-1.5023],\n",
      "        [ 0.1199],\n",
      "        [-0.4393],\n",
      "        [-1.2860],\n",
      "        [ 0.9862],\n",
      "        [-0.4393],\n",
      "        [-0.6417],\n",
      "        [-1.4492],\n",
      "        [ 0.6599],\n",
      "        [-1.3716],\n",
      "        [ 0.4817],\n",
      "        [-0.4737],\n",
      "        [ 0.4817],\n",
      "        [ 1.1446],\n",
      "        [ 1.1131]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 120/10000,\n",
      " train_loss: 0.0030,\n",
      " train_mae: 0.0416,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1779],\n",
      "        [-1.1779],\n",
      "        [ 0.6821],\n",
      "        [ 0.6258],\n",
      "        [ 0.0177],\n",
      "        [ 0.5381],\n",
      "        [ 0.8885],\n",
      "        [ 0.9352],\n",
      "        [ 0.8885],\n",
      "        [ 0.8147],\n",
      "        [-1.4730],\n",
      "        [-0.9666],\n",
      "        [ 0.5970],\n",
      "        [-0.1899],\n",
      "        [ 0.9121],\n",
      "        [-0.1899],\n",
      "        [ 0.5970],\n",
      "        [-0.6950],\n",
      "        [-0.2937],\n",
      "        [ 0.8644],\n",
      "        [ 0.7631],\n",
      "        [ 0.7892],\n",
      "        [ 1.1395],\n",
      "        [ 1.1395],\n",
      "        [ 1.0227],\n",
      "        [-0.2937],\n",
      "        [-1.2022],\n",
      "        [ 1.0227],\n",
      "        [-1.3582],\n",
      "        [ 0.9352],\n",
      "        [-1.2720],\n",
      "        [ 0.5970],\n",
      "        [-0.1899],\n",
      "        [ 0.7631],\n",
      "        [ 0.5677],\n",
      "        [ 1.1395],\n",
      "        [ 0.7631],\n",
      "        [-0.7891],\n",
      "        [-0.0859],\n",
      "        [-1.1023],\n",
      "        [-1.1532],\n",
      "        [ 0.9352],\n",
      "        [-0.9666],\n",
      "        [ 0.6542],\n",
      "        [ 1.1395],\n",
      "        [-1.4905],\n",
      "        [ 0.1202],\n",
      "        [-0.4308],\n",
      "        [-1.2720],\n",
      "        [ 0.9800],\n",
      "        [-0.4308],\n",
      "        [-0.6306],\n",
      "        [-1.4366],\n",
      "        [ 0.6542],\n",
      "        [-1.3582],\n",
      "        [ 0.4775],\n",
      "        [-0.4647],\n",
      "        [ 0.4775],\n",
      "        [ 1.1395],\n",
      "        [ 1.1077]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 121/10000,\n",
      " train_loss: 0.0030,\n",
      " train_mae: 0.0425,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1985],\n",
      "        [-1.1985],\n",
      "        [ 0.6807],\n",
      "        [ 0.6235],\n",
      "        [ 0.0069],\n",
      "        [ 0.5344],\n",
      "        [ 0.8904],\n",
      "        [ 0.9379],\n",
      "        [ 0.8904],\n",
      "        [ 0.8154],\n",
      "        [-1.4941],\n",
      "        [-0.9862],\n",
      "        [ 0.5943],\n",
      "        [-0.2031],\n",
      "        [ 0.9144],\n",
      "        [-0.2031],\n",
      "        [ 0.5943],\n",
      "        [-0.7129],\n",
      "        [-0.3080],\n",
      "        [ 0.8659],\n",
      "        [ 0.7630],\n",
      "        [ 0.7894],\n",
      "        [ 1.1456],\n",
      "        [ 1.1456],\n",
      "        [ 1.0269],\n",
      "        [-0.3080],\n",
      "        [-1.2228],\n",
      "        [ 1.0269],\n",
      "        [-1.3792],\n",
      "        [ 0.9379],\n",
      "        [-1.2928],\n",
      "        [ 0.5943],\n",
      "        [-0.2031],\n",
      "        [ 0.7630],\n",
      "        [ 0.5646],\n",
      "        [ 1.1456],\n",
      "        [ 0.7630],\n",
      "        [-0.8077],\n",
      "        [-0.0979],\n",
      "        [-1.1225],\n",
      "        [-1.1737],\n",
      "        [ 0.9379],\n",
      "        [-0.9862],\n",
      "        [ 0.6523],\n",
      "        [ 1.1456],\n",
      "        [-1.5116],\n",
      "        [ 0.1107],\n",
      "        [-0.4465],\n",
      "        [-1.2928],\n",
      "        [ 0.9834],\n",
      "        [-0.4465],\n",
      "        [-0.6480],\n",
      "        [-1.4577],\n",
      "        [ 0.6523],\n",
      "        [-1.3792],\n",
      "        [ 0.4730],\n",
      "        [-0.4807],\n",
      "        [ 0.4730],\n",
      "        [ 1.1456],\n",
      "        [ 1.1132]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 122/10000,\n",
      " train_loss: 0.0028,\n",
      " train_mae: 0.0455,\n",
      " epoch_time_duration: 0.0187\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2081],\n",
      "        [-1.2081],\n",
      "        [ 0.6991],\n",
      "        [ 0.6413],\n",
      "        [ 0.0163],\n",
      "        [ 0.5512],\n",
      "        [ 0.9107],\n",
      "        [ 0.9586],\n",
      "        [ 0.9107],\n",
      "        [ 0.8351],\n",
      "        [-1.5070],\n",
      "        [-0.9928],\n",
      "        [ 0.6117],\n",
      "        [-0.1970],\n",
      "        [ 0.9349],\n",
      "        [-0.1970],\n",
      "        [ 0.6117],\n",
      "        [-0.7152],\n",
      "        [-0.3037],\n",
      "        [ 0.8860],\n",
      "        [ 0.7822],\n",
      "        [ 0.8089],\n",
      "        [ 1.1673],\n",
      "        [ 1.1673],\n",
      "        [ 1.0481],\n",
      "        [-0.3037],\n",
      "        [-1.2327],\n",
      "        [ 1.0481],\n",
      "        [-1.3909],\n",
      "        [ 0.9586],\n",
      "        [-1.3036],\n",
      "        [ 0.6117],\n",
      "        [-0.1970],\n",
      "        [ 0.7822],\n",
      "        [ 0.5817],\n",
      "        [ 1.1673],\n",
      "        [ 0.7822],\n",
      "        [-0.8115],\n",
      "        [-0.0901],\n",
      "        [-1.1311],\n",
      "        [-1.1829],\n",
      "        [ 0.9586],\n",
      "        [-0.9928],\n",
      "        [ 0.6705],\n",
      "        [ 1.1673],\n",
      "        [-1.5247],\n",
      "        [ 0.1217],\n",
      "        [-0.4444],\n",
      "        [-1.3036],\n",
      "        [ 1.0044],\n",
      "        [-0.4444],\n",
      "        [-0.6493],\n",
      "        [-1.4703],\n",
      "        [ 0.6705],\n",
      "        [-1.3909],\n",
      "        [ 0.4890],\n",
      "        [-0.4792],\n",
      "        [ 0.4890],\n",
      "        [ 1.1673],\n",
      "        [ 1.1349]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 123/10000,\n",
      " train_loss: 0.0028,\n",
      " train_mae: 0.0442,\n",
      " epoch_time_duration: 0.0090\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2078],\n",
      "        [-1.2078],\n",
      "        [ 0.6946],\n",
      "        [ 0.6367],\n",
      "        [ 0.0129],\n",
      "        [ 0.5466],\n",
      "        [ 0.9070],\n",
      "        [ 0.9551],\n",
      "        [ 0.9070],\n",
      "        [ 0.8310],\n",
      "        [-1.5081],\n",
      "        [-0.9925],\n",
      "        [ 0.6071],\n",
      "        [-0.1996],\n",
      "        [ 0.9313],\n",
      "        [-0.1996],\n",
      "        [ 0.6071],\n",
      "        [-0.7156],\n",
      "        [-0.3058],\n",
      "        [ 0.8822],\n",
      "        [ 0.7779],\n",
      "        [ 0.8047],\n",
      "        [ 1.1657],\n",
      "        [ 1.1657],\n",
      "        [ 1.0453],\n",
      "        [-0.3058],\n",
      "        [-1.2325],\n",
      "        [ 1.0453],\n",
      "        [-1.3913],\n",
      "        [ 0.9551],\n",
      "        [-1.3036],\n",
      "        [ 0.6071],\n",
      "        [-0.1996],\n",
      "        [ 0.7779],\n",
      "        [ 0.5770],\n",
      "        [ 1.1657],\n",
      "        [ 0.7779],\n",
      "        [-0.8115],\n",
      "        [-0.0931],\n",
      "        [-1.1307],\n",
      "        [-1.1826],\n",
      "        [ 0.9551],\n",
      "        [-0.9925],\n",
      "        [ 0.6659],\n",
      "        [ 1.1657],\n",
      "        [-1.5259],\n",
      "        [ 0.1179],\n",
      "        [-0.4458],\n",
      "        [-1.3036],\n",
      "        [ 1.0012],\n",
      "        [-0.4458],\n",
      "        [-0.6499],\n",
      "        [-1.4711],\n",
      "        [ 0.6659],\n",
      "        [-1.3913],\n",
      "        [ 0.4843],\n",
      "        [-0.4805],\n",
      "        [ 0.4843],\n",
      "        [ 1.1657],\n",
      "        [ 1.1329]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 124/10000,\n",
      " train_loss: 0.0026,\n",
      " train_mae: 0.0402,\n",
      " epoch_time_duration: 0.0090\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1938],\n",
      "        [-1.1938],\n",
      "        [ 0.6782],\n",
      "        [ 0.6208],\n",
      "        [ 0.0068],\n",
      "        [ 0.5316],\n",
      "        [ 0.8895],\n",
      "        [ 0.9377],\n",
      "        [ 0.8895],\n",
      "        [ 0.8138],\n",
      "        [-1.4947],\n",
      "        [-0.9803],\n",
      "        [ 0.5915],\n",
      "        [-0.2013],\n",
      "        [ 0.9139],\n",
      "        [-0.2013],\n",
      "        [ 0.5915],\n",
      "        [-0.7074],\n",
      "        [-0.3053],\n",
      "        [ 0.8648],\n",
      "        [ 0.7609],\n",
      "        [ 0.7876],\n",
      "        [ 1.1497],\n",
      "        [ 1.1497],\n",
      "        [ 1.0282],\n",
      "        [-0.3053],\n",
      "        [-1.2184],\n",
      "        [ 1.0282],\n",
      "        [-1.3771],\n",
      "        [ 0.9377],\n",
      "        [-1.2893],\n",
      "        [ 0.5915],\n",
      "        [-0.2013],\n",
      "        [ 0.7609],\n",
      "        [ 0.5617],\n",
      "        [ 1.1497],\n",
      "        [ 0.7609],\n",
      "        [-0.8018],\n",
      "        [-0.0971],\n",
      "        [-1.1172],\n",
      "        [-1.1688],\n",
      "        [ 0.9377],\n",
      "        [-0.9803],\n",
      "        [ 0.6497],\n",
      "        [ 1.1497],\n",
      "        [-1.5126],\n",
      "        [ 0.1098],\n",
      "        [-0.4426],\n",
      "        [-1.2893],\n",
      "        [ 0.9839],\n",
      "        [-0.4426],\n",
      "        [-0.6428],\n",
      "        [-1.4573],\n",
      "        [ 0.6497],\n",
      "        [-1.3771],\n",
      "        [ 0.4702],\n",
      "        [-0.4765],\n",
      "        [ 0.4702],\n",
      "        [ 1.1497],\n",
      "        [ 1.1165]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 125/10000,\n",
      " train_loss: 0.0025,\n",
      " train_mae: 0.0388,\n",
      " epoch_time_duration: 0.0079\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1791],\n",
      "        [-1.1791],\n",
      "        [ 0.6832],\n",
      "        [ 0.6262],\n",
      "        [ 0.0173],\n",
      "        [ 0.5377],\n",
      "        [ 0.8935],\n",
      "        [ 0.9414],\n",
      "        [ 0.8935],\n",
      "        [ 0.8180],\n",
      "        [-1.4828],\n",
      "        [-0.9648],\n",
      "        [ 0.5971],\n",
      "        [-0.1891],\n",
      "        [ 0.9177],\n",
      "        [-0.1891],\n",
      "        [ 0.5971],\n",
      "        [-0.6922],\n",
      "        [-0.2924],\n",
      "        [ 0.8688],\n",
      "        [ 0.7654],\n",
      "        [ 0.7920],\n",
      "        [ 1.1531],\n",
      "        [ 1.1531],\n",
      "        [ 1.0317],\n",
      "        [-0.2924],\n",
      "        [-1.2038],\n",
      "        [ 1.0317],\n",
      "        [-1.3638],\n",
      "        [ 0.9414],\n",
      "        [-1.2752],\n",
      "        [ 0.5971],\n",
      "        [-0.1891],\n",
      "        [ 0.7654],\n",
      "        [ 0.5676],\n",
      "        [ 1.1531],\n",
      "        [ 0.7654],\n",
      "        [-0.7864],\n",
      "        [-0.0857],\n",
      "        [-1.1021],\n",
      "        [-1.1539],\n",
      "        [ 0.9414],\n",
      "        [-0.9648],\n",
      "        [ 0.6549],\n",
      "        [ 1.1531],\n",
      "        [-1.5010],\n",
      "        [ 0.1194],\n",
      "        [-0.4287],\n",
      "        [-1.2752],\n",
      "        [ 0.9875],\n",
      "        [-0.4287],\n",
      "        [-0.6279],\n",
      "        [-1.4449],\n",
      "        [ 0.6549],\n",
      "        [-1.3638],\n",
      "        [ 0.4767],\n",
      "        [-0.4625],\n",
      "        [ 0.4767],\n",
      "        [ 1.1531],\n",
      "        [ 1.1199]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 126/10000,\n",
      " train_loss: 0.0026,\n",
      " train_mae: 0.0405,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2029],\n",
      "        [-1.2029],\n",
      "        [ 0.6831],\n",
      "        [ 0.6252],\n",
      "        [ 0.0064],\n",
      "        [ 0.5352],\n",
      "        [ 0.8967],\n",
      "        [ 0.9455],\n",
      "        [ 0.8967],\n",
      "        [ 0.8201],\n",
      "        [-1.5070],\n",
      "        [-0.9875],\n",
      "        [ 0.5956],\n",
      "        [-0.2032],\n",
      "        [ 0.9213],\n",
      "        [-0.2032],\n",
      "        [ 0.5956],\n",
      "        [-0.7126],\n",
      "        [-0.3079],\n",
      "        [ 0.8717],\n",
      "        [ 0.7667],\n",
      "        [ 0.7936],\n",
      "        [ 1.1603],\n",
      "        [ 1.1603],\n",
      "        [ 1.0371],\n",
      "        [-0.3079],\n",
      "        [-1.2278],\n",
      "        [ 1.0371],\n",
      "        [-1.3881],\n",
      "        [ 0.9455],\n",
      "        [-1.2993],\n",
      "        [ 0.5956],\n",
      "        [-0.2032],\n",
      "        [ 0.7667],\n",
      "        [ 0.5656],\n",
      "        [ 1.1603],\n",
      "        [ 0.7667],\n",
      "        [-0.8076],\n",
      "        [-0.0982],\n",
      "        [-1.1256],\n",
      "        [-1.1776],\n",
      "        [ 0.9455],\n",
      "        [-0.9875],\n",
      "        [ 0.6543],\n",
      "        [ 1.1603],\n",
      "        [-1.5252],\n",
      "        [ 0.1100],\n",
      "        [-0.4460],\n",
      "        [-1.2993],\n",
      "        [ 0.9922],\n",
      "        [-0.4460],\n",
      "        [-0.6475],\n",
      "        [-1.4692],\n",
      "        [ 0.6543],\n",
      "        [-1.3881],\n",
      "        [ 0.4732],\n",
      "        [-0.4802],\n",
      "        [ 0.4732],\n",
      "        [ 1.1603],\n",
      "        [ 1.1266]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 127/10000,\n",
      " train_loss: 0.0024,\n",
      " train_mae: 0.0427,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2085],\n",
      "        [-1.2085],\n",
      "        [ 0.6939],\n",
      "        [ 0.6356],\n",
      "        [ 0.0120],\n",
      "        [ 0.5450],\n",
      "        [ 0.9088],\n",
      "        [ 0.9577],\n",
      "        [ 0.9088],\n",
      "        [ 0.8317],\n",
      "        [-1.5150],\n",
      "        [-0.9911],\n",
      "        [ 0.6058],\n",
      "        [-0.1994],\n",
      "        [ 0.9335],\n",
      "        [-0.1994],\n",
      "        [ 0.6058],\n",
      "        [-0.7136],\n",
      "        [-0.3050],\n",
      "        [ 0.8836],\n",
      "        [ 0.7780],\n",
      "        [ 0.8051],\n",
      "        [ 1.1734],\n",
      "        [ 1.1734],\n",
      "        [ 1.0498],\n",
      "        [-0.3050],\n",
      "        [-1.2335],\n",
      "        [ 1.0498],\n",
      "        [-1.3952],\n",
      "        [ 0.9577],\n",
      "        [-1.3057],\n",
      "        [ 0.6058],\n",
      "        [-0.1994],\n",
      "        [ 0.7780],\n",
      "        [ 0.5756],\n",
      "        [ 1.1734],\n",
      "        [ 0.7780],\n",
      "        [-0.8095],\n",
      "        [-0.0935],\n",
      "        [-1.1305],\n",
      "        [-1.1829],\n",
      "        [ 0.9577],\n",
      "        [-0.9911],\n",
      "        [ 0.6650],\n",
      "        [ 1.1734],\n",
      "        [-1.5334],\n",
      "        [ 0.1166],\n",
      "        [-0.4445],\n",
      "        [-1.3057],\n",
      "        [ 1.0047],\n",
      "        [-0.4445],\n",
      "        [-0.6479],\n",
      "        [-1.4770],\n",
      "        [ 0.6650],\n",
      "        [-1.3952],\n",
      "        [ 0.4826],\n",
      "        [-0.4790],\n",
      "        [ 0.4826],\n",
      "        [ 1.1734],\n",
      "        [ 1.1396]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 128/10000,\n",
      " train_loss: 0.0024,\n",
      " train_mae: 0.0394,\n",
      " epoch_time_duration: 0.0093\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1971],\n",
      "        [-1.1971],\n",
      "        [ 0.6871],\n",
      "        [ 0.6292],\n",
      "        [ 0.0122],\n",
      "        [ 0.5393],\n",
      "        [ 0.9010],\n",
      "        [ 0.9499],\n",
      "        [ 0.9010],\n",
      "        [ 0.8242],\n",
      "        [-1.5048],\n",
      "        [-0.9804],\n",
      "        [ 0.5996],\n",
      "        [-0.1966],\n",
      "        [ 0.9257],\n",
      "        [-0.1966],\n",
      "        [ 0.5996],\n",
      "        [-0.7050],\n",
      "        [-0.3010],\n",
      "        [ 0.8759],\n",
      "        [ 0.7707],\n",
      "        [ 0.7977],\n",
      "        [ 1.1661],\n",
      "        [ 1.1661],\n",
      "        [ 1.0420],\n",
      "        [-0.3010],\n",
      "        [-1.2221],\n",
      "        [ 1.0420],\n",
      "        [-1.3841],\n",
      "        [ 0.9499],\n",
      "        [-1.2944],\n",
      "        [ 0.5996],\n",
      "        [-0.1966],\n",
      "        [ 0.7707],\n",
      "        [ 0.5697],\n",
      "        [ 1.1661],\n",
      "        [ 0.7707],\n",
      "        [-0.8001],\n",
      "        [-0.0920],\n",
      "        [-1.1192],\n",
      "        [-1.1716],\n",
      "        [ 0.9499],\n",
      "        [-0.9804],\n",
      "        [ 0.6584],\n",
      "        [ 1.1661],\n",
      "        [-1.5233],\n",
      "        [ 0.1154],\n",
      "        [-0.4388],\n",
      "        [-1.2944],\n",
      "        [ 0.9969],\n",
      "        [-0.4388],\n",
      "        [-0.6400],\n",
      "        [-1.4664],\n",
      "        [ 0.6584],\n",
      "        [-1.3841],\n",
      "        [ 0.4775],\n",
      "        [-0.4728],\n",
      "        [ 0.4775],\n",
      "        [ 1.1661],\n",
      "        [ 1.1321]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 129/10000,\n",
      " train_loss: 0.0023,\n",
      " train_mae: 0.0374,\n",
      " epoch_time_duration: 0.0096\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1886],\n",
      "        [-1.1886],\n",
      "        [ 0.6739],\n",
      "        [ 0.6164],\n",
      "        [ 0.0062],\n",
      "        [ 0.5272],\n",
      "        [ 0.8871],\n",
      "        [ 0.9359],\n",
      "        [ 0.8871],\n",
      "        [ 0.8104],\n",
      "        [-1.4962],\n",
      "        [-0.9734],\n",
      "        [ 0.5870],\n",
      "        [-0.1995],\n",
      "        [ 0.9117],\n",
      "        [-0.1995],\n",
      "        [ 0.5870],\n",
      "        [-0.7008],\n",
      "        [-0.3024],\n",
      "        [ 0.8620],\n",
      "        [ 0.7571],\n",
      "        [ 0.7840],\n",
      "        [ 1.1530],\n",
      "        [ 1.1530],\n",
      "        [ 1.0282],\n",
      "        [-0.3024],\n",
      "        [-1.2135],\n",
      "        [ 1.0282],\n",
      "        [-1.3753],\n",
      "        [ 0.9359],\n",
      "        [-1.2856],\n",
      "        [ 0.5870],\n",
      "        [-0.1995],\n",
      "        [ 0.7571],\n",
      "        [ 0.5573],\n",
      "        [ 1.1530],\n",
      "        [ 0.7571],\n",
      "        [-0.7948],\n",
      "        [-0.0965],\n",
      "        [-1.1111],\n",
      "        [-1.1632],\n",
      "        [ 0.9359],\n",
      "        [-0.9734],\n",
      "        [ 0.6453],\n",
      "        [ 1.1530],\n",
      "        [-1.5149],\n",
      "        [ 0.1081],\n",
      "        [-0.4382],\n",
      "        [-1.2856],\n",
      "        [ 0.9830],\n",
      "        [-0.4382],\n",
      "        [-0.6367],\n",
      "        [-1.4577],\n",
      "        [ 0.6453],\n",
      "        [-1.3753],\n",
      "        [ 0.4659],\n",
      "        [-0.4718],\n",
      "        [ 0.4659],\n",
      "        [ 1.1530],\n",
      "        [ 1.1188]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 130/10000,\n",
      " train_loss: 0.0023,\n",
      " train_mae: 0.0379,\n",
      " epoch_time_duration: 0.0093\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1878],\n",
      "        [-1.1878],\n",
      "        [ 0.6866],\n",
      "        [ 0.6289],\n",
      "        [ 0.0162],\n",
      "        [ 0.5395],\n",
      "        [ 0.9002],\n",
      "        [ 0.9491],\n",
      "        [ 0.9002],\n",
      "        [ 0.8234],\n",
      "        [-1.4983],\n",
      "        [-0.9706],\n",
      "        [ 0.5995],\n",
      "        [-0.1908],\n",
      "        [ 0.9249],\n",
      "        [-0.1908],\n",
      "        [ 0.5995],\n",
      "        [-0.6958],\n",
      "        [-0.2943],\n",
      "        [ 0.8751],\n",
      "        [ 0.7700],\n",
      "        [ 0.7969],\n",
      "        [ 1.1663],\n",
      "        [ 1.1663],\n",
      "        [ 1.0415],\n",
      "        [-0.2943],\n",
      "        [-1.2129],\n",
      "        [ 1.0415],\n",
      "        [-1.3762],\n",
      "        [ 0.9491],\n",
      "        [-1.2857],\n",
      "        [ 0.5995],\n",
      "        [-0.1908],\n",
      "        [ 0.7700],\n",
      "        [ 0.5697],\n",
      "        [ 1.1663],\n",
      "        [ 0.7700],\n",
      "        [-0.7906],\n",
      "        [-0.0871],\n",
      "        [-1.1096],\n",
      "        [-1.1622],\n",
      "        [ 0.9491],\n",
      "        [-0.9706],\n",
      "        [ 0.6580],\n",
      "        [ 1.1663],\n",
      "        [-1.5171],\n",
      "        [ 0.1186],\n",
      "        [-0.4311],\n",
      "        [-1.2857],\n",
      "        [ 0.9962],\n",
      "        [-0.4311],\n",
      "        [-0.6311],\n",
      "        [-1.4594],\n",
      "        [ 0.6580],\n",
      "        [-1.3762],\n",
      "        [ 0.4780],\n",
      "        [-0.4649],\n",
      "        [ 0.4780],\n",
      "        [ 1.1663],\n",
      "        [ 1.1320]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 131/10000,\n",
      " train_loss: 0.0022,\n",
      " train_mae: 0.0403,\n",
      " epoch_time_duration: 0.0099\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2116],\n",
      "        [-1.2116],\n",
      "        [ 0.6835],\n",
      "        [ 0.6250],\n",
      "        [ 0.0032],\n",
      "        [ 0.5342],\n",
      "        [ 0.9003],\n",
      "        [ 0.9500],\n",
      "        [ 0.9003],\n",
      "        [ 0.8224],\n",
      "        [-1.5221],\n",
      "        [-0.9936],\n",
      "        [ 0.5951],\n",
      "        [-0.2065],\n",
      "        [ 0.9254],\n",
      "        [-0.2065],\n",
      "        [ 0.5951],\n",
      "        [-0.7169],\n",
      "        [-0.3113],\n",
      "        [ 0.8748],\n",
      "        [ 0.7682],\n",
      "        [ 0.7955],\n",
      "        [ 1.1701],\n",
      "        [ 1.1701],\n",
      "        [ 1.0436],\n",
      "        [-0.3113],\n",
      "        [-1.2368],\n",
      "        [ 1.0436],\n",
      "        [-1.4002],\n",
      "        [ 0.9500],\n",
      "        [-1.3097],\n",
      "        [ 0.5951],\n",
      "        [-0.2065],\n",
      "        [ 0.7682],\n",
      "        [ 0.5648],\n",
      "        [ 1.1701],\n",
      "        [ 0.7682],\n",
      "        [-0.8124],\n",
      "        [-0.1015],\n",
      "        [-1.1332],\n",
      "        [-1.1859],\n",
      "        [ 0.9500],\n",
      "        [-0.9936],\n",
      "        [ 0.6545],\n",
      "        [ 1.1701],\n",
      "        [-1.5408],\n",
      "        [ 0.1071],\n",
      "        [-0.4496],\n",
      "        [-1.3097],\n",
      "        [ 0.9977],\n",
      "        [-0.4496],\n",
      "        [-0.6516],\n",
      "        [-1.4833],\n",
      "        [ 0.6545],\n",
      "        [-1.4002],\n",
      "        [ 0.4718],\n",
      "        [-0.4839],\n",
      "        [ 0.4718],\n",
      "        [ 1.1701],\n",
      "        [ 1.1354]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 132/10000,\n",
      " train_loss: 0.0022,\n",
      " train_mae: 0.0389,\n",
      " epoch_time_duration: 0.0091\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1963],\n",
      "        [-1.1963],\n",
      "        [ 0.6915],\n",
      "        [ 0.6334],\n",
      "        [ 0.0163],\n",
      "        [ 0.5433],\n",
      "        [ 0.9069],\n",
      "        [ 0.9562],\n",
      "        [ 0.9069],\n",
      "        [ 0.8295],\n",
      "        [-1.5095],\n",
      "        [-0.9774],\n",
      "        [ 0.6038],\n",
      "        [-0.1921],\n",
      "        [ 0.9318],\n",
      "        [-0.1921],\n",
      "        [ 0.6038],\n",
      "        [-0.7006],\n",
      "        [-0.2963],\n",
      "        [ 0.8815],\n",
      "        [ 0.7756],\n",
      "        [ 0.8027],\n",
      "        [ 1.1753],\n",
      "        [ 1.1753],\n",
      "        [ 1.0494],\n",
      "        [-0.2963],\n",
      "        [-1.2216],\n",
      "        [ 1.0494],\n",
      "        [-1.3863],\n",
      "        [ 0.9562],\n",
      "        [-1.2950],\n",
      "        [ 0.6038],\n",
      "        [-0.1921],\n",
      "        [ 0.7756],\n",
      "        [ 0.5737],\n",
      "        [ 1.1753],\n",
      "        [ 0.7756],\n",
      "        [-0.7960],\n",
      "        [-0.0877],\n",
      "        [-1.1175],\n",
      "        [-1.1704],\n",
      "        [ 0.9562],\n",
      "        [-0.9774],\n",
      "        [ 0.6627],\n",
      "        [ 1.1753],\n",
      "        [-1.5285],\n",
      "        [ 0.1194],\n",
      "        [-0.4340],\n",
      "        [-1.2950],\n",
      "        [ 1.0037],\n",
      "        [-0.4340],\n",
      "        [-0.6354],\n",
      "        [-1.4703],\n",
      "        [ 0.6627],\n",
      "        [-1.3863],\n",
      "        [ 0.4813],\n",
      "        [-0.4681],\n",
      "        [ 0.4813],\n",
      "        [ 1.1753],\n",
      "        [ 1.1408]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 133/10000,\n",
      " train_loss: 0.0021,\n",
      " train_mae: 0.0364,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1927],\n",
      "        [-1.1927],\n",
      "        [ 0.6728],\n",
      "        [ 0.6150],\n",
      "        [ 0.0041],\n",
      "        [ 0.5254],\n",
      "        [ 0.8878],\n",
      "        [ 0.9372],\n",
      "        [ 0.8878],\n",
      "        [ 0.8103],\n",
      "        [-1.5047],\n",
      "        [-0.9759],\n",
      "        [ 0.5855],\n",
      "        [-0.2014],\n",
      "        [ 0.9127],\n",
      "        [-0.2014],\n",
      "        [ 0.5855],\n",
      "        [-0.7025],\n",
      "        [-0.3041],\n",
      "        [ 0.8624],\n",
      "        [ 0.7566],\n",
      "        [ 0.7837],\n",
      "        [ 1.1576],\n",
      "        [ 1.1576],\n",
      "        [ 1.0307],\n",
      "        [-0.3041],\n",
      "        [-1.2179],\n",
      "        [ 1.0307],\n",
      "        [-1.3817],\n",
      "        [ 0.9372],\n",
      "        [-1.2907],\n",
      "        [ 0.5855],\n",
      "        [-0.2014],\n",
      "        [ 0.7566],\n",
      "        [ 0.5556],\n",
      "        [ 1.1576],\n",
      "        [ 0.7566],\n",
      "        [-0.7967],\n",
      "        [-0.0985],\n",
      "        [-1.1145],\n",
      "        [-1.1671],\n",
      "        [ 0.9372],\n",
      "        [-0.9759],\n",
      "        [ 0.6441],\n",
      "        [ 1.1576],\n",
      "        [-1.5237],\n",
      "        [ 0.1058],\n",
      "        [-0.4398],\n",
      "        [-1.2907],\n",
      "        [ 0.9848],\n",
      "        [-0.4398],\n",
      "        [-0.6383],\n",
      "        [-1.4655],\n",
      "        [ 0.6441],\n",
      "        [-1.3817],\n",
      "        [ 0.4639],\n",
      "        [-0.4734],\n",
      "        [ 0.4639],\n",
      "        [ 1.1576],\n",
      "        [ 1.1227]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 134/10000,\n",
      " train_loss: 0.0021,\n",
      " train_mae: 0.0363,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1862],\n",
      "        [-1.1862],\n",
      "        [ 0.6845],\n",
      "        [ 0.6267],\n",
      "        [ 0.0158],\n",
      "        [ 0.5373],\n",
      "        [ 0.8991],\n",
      "        [ 0.9484],\n",
      "        [ 0.8991],\n",
      "        [ 0.8218],\n",
      "        [-1.5010],\n",
      "        [-0.9678],\n",
      "        [ 0.5973],\n",
      "        [-0.1900],\n",
      "        [ 0.9240],\n",
      "        [-0.1900],\n",
      "        [ 0.5973],\n",
      "        [-0.6929],\n",
      "        [-0.2930],\n",
      "        [ 0.8738],\n",
      "        [ 0.7682],\n",
      "        [ 0.7952],\n",
      "        [ 1.1683],\n",
      "        [ 1.1683],\n",
      "        [ 1.0417],\n",
      "        [-0.2930],\n",
      "        [-1.2116],\n",
      "        [ 1.0417],\n",
      "        [-1.3768],\n",
      "        [ 0.9484],\n",
      "        [-1.2851],\n",
      "        [ 0.5973],\n",
      "        [-0.1900],\n",
      "        [ 0.7682],\n",
      "        [ 0.5675],\n",
      "        [ 1.1683],\n",
      "        [ 0.7682],\n",
      "        [-0.7876],\n",
      "        [-0.0869],\n",
      "        [-1.1075],\n",
      "        [-1.1604],\n",
      "        [ 0.9484],\n",
      "        [-0.9678],\n",
      "        [ 0.6558],\n",
      "        [ 1.1683],\n",
      "        [-1.5202],\n",
      "        [ 0.1177],\n",
      "        [-0.4291],\n",
      "        [-1.2851],\n",
      "        [ 0.9960],\n",
      "        [-0.4291],\n",
      "        [-0.6284],\n",
      "        [-1.4614],\n",
      "        [ 0.6558],\n",
      "        [-1.3768],\n",
      "        [ 0.4758],\n",
      "        [-0.4628],\n",
      "        [ 0.4758],\n",
      "        [ 1.1683],\n",
      "        [ 1.1336]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 135/10000,\n",
      " train_loss: 0.0021,\n",
      " train_mae: 0.0384,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2087],\n",
      "        [-1.2087],\n",
      "        [ 0.6812],\n",
      "        [ 0.6227],\n",
      "        [ 0.0034],\n",
      "        [ 0.5319],\n",
      "        [ 0.8988],\n",
      "        [ 0.9488],\n",
      "        [ 0.8988],\n",
      "        [ 0.8205],\n",
      "        [-1.5234],\n",
      "        [-0.9896],\n",
      "        [ 0.5928],\n",
      "        [-0.2050],\n",
      "        [ 0.9240],\n",
      "        [-0.2050],\n",
      "        [ 0.5928],\n",
      "        [-0.7129],\n",
      "        [-0.3091],\n",
      "        [ 0.8731],\n",
      "        [ 0.7660],\n",
      "        [ 0.7935],\n",
      "        [ 1.1714],\n",
      "        [ 1.1714],\n",
      "        [ 1.0433],\n",
      "        [-0.3091],\n",
      "        [-1.2341],\n",
      "        [ 1.0433],\n",
      "        [-1.3994],\n",
      "        [ 0.9488],\n",
      "        [-1.3077],\n",
      "        [ 0.5928],\n",
      "        [-0.2050],\n",
      "        [ 0.7660],\n",
      "        [ 0.5626],\n",
      "        [ 1.1714],\n",
      "        [ 0.7660],\n",
      "        [-0.8082],\n",
      "        [-0.1006],\n",
      "        [-1.1298],\n",
      "        [-1.1828],\n",
      "        [ 0.9488],\n",
      "        [-0.9896],\n",
      "        [ 0.6522],\n",
      "        [ 1.1714],\n",
      "        [-1.5425],\n",
      "        [ 0.1066],\n",
      "        [-0.4467],\n",
      "        [-1.3077],\n",
      "        [ 0.9969],\n",
      "        [-0.4467],\n",
      "        [-0.6478],\n",
      "        [-1.4838],\n",
      "        [ 0.6522],\n",
      "        [-1.3994],\n",
      "        [ 0.4697],\n",
      "        [-0.4807],\n",
      "        [ 0.4697],\n",
      "        [ 1.1714],\n",
      "        [ 1.1362]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 136/10000,\n",
      " train_loss: 0.0020,\n",
      " train_mae: 0.0377,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1954],\n",
      "        [-1.1954],\n",
      "        [ 0.6908],\n",
      "        [ 0.6325],\n",
      "        [ 0.0166],\n",
      "        [ 0.5423],\n",
      "        [ 0.9071],\n",
      "        [ 0.9568],\n",
      "        [ 0.9071],\n",
      "        [ 0.8292],\n",
      "        [-1.5128],\n",
      "        [-0.9752],\n",
      "        [ 0.6028],\n",
      "        [-0.1910],\n",
      "        [ 0.9321],\n",
      "        [-0.1910],\n",
      "        [ 0.6028],\n",
      "        [-0.6980],\n",
      "        [-0.2948],\n",
      "        [ 0.8815],\n",
      "        [ 0.7751],\n",
      "        [ 0.8023],\n",
      "        [ 1.1783],\n",
      "        [ 1.1783],\n",
      "        [ 1.0508],\n",
      "        [-0.2948],\n",
      "        [-1.2210],\n",
      "        [ 1.0508],\n",
      "        [-1.3875],\n",
      "        [ 0.9568],\n",
      "        [-1.2951],\n",
      "        [ 0.6028],\n",
      "        [-0.1910],\n",
      "        [ 0.7751],\n",
      "        [ 0.5728],\n",
      "        [ 1.1783],\n",
      "        [ 0.7751],\n",
      "        [-0.7935],\n",
      "        [-0.0870],\n",
      "        [-1.1160],\n",
      "        [-1.1694],\n",
      "        [ 0.9568],\n",
      "        [-0.9752],\n",
      "        [ 0.6618],\n",
      "        [ 1.1783],\n",
      "        [-1.5321],\n",
      "        [ 0.1193],\n",
      "        [-0.4320],\n",
      "        [-1.2951],\n",
      "        [ 1.0047],\n",
      "        [-0.4320],\n",
      "        [-0.6330],\n",
      "        [-1.4728],\n",
      "        [ 0.6618],\n",
      "        [-1.3875],\n",
      "        [ 0.4804],\n",
      "        [-0.4660],\n",
      "        [ 0.4804],\n",
      "        [ 1.1783],\n",
      "        [ 1.1433]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 137/10000,\n",
      " train_loss: 0.0020,\n",
      " train_mae: 0.0356,\n",
      " epoch_time_duration: 0.0079\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1963e+00],\n",
      "        [-1.1963e+00],\n",
      "        [ 6.7016e-01],\n",
      "        [ 6.1210e-01],\n",
      "        [ 1.0485e-03],\n",
      "        [ 5.2228e-01],\n",
      "        [ 8.8645e-01],\n",
      "        [ 9.3631e-01],\n",
      "        [ 8.8645e-01],\n",
      "        [ 8.0844e-01],\n",
      "        [-1.5118e+00],\n",
      "        [-9.7834e-01],\n",
      "        [ 5.8251e-01],\n",
      "        [-2.0405e-01],\n",
      "        [ 9.1159e-01],\n",
      "        [-2.0405e-01],\n",
      "        [ 5.8251e-01],\n",
      "        [-7.0451e-01],\n",
      "        [-3.0654e-01],\n",
      "        [ 8.6087e-01],\n",
      "        [ 7.5434e-01],\n",
      "        [ 7.8160e-01],\n",
      "        [ 1.1594e+00],\n",
      "        [ 1.1594e+00],\n",
      "        [ 1.0308e+00],\n",
      "        [-3.0654e-01],\n",
      "        [-1.2217e+00],\n",
      "        [ 1.0308e+00],\n",
      "        [-1.3870e+00],\n",
      "        [ 9.3631e-01],\n",
      "        [-1.2952e+00],\n",
      "        [ 5.8251e-01],\n",
      "        [-2.0405e-01],\n",
      "        [ 7.5434e-01],\n",
      "        [ 5.5257e-01],\n",
      "        [ 1.1594e+00],\n",
      "        [ 7.5434e-01],\n",
      "        [-7.9875e-01],\n",
      "        [-1.0135e-01],\n",
      "        [-1.1176e+00],\n",
      "        [-1.1705e+00],\n",
      "        [ 9.3631e-01],\n",
      "        [-9.7834e-01],\n",
      "        [ 6.4132e-01],\n",
      "        [ 1.1594e+00],\n",
      "        [-1.5311e+00],\n",
      "        [ 1.0265e-01],\n",
      "        [-4.4199e-01],\n",
      "        [-1.2952e+00],\n",
      "        [ 9.8443e-01],\n",
      "        [-4.4199e-01],\n",
      "        [-6.4025e-01],\n",
      "        [-1.4719e+00],\n",
      "        [ 6.4132e-01],\n",
      "        [-1.3870e+00],\n",
      "        [ 4.6070e-01],\n",
      "        [-4.7551e-01],\n",
      "        [ 4.6070e-01],\n",
      "        [ 1.1594e+00],\n",
      "        [ 1.1240e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 138/10000,\n",
      " train_loss: 0.0020,\n",
      " train_mae: 0.0366,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1816],\n",
      "        [-1.1816],\n",
      "        [ 0.6899],\n",
      "        [ 0.6321],\n",
      "        [ 0.0223],\n",
      "        [ 0.5427],\n",
      "        [ 0.9050],\n",
      "        [ 0.9545],\n",
      "        [ 0.9050],\n",
      "        [ 0.8275],\n",
      "        [-1.5011],\n",
      "        [-0.9615],\n",
      "        [ 0.6027],\n",
      "        [-0.1830],\n",
      "        [ 0.9300],\n",
      "        [-0.1830],\n",
      "        [ 0.6027],\n",
      "        [-0.6856],\n",
      "        [-0.2858],\n",
      "        [ 0.8796],\n",
      "        [ 0.7737],\n",
      "        [ 0.8008],\n",
      "        [ 1.1760],\n",
      "        [ 1.1760],\n",
      "        [ 1.0483],\n",
      "        [-0.2858],\n",
      "        [-1.2073],\n",
      "        [ 1.0483],\n",
      "        [-1.3746],\n",
      "        [ 0.9545],\n",
      "        [-1.2816],\n",
      "        [ 0.6027],\n",
      "        [-0.1830],\n",
      "        [ 0.7737],\n",
      "        [ 0.5729],\n",
      "        [ 1.1760],\n",
      "        [ 0.7737],\n",
      "        [-0.7805],\n",
      "        [-0.0802],\n",
      "        [-1.1021],\n",
      "        [-1.1556],\n",
      "        [ 0.9545],\n",
      "        [-0.9615],\n",
      "        [ 0.6612],\n",
      "        [ 1.1760],\n",
      "        [-1.5206],\n",
      "        [ 0.1239],\n",
      "        [-0.4217],\n",
      "        [-1.2816],\n",
      "        [ 1.0023],\n",
      "        [-0.4217],\n",
      "        [-0.6209],\n",
      "        [-1.4607],\n",
      "        [ 0.6612],\n",
      "        [-1.3746],\n",
      "        [ 0.4813],\n",
      "        [-0.4554],\n",
      "        [ 0.4813],\n",
      "        [ 1.1760],\n",
      "        [ 1.1409]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 139/10000,\n",
      " train_loss: 0.0020,\n",
      " train_mae: 0.0385,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2183],\n",
      "        [-1.2183],\n",
      "        [ 0.6685],\n",
      "        [ 0.6096],\n",
      "        [-0.0098],\n",
      "        [ 0.5185],\n",
      "        [ 0.8878],\n",
      "        [ 0.9384],\n",
      "        [ 0.8878],\n",
      "        [ 0.8087],\n",
      "        [-1.5347],\n",
      "        [-0.9992],\n",
      "        [ 0.5796],\n",
      "        [-0.2175],\n",
      "        [ 0.9134],\n",
      "        [-0.2175],\n",
      "        [ 0.5796],\n",
      "        [-0.7232],\n",
      "        [-0.3212],\n",
      "        [ 0.8619],\n",
      "        [ 0.7539],\n",
      "        [ 0.7815],\n",
      "        [ 1.1646],\n",
      "        [ 1.1646],\n",
      "        [ 1.0342],\n",
      "        [-0.3212],\n",
      "        [-1.2438],\n",
      "        [ 1.0342],\n",
      "        [-1.4098],\n",
      "        [ 0.9384],\n",
      "        [-1.3176],\n",
      "        [ 0.5796],\n",
      "        [-0.2175],\n",
      "        [ 0.7539],\n",
      "        [ 0.5492],\n",
      "        [ 1.1646],\n",
      "        [ 0.7539],\n",
      "        [-0.8182],\n",
      "        [-0.1135],\n",
      "        [-1.1393],\n",
      "        [-1.1924],\n",
      "        [ 0.9384],\n",
      "        [-0.9992],\n",
      "        [ 0.6392],\n",
      "        [ 1.1646],\n",
      "        [-1.5540],\n",
      "        [ 0.0931],\n",
      "        [-0.4581],\n",
      "        [-1.3176],\n",
      "        [ 0.9872],\n",
      "        [-0.4581],\n",
      "        [-0.6583],\n",
      "        [-1.4948],\n",
      "        [ 0.6392],\n",
      "        [-1.4098],\n",
      "        [ 0.4560],\n",
      "        [-0.4920],\n",
      "        [ 0.4560],\n",
      "        [ 1.1646],\n",
      "        [ 1.1288]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 140/10000,\n",
      " train_loss: 0.0022,\n",
      " train_mae: 0.0418,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1741],\n",
      "        [-1.1741],\n",
      "        [ 0.7093],\n",
      "        [ 0.6515],\n",
      "        [ 0.0402],\n",
      "        [ 0.5620],\n",
      "        [ 0.9242],\n",
      "        [ 0.9737],\n",
      "        [ 0.9242],\n",
      "        [ 0.8468],\n",
      "        [-1.4980],\n",
      "        [-0.9514],\n",
      "        [ 0.6220],\n",
      "        [-0.1662],\n",
      "        [ 0.9492],\n",
      "        [-0.1662],\n",
      "        [ 0.6220],\n",
      "        [-0.6726],\n",
      "        [-0.2696],\n",
      "        [ 0.8988],\n",
      "        [ 0.7930],\n",
      "        [ 0.8201],\n",
      "        [ 1.1946],\n",
      "        [ 1.1946],\n",
      "        [ 1.0673],\n",
      "        [-0.2696],\n",
      "        [-1.2001],\n",
      "        [ 1.0673],\n",
      "        [-1.3698],\n",
      "        [ 0.9737],\n",
      "        [-1.2754],\n",
      "        [ 0.6220],\n",
      "        [-0.1662],\n",
      "        [ 0.7930],\n",
      "        [ 0.5922],\n",
      "        [ 1.1946],\n",
      "        [ 0.7930],\n",
      "        [-0.7684],\n",
      "        [-0.0627],\n",
      "        [-1.0936],\n",
      "        [-1.1477],\n",
      "        [ 0.9737],\n",
      "        [-0.9514],\n",
      "        [ 0.6806],\n",
      "        [ 1.1946],\n",
      "        [-1.5179],\n",
      "        [ 0.1423],\n",
      "        [-0.4065],\n",
      "        [-1.2754],\n",
      "        [ 1.0214],\n",
      "        [-0.4065],\n",
      "        [-0.6073],\n",
      "        [-1.4570],\n",
      "        [ 0.6806],\n",
      "        [-1.3698],\n",
      "        [ 0.5006],\n",
      "        [-0.4404],\n",
      "        [ 0.5006],\n",
      "        [ 1.1946],\n",
      "        [ 1.1596]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 141/10000,\n",
      " train_loss: 0.0025,\n",
      " train_mae: 0.0489,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2312],\n",
      "        [-1.2312],\n",
      "        [ 0.6334],\n",
      "        [ 0.5745],\n",
      "        [-0.0411],\n",
      "        [ 0.4834],\n",
      "        [ 0.8540],\n",
      "        [ 0.9051],\n",
      "        [ 0.8540],\n",
      "        [ 0.7743],\n",
      "        [-1.5441],\n",
      "        [-1.0153],\n",
      "        [ 0.5445],\n",
      "        [-0.2461],\n",
      "        [ 0.8798],\n",
      "        [-0.2461],\n",
      "        [ 0.5445],\n",
      "        [-0.7438],\n",
      "        [-0.3483],\n",
      "        [ 0.8279],\n",
      "        [ 0.7191],\n",
      "        [ 0.7469],\n",
      "        [ 1.1348],\n",
      "        [ 1.1348],\n",
      "        [ 1.0022],\n",
      "        [-0.3483],\n",
      "        [-1.2564],\n",
      "        [ 1.0022],\n",
      "        [-1.4203],\n",
      "        [ 0.9051],\n",
      "        [-1.3292],\n",
      "        [ 0.5445],\n",
      "        [-0.2461],\n",
      "        [ 0.7191],\n",
      "        [ 0.5141],\n",
      "        [ 1.1348],\n",
      "        [ 0.7191],\n",
      "        [-0.8373],\n",
      "        [-0.1435],\n",
      "        [-1.1533],\n",
      "        [-1.2057],\n",
      "        [ 0.9051],\n",
      "        [-1.0153],\n",
      "        [ 0.6041],\n",
      "        [ 1.1348],\n",
      "        [-1.5632],\n",
      "        [ 0.0607],\n",
      "        [-0.4831],\n",
      "        [-1.3292],\n",
      "        [ 0.9545],\n",
      "        [-0.4831],\n",
      "        [-0.6801],\n",
      "        [-1.5045],\n",
      "        [ 0.6041],\n",
      "        [-1.4203],\n",
      "        [ 0.4211],\n",
      "        [-0.5164],\n",
      "        [ 0.4211],\n",
      "        [ 1.1348],\n",
      "        [ 1.0983]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 142/10000,\n",
      " train_loss: 0.0037,\n",
      " train_mae: 0.0778,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1229],\n",
      "        [-1.1229],\n",
      "        [ 0.7566],\n",
      "        [ 0.7000],\n",
      "        [ 0.0980],\n",
      "        [ 0.6122],\n",
      "        [ 0.9667],\n",
      "        [ 1.0149],\n",
      "        [ 0.9667],\n",
      "        [ 0.8910],\n",
      "        [-1.4559],\n",
      "        [-0.8959],\n",
      "        [ 0.6711],\n",
      "        [-0.1070],\n",
      "        [ 0.9910],\n",
      "        [-0.1070],\n",
      "        [ 0.6711],\n",
      "        [-0.6140],\n",
      "        [-0.2100],\n",
      "        [ 0.9419],\n",
      "        [ 0.8385],\n",
      "        [ 0.8650],\n",
      "        [ 1.2301],\n",
      "        [ 1.2301],\n",
      "        [ 1.1062],\n",
      "        [-0.2100],\n",
      "        [-1.1495],\n",
      "        [ 1.1062],\n",
      "        [-1.3236],\n",
      "        [ 1.0149],\n",
      "        [-1.2267],\n",
      "        [ 0.6711],\n",
      "        [-0.1070],\n",
      "        [ 0.8385],\n",
      "        [ 0.6419],\n",
      "        [ 1.2301],\n",
      "        [ 0.8385],\n",
      "        [-0.7106],\n",
      "        [-0.0042],\n",
      "        [-1.0407],\n",
      "        [-1.0959],\n",
      "        [ 1.0149],\n",
      "        [-0.8959],\n",
      "        [ 0.7285],\n",
      "        [ 1.2301],\n",
      "        [-1.4764],\n",
      "        [ 0.1989],\n",
      "        [-0.3467],\n",
      "        [-1.2267],\n",
      "        [ 1.0614],\n",
      "        [-0.3467],\n",
      "        [-0.5483],\n",
      "        [-1.4135],\n",
      "        [ 0.7285],\n",
      "        [-1.3236],\n",
      "        [ 0.5519],\n",
      "        [-0.3807],\n",
      "        [ 0.5519],\n",
      "        [ 1.2301],\n",
      "        [ 1.1961]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 143/10000,\n",
      " train_loss: 0.0074,\n",
      " train_mae: 0.1228,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.3247],\n",
      "        [-1.3247],\n",
      "        [ 0.5598],\n",
      "        [ 0.4982],\n",
      "        [-0.1371],\n",
      "        [ 0.4035],\n",
      "        [ 0.7908],\n",
      "        [ 0.8445],\n",
      "        [ 0.7908],\n",
      "        [ 0.7071],\n",
      "        [-1.6268],\n",
      "        [-1.1136],\n",
      "        [ 0.4670],\n",
      "        [-0.3456],\n",
      "        [ 0.8179],\n",
      "        [-0.3456],\n",
      "        [ 0.4670],\n",
      "        [-0.8453],\n",
      "        [-0.4490],\n",
      "        [ 0.7633],\n",
      "        [ 0.6493],\n",
      "        [ 0.6784],\n",
      "        [ 1.0868],\n",
      "        [ 1.0868],\n",
      "        [ 0.9467],\n",
      "        [-0.4490],\n",
      "        [-1.3492],\n",
      "        [ 0.9467],\n",
      "        [-1.5078],\n",
      "        [ 0.8445],\n",
      "        [-1.4198],\n",
      "        [ 0.4670],\n",
      "        [-0.3456],\n",
      "        [ 0.6493],\n",
      "        [ 0.4354],\n",
      "        [ 1.0868],\n",
      "        [ 0.6493],\n",
      "        [-0.9381],\n",
      "        [-0.2415],\n",
      "        [-1.2488],\n",
      "        [-1.2999],\n",
      "        [ 0.8445],\n",
      "        [-1.1136],\n",
      "        [ 0.5292],\n",
      "        [ 1.0868],\n",
      "        [-1.6451],\n",
      "        [-0.0329],\n",
      "        [-0.5848],\n",
      "        [-1.4198],\n",
      "        [ 0.8965],\n",
      "        [-0.5848],\n",
      "        [-0.7819],\n",
      "        [-1.5888],\n",
      "        [ 0.5292],\n",
      "        [-1.5078],\n",
      "        [ 0.3388],\n",
      "        [-0.6182],\n",
      "        [ 0.3388],\n",
      "        [ 1.0868],\n",
      "        [ 1.0482]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 144/10000,\n",
      " train_loss: 0.0171,\n",
      " train_mae: 0.1863,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.0253],\n",
      "        [-1.0253],\n",
      "        [ 0.8789],\n",
      "        [ 0.8242],\n",
      "        [ 0.2312],\n",
      "        [ 0.7390],\n",
      "        [ 1.0802],\n",
      "        [ 1.1260],\n",
      "        [ 1.0802],\n",
      "        [ 1.0080],\n",
      "        [-1.3776],\n",
      "        [-0.7871],\n",
      "        [ 0.7962],\n",
      "        [ 0.0249],\n",
      "        [ 1.1033],\n",
      "        [ 0.0249],\n",
      "        [ 0.7962],\n",
      "        [-0.4938],\n",
      "        [-0.0795],\n",
      "        [ 1.0566],\n",
      "        [ 0.9577],\n",
      "        [ 0.9831],\n",
      "        [ 1.3286],\n",
      "        [ 1.3286],\n",
      "        [ 1.2123],\n",
      "        [-0.0795],\n",
      "        [-1.0533],\n",
      "        [ 1.2123],\n",
      "        [-1.2373],\n",
      "        [ 1.1260],\n",
      "        [-1.1348],\n",
      "        [ 0.7962],\n",
      "        [ 0.0249],\n",
      "        [ 0.9577],\n",
      "        [ 0.7678],\n",
      "        [ 1.3286],\n",
      "        [ 0.9577],\n",
      "        [-0.5940],\n",
      "        [ 0.1286],\n",
      "        [-0.9389],\n",
      "        [-0.9969],\n",
      "        [ 1.1260],\n",
      "        [-0.7871],\n",
      "        [ 0.8518],\n",
      "        [ 1.3286],\n",
      "        [-1.3995],\n",
      "        [ 0.3320],\n",
      "        [-0.2190],\n",
      "        [-1.1348],\n",
      "        [ 1.1700],\n",
      "        [-0.2190],\n",
      "        [-0.4260],\n",
      "        [-1.3327],\n",
      "        [ 0.8518],\n",
      "        [-1.2373],\n",
      "        [ 0.6802],\n",
      "        [-0.2537],\n",
      "        [ 0.6802],\n",
      "        [ 1.3286],\n",
      "        [ 1.2967]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 145/10000,\n",
      " train_loss: 0.0373,\n",
      " train_mae: 0.1867,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.4054],\n",
      "        [-1.4054],\n",
      "        [ 0.5035],\n",
      "        [ 0.4399],\n",
      "        [-0.2138],\n",
      "        [ 0.3418],\n",
      "        [ 0.7432],\n",
      "        [ 0.7990],\n",
      "        [ 0.7432],\n",
      "        [ 0.6563],\n",
      "        [-1.6995],\n",
      "        [-1.1974],\n",
      "        [ 0.4075],\n",
      "        [-0.4264],\n",
      "        [ 0.7713],\n",
      "        [-0.4264],\n",
      "        [ 0.4075],\n",
      "        [-0.9304],\n",
      "        [-0.5313],\n",
      "        [ 0.7147],\n",
      "        [ 0.5964],\n",
      "        [ 0.6266],\n",
      "        [ 1.0509],\n",
      "        [ 1.0509],\n",
      "        [ 0.9052],\n",
      "        [-0.5313],\n",
      "        [-1.4294],\n",
      "        [ 0.9052],\n",
      "        [-1.5842],\n",
      "        [ 0.7990],\n",
      "        [-1.4985],\n",
      "        [ 0.4075],\n",
      "        [-0.4264],\n",
      "        [ 0.5964],\n",
      "        [ 0.3748],\n",
      "        [ 1.0509],\n",
      "        [ 0.5964],\n",
      "        [-1.0230],\n",
      "        [-0.3204],\n",
      "        [-1.3308],\n",
      "        [-1.3810],\n",
      "        [ 0.7990],\n",
      "        [-1.1974],\n",
      "        [ 0.4719],\n",
      "        [ 1.0509],\n",
      "        [-1.7172],\n",
      "        [-0.1073],\n",
      "        [-0.6686],\n",
      "        [-1.4985],\n",
      "        [ 0.8530],\n",
      "        [-0.6686],\n",
      "        [-0.8668],\n",
      "        [-1.6628],\n",
      "        [ 0.4719],\n",
      "        [-1.5842],\n",
      "        [ 0.2750],\n",
      "        [-0.7023],\n",
      "        [ 0.2750],\n",
      "        [ 1.0509],\n",
      "        [ 1.0107]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 146/10000,\n",
      " train_loss: 0.0376,\n",
      " train_mae: 0.1097,\n",
      " epoch_time_duration: 0.0082\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1091],\n",
      "        [-1.1091],\n",
      "        [ 0.8050],\n",
      "        [ 0.7480],\n",
      "        [ 0.1368],\n",
      "        [ 0.6594],\n",
      "        [ 1.0159],\n",
      "        [ 1.0641],\n",
      "        [ 1.0159],\n",
      "        [ 0.9401],\n",
      "        [-1.4455],\n",
      "        [-0.8783],\n",
      "        [ 0.7188],\n",
      "        [-0.0724],\n",
      "        [ 1.0402],\n",
      "        [-0.0724],\n",
      "        [ 0.7188],\n",
      "        [-0.5906],\n",
      "        [-0.1776],\n",
      "        [ 0.9911],\n",
      "        [ 0.8874],\n",
      "        [ 0.9140],\n",
      "        [ 1.2780],\n",
      "        [ 1.2780],\n",
      "        [ 1.1551],\n",
      "        [-0.1776],\n",
      "        [-1.1361],\n",
      "        [ 1.1551],\n",
      "        [-1.3123],\n",
      "        [ 1.0641],\n",
      "        [-1.2143],\n",
      "        [ 0.7188],\n",
      "        [-0.0724],\n",
      "        [ 0.8874],\n",
      "        [ 0.6893],\n",
      "        [ 1.2780],\n",
      "        [ 0.8874],\n",
      "        [-0.6893],\n",
      "        [ 0.0326],\n",
      "        [-1.0257],\n",
      "        [-1.0817],\n",
      "        [ 1.0641],\n",
      "        [-0.8783],\n",
      "        [ 0.7767],\n",
      "        [ 1.2780],\n",
      "        [-1.4662],\n",
      "        [ 0.2397],\n",
      "        [-0.3175],\n",
      "        [-1.2143],\n",
      "        [ 1.1105],\n",
      "        [-0.3175],\n",
      "        [-0.5235],\n",
      "        [-1.4029],\n",
      "        [ 0.7767],\n",
      "        [-1.3123],\n",
      "        [ 0.5983],\n",
      "        [-0.3522],\n",
      "        [ 0.5983],\n",
      "        [ 1.2780],\n",
      "        [ 1.2443]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 147/10000,\n",
      " train_loss: 0.0143,\n",
      " train_mae: 0.0494,\n",
      " epoch_time_duration: 0.0139\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1384],\n",
      "        [-1.1384],\n",
      "        [ 0.7077],\n",
      "        [ 0.6507],\n",
      "        [ 0.0513],\n",
      "        [ 0.5626],\n",
      "        [ 0.9202],\n",
      "        [ 0.9693],\n",
      "        [ 0.9202],\n",
      "        [ 0.8435],\n",
      "        [-1.4599],\n",
      "        [-0.9189],\n",
      "        [ 0.6217],\n",
      "        [-0.1504],\n",
      "        [ 0.9450],\n",
      "        [-0.1504],\n",
      "        [ 0.6217],\n",
      "        [-0.6454],\n",
      "        [-0.2514],\n",
      "        [ 0.8950],\n",
      "        [ 0.7903],\n",
      "        [ 0.8171],\n",
      "        [ 1.1896],\n",
      "        [ 1.1896],\n",
      "        [ 1.0625],\n",
      "        [-0.2514],\n",
      "        [-1.1640],\n",
      "        [ 1.0625],\n",
      "        [-1.3322],\n",
      "        [ 0.9693],\n",
      "        [-1.2386],\n",
      "        [ 0.6217],\n",
      "        [-0.1504],\n",
      "        [ 0.7903],\n",
      "        [ 0.5923],\n",
      "        [ 1.1896],\n",
      "        [ 0.7903],\n",
      "        [-0.7393],\n",
      "        [-0.0493],\n",
      "        [-1.0590],\n",
      "        [-1.1123],\n",
      "        [ 0.9693],\n",
      "        [-0.9189],\n",
      "        [ 0.6794],\n",
      "        [ 1.1896],\n",
      "        [-1.4797],\n",
      "        [ 0.1511],\n",
      "        [-0.3851],\n",
      "        [-1.2386],\n",
      "        [ 1.0167],\n",
      "        [-0.3851],\n",
      "        [-0.5815],\n",
      "        [-1.4190],\n",
      "        [ 0.6794],\n",
      "        [-1.3322],\n",
      "        [ 0.5022],\n",
      "        [-0.4183],\n",
      "        [ 0.5022],\n",
      "        [ 1.1896],\n",
      "        [ 1.1546]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 148/10000,\n",
      " train_loss: 0.0034,\n",
      " train_mae: 0.1485,\n",
      " epoch_time_duration: 0.0082\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.3190],\n",
      "        [-1.3190],\n",
      "        [ 0.5175],\n",
      "        [ 0.4568],\n",
      "        [-0.1660],\n",
      "        [ 0.3634],\n",
      "        [ 0.7467],\n",
      "        [ 0.8002],\n",
      "        [ 0.7467],\n",
      "        [ 0.6635],\n",
      "        [-1.6132],\n",
      "        [-1.1141],\n",
      "        [ 0.4259],\n",
      "        [-0.3690],\n",
      "        [ 0.7737],\n",
      "        [-0.3690],\n",
      "        [ 0.4259],\n",
      "        [-0.8539],\n",
      "        [-0.4694],\n",
      "        [ 0.7194],\n",
      "        [ 0.6062],\n",
      "        [ 0.6351],\n",
      "        [ 1.0430],\n",
      "        [ 1.0430],\n",
      "        [ 0.9024],\n",
      "        [-0.4694],\n",
      "        [-1.3427],\n",
      "        [ 0.9024],\n",
      "        [-1.4971],\n",
      "        [ 0.8002],\n",
      "        [-1.4114],\n",
      "        [ 0.4259],\n",
      "        [-0.3690],\n",
      "        [ 0.6062],\n",
      "        [ 0.3948],\n",
      "        [ 1.0430],\n",
      "        [ 0.6062],\n",
      "        [-0.9438],\n",
      "        [-0.2677],\n",
      "        [-1.2452],\n",
      "        [-1.2948],\n",
      "        [ 0.8002],\n",
      "        [-1.1141],\n",
      "        [ 0.4873],\n",
      "        [ 1.0430],\n",
      "        [-1.6311],\n",
      "        [-0.0644],\n",
      "        [-0.6012],\n",
      "        [-1.4114],\n",
      "        [ 0.8521],\n",
      "        [-0.6012],\n",
      "        [-0.7924],\n",
      "        [-1.5761],\n",
      "        [ 0.4873],\n",
      "        [-1.4971],\n",
      "        [ 0.2997],\n",
      "        [-0.6337],\n",
      "        [ 0.2997],\n",
      "        [ 1.0430],\n",
      "        [ 1.0041]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 149/10000,\n",
      " train_loss: 0.0241,\n",
      " train_mae: 0.1470,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.0992],\n",
      "        [-1.0992],\n",
      "        [ 0.8580],\n",
      "        [ 0.8005],\n",
      "        [ 0.1778],\n",
      "        [ 0.7109],\n",
      "        [ 1.0691],\n",
      "        [ 1.1170],\n",
      "        [ 1.0691],\n",
      "        [ 0.9935],\n",
      "        [-1.4371],\n",
      "        [-0.8645],\n",
      "        [ 0.7711],\n",
      "        [-0.0371],\n",
      "        [ 1.0933],\n",
      "        [-0.0371],\n",
      "        [ 0.7711],\n",
      "        [-0.5700],\n",
      "        [-0.1454],\n",
      "        [ 1.0444],\n",
      "        [ 0.9408],\n",
      "        [ 0.9674],\n",
      "        [ 1.3277],\n",
      "        [ 1.3277],\n",
      "        [ 1.2070],\n",
      "        [-0.1454],\n",
      "        [-1.1265],\n",
      "        [ 1.2070],\n",
      "        [-1.3040],\n",
      "        [ 1.1170],\n",
      "        [-1.2055],\n",
      "        [ 0.7711],\n",
      "        [-0.0371],\n",
      "        [ 0.9408],\n",
      "        [ 0.7412],\n",
      "        [ 1.3277],\n",
      "        [ 0.9408],\n",
      "        [-0.6712],\n",
      "        [ 0.0708],\n",
      "        [-1.0146],\n",
      "        [-1.0715],\n",
      "        [ 1.1170],\n",
      "        [-0.8645],\n",
      "        [ 0.8295],\n",
      "        [ 1.3277],\n",
      "        [-1.4576],\n",
      "        [ 0.2833],\n",
      "        [-0.2892],\n",
      "        [-1.2055],\n",
      "        [ 1.1630],\n",
      "        [-0.2892],\n",
      "        [-0.5011],\n",
      "        [-1.3947],\n",
      "        [ 0.8295],\n",
      "        [-1.3040],\n",
      "        [ 0.6491],\n",
      "        [-0.3250],\n",
      "        [ 0.6491],\n",
      "        [ 1.3277],\n",
      "        [ 1.2948]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 150/10000,\n",
      " train_loss: 0.0248,\n",
      " train_mae: 0.0457,\n",
      " epoch_time_duration: 0.0077\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2266],\n",
      "        [-1.2266],\n",
      "        [ 0.7031],\n",
      "        [ 0.6430],\n",
      "        [ 0.0070],\n",
      "        [ 0.5498],\n",
      "        [ 0.9263],\n",
      "        [ 0.9775],\n",
      "        [ 0.9263],\n",
      "        [ 0.8460],\n",
      "        [-1.5414],\n",
      "        [-1.0056],\n",
      "        [ 0.6123],\n",
      "        [-0.2066],\n",
      "        [ 0.9521],\n",
      "        [-0.2066],\n",
      "        [ 0.6123],\n",
      "        [-0.7250],\n",
      "        [-0.3131],\n",
      "        [ 0.9000],\n",
      "        [ 0.7901],\n",
      "        [ 0.8183],\n",
      "        [ 1.2052],\n",
      "        [ 1.2052],\n",
      "        [ 1.0743],\n",
      "        [-0.3131],\n",
      "        [-1.2521],\n",
      "        [ 1.0743],\n",
      "        [-1.4178],\n",
      "        [ 0.9775],\n",
      "        [-1.3259],\n",
      "        [ 0.6123],\n",
      "        [-0.2066],\n",
      "        [ 0.7901],\n",
      "        [ 0.5812],\n",
      "        [ 1.2052],\n",
      "        [ 0.7901],\n",
      "        [-0.8219],\n",
      "        [-0.0997],\n",
      "        [-1.1471],\n",
      "        [-1.2005],\n",
      "        [ 0.9775],\n",
      "        [-1.0056],\n",
      "        [ 0.6732],\n",
      "        [ 1.2052],\n",
      "        [-1.5604],\n",
      "        [ 0.1129],\n",
      "        [-0.4537],\n",
      "        [-1.3259],\n",
      "        [ 1.0268],\n",
      "        [-0.4537],\n",
      "        [-0.6588],\n",
      "        [-1.5020],\n",
      "        [ 0.6732],\n",
      "        [-1.4178],\n",
      "        [ 0.4858],\n",
      "        [-0.4885],\n",
      "        [ 0.4858],\n",
      "        [ 1.2052],\n",
      "        [ 1.1693]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 151/10000,\n",
      " train_loss: 0.0025,\n",
      " train_mae: 0.1446,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2785],\n",
      "        [-1.2785],\n",
      "        [ 0.5075],\n",
      "        [ 0.4485],\n",
      "        [-0.1550],\n",
      "        [ 0.3578],\n",
      "        [ 0.7304],\n",
      "        [ 0.7826],\n",
      "        [ 0.7304],\n",
      "        [ 0.6494],\n",
      "        [-1.5697],\n",
      "        [-1.0772],\n",
      "        [ 0.4185],\n",
      "        [-0.3518],\n",
      "        [ 0.7567],\n",
      "        [-0.3518],\n",
      "        [ 0.4185],\n",
      "        [-0.8231],\n",
      "        [-0.4492],\n",
      "        [ 0.7038],\n",
      "        [ 0.5936],\n",
      "        [ 0.6217],\n",
      "        [ 1.0200],\n",
      "        [ 1.0200],\n",
      "        [ 0.8823],\n",
      "        [-0.4492],\n",
      "        [-1.3019],\n",
      "        [ 0.8823],\n",
      "        [-1.4545],\n",
      "        [ 0.7826],\n",
      "        [-1.3697],\n",
      "        [ 0.4185],\n",
      "        [-0.3518],\n",
      "        [ 0.5936],\n",
      "        [ 0.3883],\n",
      "        [ 1.0200],\n",
      "        [ 0.5936],\n",
      "        [-0.9108],\n",
      "        [-0.2536],\n",
      "        [-1.2060],\n",
      "        [-1.2547],\n",
      "        [ 0.7826],\n",
      "        [-1.0772],\n",
      "        [ 0.4781],\n",
      "        [ 1.0200],\n",
      "        [-1.5876],\n",
      "        [-0.0566],\n",
      "        [-0.5772],\n",
      "        [-1.3697],\n",
      "        [ 0.8332],\n",
      "        [-0.5772],\n",
      "        [-0.7631],\n",
      "        [-1.5329],\n",
      "        [ 0.4781],\n",
      "        [-1.4545],\n",
      "        [ 0.2961],\n",
      "        [-0.6087],\n",
      "        [ 0.2961],\n",
      "        [ 1.0200],\n",
      "        [ 0.9818]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 152/10000,\n",
      " train_loss: 0.0233,\n",
      " train_mae: 0.1436,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.0553],\n",
      "        [-1.0553],\n",
      "        [ 0.8302],\n",
      "        [ 0.7744],\n",
      "        [ 0.1747],\n",
      "        [ 0.6877],\n",
      "        [ 1.0357],\n",
      "        [ 1.0826],\n",
      "        [ 1.0357],\n",
      "        [ 0.9619],\n",
      "        [-1.3870],\n",
      "        [-0.8273],\n",
      "        [ 0.7459],\n",
      "        [-0.0314],\n",
      "        [ 1.0593],\n",
      "        [-0.0314],\n",
      "        [ 0.7459],\n",
      "        [-0.5430],\n",
      "        [-0.1352],\n",
      "        [ 1.0115],\n",
      "        [ 0.9105],\n",
      "        [ 0.9365],\n",
      "        [ 1.2899],\n",
      "        [ 1.2899],\n",
      "        [ 1.1709],\n",
      "        [-0.1352],\n",
      "        [-1.0819],\n",
      "        [ 1.1709],\n",
      "        [-1.2557],\n",
      "        [ 1.0826],\n",
      "        [-1.1591],\n",
      "        [ 0.7459],\n",
      "        [-0.0314],\n",
      "        [ 0.9105],\n",
      "        [ 0.7170],\n",
      "        [ 1.2899],\n",
      "        [ 0.9105],\n",
      "        [-0.6406],\n",
      "        [ 0.0721],\n",
      "        [-0.9729],\n",
      "        [-1.0283],\n",
      "        [ 1.0826],\n",
      "        [-0.8273],\n",
      "        [ 0.8025],\n",
      "        [ 1.2899],\n",
      "        [-1.4074],\n",
      "        [ 0.2760],\n",
      "        [-0.2732],\n",
      "        [-1.1591],\n",
      "        [ 1.1276],\n",
      "        [-0.2732],\n",
      "        [-0.4767],\n",
      "        [-1.3451],\n",
      "        [ 0.8025],\n",
      "        [-1.2557],\n",
      "        [ 0.6280],\n",
      "        [-0.3075],\n",
      "        [ 0.6280],\n",
      "        [ 1.2899],\n",
      "        [ 1.2573]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 153/10000,\n",
      " train_loss: 0.0230,\n",
      " train_mae: 0.0590,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2449],\n",
      "        [-1.2449],\n",
      "        [ 0.7340],\n",
      "        [ 0.6730],\n",
      "        [ 0.0214],\n",
      "        [ 0.5782],\n",
      "        [ 0.9587],\n",
      "        [ 1.0098],\n",
      "        [ 0.9587],\n",
      "        [ 0.8781],\n",
      "        [-1.5595],\n",
      "        [-1.0207],\n",
      "        [ 0.6419],\n",
      "        [-0.1991],\n",
      "        [ 0.9845],\n",
      "        [-0.1991],\n",
      "        [ 0.6419],\n",
      "        [-0.7336],\n",
      "        [-0.3091],\n",
      "        [ 0.9323],\n",
      "        [ 0.8219],\n",
      "        [ 0.8503],\n",
      "        [ 1.2352],\n",
      "        [ 1.2352],\n",
      "        [ 1.1060],\n",
      "        [-0.3091],\n",
      "        [-1.2706],\n",
      "        [ 1.1060],\n",
      "        [-1.4367],\n",
      "        [ 1.0098],\n",
      "        [-1.3449],\n",
      "        [ 0.6419],\n",
      "        [-0.1991],\n",
      "        [ 0.8219],\n",
      "        [ 0.6103],\n",
      "        [ 1.2352],\n",
      "        [ 0.8219],\n",
      "        [-0.8330],\n",
      "        [-0.0887],\n",
      "        [-1.1645],\n",
      "        [-1.2186],\n",
      "        [ 1.0098],\n",
      "        [-1.0207],\n",
      "        [ 0.7037],\n",
      "        [ 1.2352],\n",
      "        [-1.5782],\n",
      "        [ 0.1305],\n",
      "        [-0.4542],\n",
      "        [-1.3449],\n",
      "        [ 1.0590],\n",
      "        [-0.4542],\n",
      "        [-0.6655],\n",
      "        [-1.5205],\n",
      "        [ 0.7037],\n",
      "        [-1.4367],\n",
      "        [ 0.5130],\n",
      "        [-0.4900],\n",
      "        [ 0.5130],\n",
      "        [ 1.2352],\n",
      "        [ 1.1999]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 154/10000,\n",
      " train_loss: 0.0044,\n",
      " train_mae: 0.1425,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.3296],\n",
      "        [-1.3296],\n",
      "        [ 0.5339],\n",
      "        [ 0.4727],\n",
      "        [-0.1598],\n",
      "        [ 0.3783],\n",
      "        [ 0.7635],\n",
      "        [ 0.8167],\n",
      "        [ 0.7635],\n",
      "        [ 0.6804],\n",
      "        [-1.6207],\n",
      "        [-1.1241],\n",
      "        [ 0.4415],\n",
      "        [-0.3670],\n",
      "        [ 0.7903],\n",
      "        [-0.3670],\n",
      "        [ 0.4415],\n",
      "        [-0.8610],\n",
      "        [-0.4695],\n",
      "        [ 0.7362],\n",
      "        [ 0.6229],\n",
      "        [ 0.6519],\n",
      "        [ 1.0564],\n",
      "        [ 1.0564],\n",
      "        [ 0.9180],\n",
      "        [-0.4695],\n",
      "        [-1.3533],\n",
      "        [ 0.9180],\n",
      "        [-1.5065],\n",
      "        [ 0.8167],\n",
      "        [-1.4216],\n",
      "        [ 0.4415],\n",
      "        [-0.3670],\n",
      "        [ 0.6229],\n",
      "        [ 0.4101],\n",
      "        [ 1.0564],\n",
      "        [ 0.6229],\n",
      "        [-0.9522],\n",
      "        [-0.2636],\n",
      "        [-1.2558],\n",
      "        [-1.3054],\n",
      "        [ 0.8167],\n",
      "        [-1.1241],\n",
      "        [ 0.5034],\n",
      "        [ 1.0564],\n",
      "        [-1.6382],\n",
      "        [-0.0562],\n",
      "        [-0.6040],\n",
      "        [-1.4216],\n",
      "        [ 0.8682],\n",
      "        [-0.6040],\n",
      "        [-0.7985],\n",
      "        [-1.5843],\n",
      "        [ 0.5034],\n",
      "        [-1.5065],\n",
      "        [ 0.3139],\n",
      "        [-0.6371],\n",
      "        [ 0.3139],\n",
      "        [ 1.0564],\n",
      "        [ 1.0183]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 155/10000,\n",
      " train_loss: 0.0225,\n",
      " train_mae: 0.1049,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.0584],\n",
      "        [-1.0584],\n",
      "        [ 0.7640],\n",
      "        [ 0.7091],\n",
      "        [ 0.1247],\n",
      "        [ 0.6240],\n",
      "        [ 0.9674],\n",
      "        [ 1.0141],\n",
      "        [ 0.9674],\n",
      "        [ 0.8942],\n",
      "        [-1.3790],\n",
      "        [-0.8391],\n",
      "        [ 0.6811],\n",
      "        [-0.0743],\n",
      "        [ 0.9910],\n",
      "        [-0.0743],\n",
      "        [ 0.6811],\n",
      "        [-0.5661],\n",
      "        [-0.1743],\n",
      "        [ 0.9435],\n",
      "        [ 0.8433],\n",
      "        [ 0.8690],\n",
      "        [ 1.2221],\n",
      "        [ 1.2221],\n",
      "        [ 1.1024],\n",
      "        [-0.1743],\n",
      "        [-1.0840],\n",
      "        [ 1.1024],\n",
      "        [-1.2518],\n",
      "        [ 1.0141],\n",
      "        [-1.1585],\n",
      "        [ 0.6811],\n",
      "        [-0.0743],\n",
      "        [ 0.8433],\n",
      "        [ 0.6527],\n",
      "        [ 1.2221],\n",
      "        [ 0.8433],\n",
      "        [-0.6598],\n",
      "        [ 0.0255],\n",
      "        [-0.9791],\n",
      "        [-1.0324],\n",
      "        [ 1.0141],\n",
      "        [-0.8391],\n",
      "        [ 0.7367],\n",
      "        [ 1.2221],\n",
      "        [-1.3988],\n",
      "        [ 0.2228],\n",
      "        [-0.3070],\n",
      "        [-1.1585],\n",
      "        [ 1.0591],\n",
      "        [-0.3070],\n",
      "        [-0.5025],\n",
      "        [-1.3383],\n",
      "        [ 0.7367],\n",
      "        [-1.2518],\n",
      "        [ 0.5655],\n",
      "        [-0.3400],\n",
      "        [ 0.5655],\n",
      "        [ 1.2221],\n",
      "        [ 1.1892]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 156/10000,\n",
      " train_loss: 0.0130,\n",
      " train_mae: 0.0628,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1517],\n",
      "        [-1.1517],\n",
      "        [ 0.7491],\n",
      "        [ 0.6914],\n",
      "        [ 0.0740],\n",
      "        [ 0.6017],\n",
      "        [ 0.9622],\n",
      "        [ 1.0108],\n",
      "        [ 0.9622],\n",
      "        [ 0.8857],\n",
      "        [-1.4681],\n",
      "        [-0.9302],\n",
      "        [ 0.6619],\n",
      "        [-0.1360],\n",
      "        [ 0.9867],\n",
      "        [-0.1360],\n",
      "        [ 0.6619],\n",
      "        [-0.6499],\n",
      "        [-0.2411],\n",
      "        [ 0.9371],\n",
      "        [ 0.8324],\n",
      "        [ 0.8593],\n",
      "        [ 1.2257],\n",
      "        [ 1.2257],\n",
      "        [ 1.1024],\n",
      "        [-0.2411],\n",
      "        [-1.1774],\n",
      "        [ 1.1024],\n",
      "        [-1.3438],\n",
      "        [ 1.0108],\n",
      "        [-1.2515],\n",
      "        [ 0.6619],\n",
      "        [-0.1360],\n",
      "        [ 0.8324],\n",
      "        [ 0.6320],\n",
      "        [ 1.2257],\n",
      "        [ 0.8324],\n",
      "        [-0.7465],\n",
      "        [-0.0307],\n",
      "        [-1.0720],\n",
      "        [-1.1256],\n",
      "        [ 1.0108],\n",
      "        [-0.9302],\n",
      "        [ 0.7204],\n",
      "        [ 1.2257],\n",
      "        [-1.4872],\n",
      "        [ 0.1776],\n",
      "        [-0.3802],\n",
      "        [-1.2515],\n",
      "        [ 1.0575],\n",
      "        [-0.3802],\n",
      "        [-0.5839],\n",
      "        [-1.4285],\n",
      "        [ 0.7204],\n",
      "        [-1.3438],\n",
      "        [ 0.5399],\n",
      "        [-0.4147],\n",
      "        [ 0.5399],\n",
      "        [ 1.2257],\n",
      "        [ 1.1920]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 157/10000,\n",
      " train_loss: 0.0055,\n",
      " train_mae: 0.1232,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.3501],\n",
      "        [-1.3501],\n",
      "        [ 0.5758],\n",
      "        [ 0.5133],\n",
      "        [-0.1406],\n",
      "        [ 0.4166],\n",
      "        [ 0.8084],\n",
      "        [ 0.8619],\n",
      "        [ 0.8084],\n",
      "        [ 0.7246],\n",
      "        [-1.6412],\n",
      "        [-1.1408],\n",
      "        [ 0.4814],\n",
      "        [-0.3564],\n",
      "        [ 0.8354],\n",
      "        [-0.3564],\n",
      "        [ 0.4814],\n",
      "        [-0.8699],\n",
      "        [-0.4632],\n",
      "        [ 0.7809],\n",
      "        [ 0.6664],\n",
      "        [ 0.6957],\n",
      "        [ 1.0999],\n",
      "        [ 1.0999],\n",
      "        [ 0.9630],\n",
      "        [-0.4632],\n",
      "        [-1.3740],\n",
      "        [ 0.9630],\n",
      "        [-1.5279],\n",
      "        [ 0.8619],\n",
      "        [-1.4429],\n",
      "        [ 0.4814],\n",
      "        [-0.3564],\n",
      "        [ 0.6664],\n",
      "        [ 0.4492],\n",
      "        [ 1.0999],\n",
      "        [ 0.6664],\n",
      "        [-0.9641],\n",
      "        [-0.2487],\n",
      "        [-1.2753],\n",
      "        [-1.3256],\n",
      "        [ 0.8619],\n",
      "        [-1.1408],\n",
      "        [ 0.5447],\n",
      "        [ 1.0999],\n",
      "        [-1.6585],\n",
      "        [-0.0328],\n",
      "        [-0.6032],\n",
      "        [-1.4429],\n",
      "        [ 0.9134],\n",
      "        [-0.6032],\n",
      "        [-0.8053],\n",
      "        [-1.6053],\n",
      "        [ 0.5447],\n",
      "        [-1.5279],\n",
      "        [ 0.3503],\n",
      "        [-0.6376],\n",
      "        [ 0.3503],\n",
      "        [ 1.0999],\n",
      "        [ 1.0623]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 158/10000,\n",
      " train_loss: 0.0182,\n",
      " train_mae: 0.0443,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1751],\n",
      "        [-1.1751],\n",
      "        [ 0.7124],\n",
      "        [ 0.6543],\n",
      "        [ 0.0363],\n",
      "        [ 0.5642],\n",
      "        [ 0.9270],\n",
      "        [ 0.9761],\n",
      "        [ 0.9270],\n",
      "        [ 0.8499],\n",
      "        [-1.4843],\n",
      "        [-0.9577],\n",
      "        [ 0.6247],\n",
      "        [-0.1727],\n",
      "        [ 0.9518],\n",
      "        [-0.1727],\n",
      "        [ 0.6247],\n",
      "        [-0.6817],\n",
      "        [-0.2771],\n",
      "        [ 0.9018],\n",
      "        [ 0.7962],\n",
      "        [ 0.8233],\n",
      "        [ 1.1933],\n",
      "        [ 1.1933],\n",
      "        [ 1.0685],\n",
      "        [-0.2771],\n",
      "        [-1.2002],\n",
      "        [ 1.0685],\n",
      "        [-1.3630],\n",
      "        [ 0.9761],\n",
      "        [-1.2728],\n",
      "        [ 0.6247],\n",
      "        [-0.1727],\n",
      "        [ 0.7962],\n",
      "        [ 0.5947],\n",
      "        [ 1.1933],\n",
      "        [ 0.7962],\n",
      "        [-0.7770],\n",
      "        [-0.0680],\n",
      "        [-1.0970],\n",
      "        [-1.1495],\n",
      "        [ 0.9761],\n",
      "        [-0.9577],\n",
      "        [ 0.6836],\n",
      "        [ 1.1933],\n",
      "        [-1.5029],\n",
      "        [ 0.1397],\n",
      "        [-0.4151],\n",
      "        [-1.2728],\n",
      "        [ 1.0232],\n",
      "        [-0.4151],\n",
      "        [-0.6166],\n",
      "        [-1.4457],\n",
      "        [ 0.6836],\n",
      "        [-1.3630],\n",
      "        [ 0.5023],\n",
      "        [-0.4492],\n",
      "        [ 0.5023],\n",
      "        [ 1.1933],\n",
      "        [ 1.1591]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 159/10000,\n",
      " train_loss: 0.0028,\n",
      " train_mae: 0.1015,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.0669],\n",
      "        [-1.0669],\n",
      "        [ 0.7635],\n",
      "        [ 0.7085],\n",
      "        [ 0.1202],\n",
      "        [ 0.6231],\n",
      "        [ 0.9669],\n",
      "        [ 1.0134],\n",
      "        [ 0.9669],\n",
      "        [ 0.8938],\n",
      "        [-1.3829],\n",
      "        [-0.8487],\n",
      "        [ 0.6804],\n",
      "        [-0.0805],\n",
      "        [ 0.9904],\n",
      "        [-0.0805],\n",
      "        [ 0.6804],\n",
      "        [-0.5755],\n",
      "        [-0.1813],\n",
      "        [ 0.9430],\n",
      "        [ 0.8430],\n",
      "        [ 0.8686],\n",
      "        [ 1.2197],\n",
      "        [ 1.2197],\n",
      "        [ 1.1011],\n",
      "        [-0.1813],\n",
      "        [-1.0923],\n",
      "        [ 1.1011],\n",
      "        [-1.2580],\n",
      "        [ 1.0134],\n",
      "        [-1.1659],\n",
      "        [ 0.6804],\n",
      "        [-0.0805],\n",
      "        [ 0.8430],\n",
      "        [ 0.6519],\n",
      "        [ 1.2197],\n",
      "        [ 0.8430],\n",
      "        [-0.6694],\n",
      "        [ 0.0202],\n",
      "        [-0.9881],\n",
      "        [-1.0411],\n",
      "        [ 1.0134],\n",
      "        [-0.8487],\n",
      "        [ 0.7362],\n",
      "        [ 1.2197],\n",
      "        [-1.4022],\n",
      "        [ 0.2191],\n",
      "        [-0.3150],\n",
      "        [-1.1659],\n",
      "        [ 1.0582],\n",
      "        [-0.3150],\n",
      "        [-0.5116],\n",
      "        [-1.3430],\n",
      "        [ 0.7362],\n",
      "        [-1.2580],\n",
      "        [ 0.5643],\n",
      "        [-0.3482],\n",
      "        [ 0.5643],\n",
      "        [ 1.2197],\n",
      "        [ 1.1872]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 160/10000,\n",
      " train_loss: 0.0122,\n",
      " train_mae: 0.0790,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2728],\n",
      "        [-1.2728],\n",
      "        [ 0.6087],\n",
      "        [ 0.5486],\n",
      "        [-0.0825],\n",
      "        [ 0.4556],\n",
      "        [ 0.8321],\n",
      "        [ 0.8834],\n",
      "        [ 0.8321],\n",
      "        [ 0.7516],\n",
      "        [-1.5664],\n",
      "        [-1.0638],\n",
      "        [ 0.5180],\n",
      "        [-0.2923],\n",
      "        [ 0.8580],\n",
      "        [-0.2923],\n",
      "        [ 0.5180],\n",
      "        [-0.7955],\n",
      "        [-0.3964],\n",
      "        [ 0.8057],\n",
      "        [ 0.6958],\n",
      "        [ 0.7239],\n",
      "        [ 1.1118],\n",
      "        [ 1.1118],\n",
      "        [ 0.9805],\n",
      "        [-0.3964],\n",
      "        [-1.2968],\n",
      "        [ 0.9805],\n",
      "        [-1.4517],\n",
      "        [ 0.8834],\n",
      "        [-1.3660],\n",
      "        [ 0.5180],\n",
      "        [-0.2923],\n",
      "        [ 0.6958],\n",
      "        [ 0.4870],\n",
      "        [ 1.1118],\n",
      "        [ 0.6958],\n",
      "        [-0.8885],\n",
      "        [-0.1874],\n",
      "        [-1.1979],\n",
      "        [-1.2483],\n",
      "        [ 0.8834],\n",
      "        [-1.0638],\n",
      "        [ 0.5789],\n",
      "        [ 1.1118],\n",
      "        [-1.5840],\n",
      "        [ 0.0219],\n",
      "        [-0.5333],\n",
      "        [-1.3660],\n",
      "        [ 0.9329],\n",
      "        [-0.5333],\n",
      "        [-0.7317],\n",
      "        [-1.5300],\n",
      "        [ 0.5789],\n",
      "        [-1.4517],\n",
      "        [ 0.3919],\n",
      "        [-0.5670],\n",
      "        [ 0.3919],\n",
      "        [ 1.1118],\n",
      "        [ 1.0758]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 161/10000,\n",
      " train_loss: 0.0080,\n",
      " train_mae: 0.0546,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2699],\n",
      "        [-1.2699],\n",
      "        [ 0.6584],\n",
      "        [ 0.5977],\n",
      "        [-0.0465],\n",
      "        [ 0.5035],\n",
      "        [ 0.8826],\n",
      "        [ 0.9337],\n",
      "        [ 0.8826],\n",
      "        [ 0.8021],\n",
      "        [-1.5668],\n",
      "        [-1.0566],\n",
      "        [ 0.5667],\n",
      "        [-0.2624],\n",
      "        [ 0.9084],\n",
      "        [-0.2624],\n",
      "        [ 0.5667],\n",
      "        [-0.7810],\n",
      "        [-0.3698],\n",
      "        [ 0.8562],\n",
      "        [ 0.7460],\n",
      "        [ 0.7743],\n",
      "        [ 1.1594],\n",
      "        [ 1.1594],\n",
      "        [ 1.0300],\n",
      "        [-0.3698],\n",
      "        [-1.2944],\n",
      "        [ 1.0300],\n",
      "        [-1.4513],\n",
      "        [ 0.9337],\n",
      "        [-1.3647],\n",
      "        [ 0.5667],\n",
      "        [-0.2624],\n",
      "        [ 0.7460],\n",
      "        [ 0.5353],\n",
      "        [ 1.1594],\n",
      "        [ 0.7460],\n",
      "        [-0.8767],\n",
      "        [-0.1544],\n",
      "        [-1.1936],\n",
      "        [-1.2450],\n",
      "        [ 0.9337],\n",
      "        [-1.0566],\n",
      "        [ 0.6283],\n",
      "        [ 1.1594],\n",
      "        [-1.5844],\n",
      "        [ 0.0608],\n",
      "        [-0.5109],\n",
      "        [-1.3647],\n",
      "        [ 0.9829],\n",
      "        [-0.5109],\n",
      "        [-0.7154],\n",
      "        [-1.5302],\n",
      "        [ 0.6283],\n",
      "        [-1.4513],\n",
      "        [ 0.4388],\n",
      "        [-0.5456],\n",
      "        [ 0.4388],\n",
      "        [ 1.1594],\n",
      "        [ 1.1240]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 162/10000,\n",
      " train_loss: 0.0047,\n",
      " train_mae: 0.0908,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1086],\n",
      "        [-1.1086],\n",
      "        [ 0.7753],\n",
      "        [ 0.7188],\n",
      "        [ 0.1101],\n",
      "        [ 0.6308],\n",
      "        [ 0.9829],\n",
      "        [ 1.0301],\n",
      "        [ 0.9829],\n",
      "        [ 0.9086],\n",
      "        [-1.4216],\n",
      "        [-0.8885],\n",
      "        [ 0.6899],\n",
      "        [-0.0983],\n",
      "        [ 1.0068],\n",
      "        [-0.0983],\n",
      "        [ 0.6899],\n",
      "        [-0.6096],\n",
      "        [-0.2028],\n",
      "        [ 0.9586],\n",
      "        [ 0.8566],\n",
      "        [ 0.8828],\n",
      "        [ 1.2377],\n",
      "        [ 1.2377],\n",
      "        [ 1.1188],\n",
      "        [-0.2028],\n",
      "        [-1.1341],\n",
      "        [ 1.1188],\n",
      "        [-1.2988],\n",
      "        [ 1.0301],\n",
      "        [-1.2075],\n",
      "        [ 0.6899],\n",
      "        [-0.0983],\n",
      "        [ 0.8566],\n",
      "        [ 0.6606],\n",
      "        [ 1.2377],\n",
      "        [ 0.8566],\n",
      "        [-0.7058],\n",
      "        [ 0.0062],\n",
      "        [-1.0295],\n",
      "        [-1.0827],\n",
      "        [ 1.0301],\n",
      "        [-0.8885],\n",
      "        [ 0.7472],\n",
      "        [ 1.2377],\n",
      "        [-1.4404],\n",
      "        [ 0.2127],\n",
      "        [-0.3412],\n",
      "        [-1.2075],\n",
      "        [ 1.0754],\n",
      "        [-0.3412],\n",
      "        [-0.5439],\n",
      "        [-1.3825],\n",
      "        [ 0.7472],\n",
      "        [-1.2988],\n",
      "        [ 0.5702],\n",
      "        [-0.3754],\n",
      "        [ 0.5702],\n",
      "        [ 1.2377],\n",
      "        [ 1.2052]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 163/10000,\n",
      " train_loss: 0.0102,\n",
      " train_mae: 0.0378,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1752],\n",
      "        [-1.1752],\n",
      "        [ 0.6723],\n",
      "        [ 0.6148],\n",
      "        [ 0.0059],\n",
      "        [ 0.5257],\n",
      "        [ 0.8853],\n",
      "        [ 0.9341],\n",
      "        [ 0.8853],\n",
      "        [ 0.8087],\n",
      "        [-1.4752],\n",
      "        [-0.9641],\n",
      "        [ 0.5854],\n",
      "        [-0.1989],\n",
      "        [ 0.9100],\n",
      "        [-0.1989],\n",
      "        [ 0.5854],\n",
      "        [-0.6956],\n",
      "        [-0.3010],\n",
      "        [ 0.8602],\n",
      "        [ 0.7554],\n",
      "        [ 0.7823],\n",
      "        [ 1.1509],\n",
      "        [ 1.1509],\n",
      "        [ 1.0263],\n",
      "        [-0.3010],\n",
      "        [-1.1996],\n",
      "        [ 1.0263],\n",
      "        [-1.3575],\n",
      "        [ 0.9341],\n",
      "        [-1.2700],\n",
      "        [ 0.5854],\n",
      "        [-0.1989],\n",
      "        [ 0.7554],\n",
      "        [ 0.5557],\n",
      "        [ 1.1509],\n",
      "        [ 0.7554],\n",
      "        [-0.7884],\n",
      "        [-0.0964],\n",
      "        [-1.0994],\n",
      "        [-1.1504],\n",
      "        [ 0.9341],\n",
      "        [-0.9641],\n",
      "        [ 0.6437],\n",
      "        [ 1.1509],\n",
      "        [-1.4933],\n",
      "        [ 0.1074],\n",
      "        [-0.4358],\n",
      "        [-1.2700],\n",
      "        [ 0.9811],\n",
      "        [-0.4358],\n",
      "        [-0.6322],\n",
      "        [-1.4377],\n",
      "        [ 0.6437],\n",
      "        [-1.3575],\n",
      "        [ 0.4644],\n",
      "        [-0.4690],\n",
      "        [ 0.4644],\n",
      "        [ 1.1509],\n",
      "        [ 1.1167]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 164/10000,\n",
      " train_loss: 0.0025,\n",
      " train_mae: 0.0850,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2689],\n",
      "        [-1.2689],\n",
      "        [ 0.5994],\n",
      "        [ 0.5396],\n",
      "        [-0.0884],\n",
      "        [ 0.4471],\n",
      "        [ 0.8216],\n",
      "        [ 0.8726],\n",
      "        [ 0.8216],\n",
      "        [ 0.7416],\n",
      "        [-1.5584],\n",
      "        [-1.0623],\n",
      "        [ 0.5091],\n",
      "        [-0.2970],\n",
      "        [ 0.8474],\n",
      "        [-0.2970],\n",
      "        [ 0.5091],\n",
      "        [-0.7966],\n",
      "        [-0.4005],\n",
      "        [ 0.7954],\n",
      "        [ 0.6860],\n",
      "        [ 0.7140],\n",
      "        [ 1.0996],\n",
      "        [ 1.0996],\n",
      "        [ 0.9691],\n",
      "        [-0.4005],\n",
      "        [-1.2926],\n",
      "        [ 0.9691],\n",
      "        [-1.4455],\n",
      "        [ 0.8726],\n",
      "        [-1.3610],\n",
      "        [ 0.5091],\n",
      "        [-0.2970],\n",
      "        [ 0.6860],\n",
      "        [ 0.4783],\n",
      "        [ 1.0996],\n",
      "        [ 0.6860],\n",
      "        [-0.8888],\n",
      "        [-0.1927],\n",
      "        [-1.1949],\n",
      "        [-1.2447],\n",
      "        [ 0.8726],\n",
      "        [-1.0623],\n",
      "        [ 0.5697],\n",
      "        [ 1.0996],\n",
      "        [-1.5757],\n",
      "        [ 0.0155],\n",
      "        [-0.5364],\n",
      "        [-1.3610],\n",
      "        [ 0.9218],\n",
      "        [-0.5364],\n",
      "        [-0.7334],\n",
      "        [-1.5226],\n",
      "        [ 0.5697],\n",
      "        [-1.4455],\n",
      "        [ 0.3836],\n",
      "        [-0.5699],\n",
      "        [ 0.3836],\n",
      "        [ 1.0996],\n",
      "        [ 1.0638]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 165/10000,\n",
      " train_loss: 0.0089,\n",
      " train_mae: 0.0557,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1752],\n",
      "        [-1.1752],\n",
      "        [ 0.7412],\n",
      "        [ 0.6829],\n",
      "        [ 0.0557],\n",
      "        [ 0.5922],\n",
      "        [ 0.9551],\n",
      "        [ 1.0037],\n",
      "        [ 0.9551],\n",
      "        [ 0.8786],\n",
      "        [-1.4806],\n",
      "        [-0.9571],\n",
      "        [ 0.6531],\n",
      "        [-0.1579],\n",
      "        [ 0.9797],\n",
      "        [-0.1579],\n",
      "        [ 0.6531],\n",
      "        [-0.6775],\n",
      "        [-0.2647],\n",
      "        [ 0.9301],\n",
      "        [ 0.8251],\n",
      "        [ 0.8521],\n",
      "        [ 1.2164],\n",
      "        [ 1.2164],\n",
      "        [ 1.0947],\n",
      "        [-0.2647],\n",
      "        [-1.2003],\n",
      "        [ 1.0947],\n",
      "        [-1.3616],\n",
      "        [ 1.0037],\n",
      "        [-1.2724],\n",
      "        [ 0.6531],\n",
      "        [-0.1579],\n",
      "        [ 0.8251],\n",
      "        [ 0.6229],\n",
      "        [ 1.2164],\n",
      "        [ 0.8251],\n",
      "        [-0.7743],\n",
      "        [-0.0509],\n",
      "        [-1.0971],\n",
      "        [-1.1497],\n",
      "        [ 1.0037],\n",
      "        [-0.9571],\n",
      "        [ 0.7123],\n",
      "        [ 1.2164],\n",
      "        [-1.4987],\n",
      "        [ 0.1612],\n",
      "        [-0.4057],\n",
      "        [-1.2724],\n",
      "        [ 1.0502],\n",
      "        [-0.4057],\n",
      "        [-0.6112],\n",
      "        [-1.4428],\n",
      "        [ 0.7123],\n",
      "        [-1.3616],\n",
      "        [ 0.5296],\n",
      "        [-0.4405],\n",
      "        [ 0.5296],\n",
      "        [ 1.2164],\n",
      "        [ 1.1832]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 166/10000,\n",
      " train_loss: 0.0045,\n",
      " train_mae: 0.0619,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1521],\n",
      "        [-1.1521],\n",
      "        [ 0.7460],\n",
      "        [ 0.6885],\n",
      "        [ 0.0698],\n",
      "        [ 0.5990],\n",
      "        [ 0.9571],\n",
      "        [ 1.0051],\n",
      "        [ 0.9571],\n",
      "        [ 0.8816],\n",
      "        [-1.4587],\n",
      "        [-0.9342],\n",
      "        [ 0.6591],\n",
      "        [-0.1412],\n",
      "        [ 0.9813],\n",
      "        [-0.1412],\n",
      "        [ 0.6591],\n",
      "        [-0.6560],\n",
      "        [-0.2469],\n",
      "        [ 0.9324],\n",
      "        [ 0.8287],\n",
      "        [ 0.8554],\n",
      "        [ 1.2155],\n",
      "        [ 1.2155],\n",
      "        [ 1.0950],\n",
      "        [-0.2469],\n",
      "        [-1.1772],\n",
      "        [ 1.0950],\n",
      "        [-1.3390],\n",
      "        [ 1.0051],\n",
      "        [-1.2494],\n",
      "        [ 0.6591],\n",
      "        [-0.1412],\n",
      "        [ 0.8287],\n",
      "        [ 0.6293],\n",
      "        [ 1.2155],\n",
      "        [ 0.8287],\n",
      "        [-0.7522],\n",
      "        [-0.0355],\n",
      "        [-1.0739],\n",
      "        [-1.1265],\n",
      "        [ 1.0051],\n",
      "        [-0.9342],\n",
      "        [ 0.7175],\n",
      "        [ 1.2155],\n",
      "        [-1.4770],\n",
      "        [ 0.1739],\n",
      "        [-0.3864],\n",
      "        [-1.2494],\n",
      "        [ 1.0510],\n",
      "        [-0.3864],\n",
      "        [-0.5902],\n",
      "        [-1.4207],\n",
      "        [ 0.7175],\n",
      "        [-1.3390],\n",
      "        [ 0.5373],\n",
      "        [-0.4209],\n",
      "        [ 0.5373],\n",
      "        [ 1.2155],\n",
      "        [ 1.1826]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 167/10000,\n",
      " train_loss: 0.0054,\n",
      " train_mae: 0.0692,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2412],\n",
      "        [-1.2412],\n",
      "        [ 0.6135],\n",
      "        [ 0.5547],\n",
      "        [-0.0649],\n",
      "        [ 0.4636],\n",
      "        [ 0.8319],\n",
      "        [ 0.8820],\n",
      "        [ 0.8319],\n",
      "        [ 0.7533],\n",
      "        [-1.5327],\n",
      "        [-1.0341],\n",
      "        [ 0.5247],\n",
      "        [-0.2716],\n",
      "        [ 0.8572],\n",
      "        [-0.2716],\n",
      "        [ 0.5247],\n",
      "        [-0.7685],\n",
      "        [-0.3743],\n",
      "        [ 0.8062],\n",
      "        [ 0.6987],\n",
      "        [ 0.7262],\n",
      "        [ 1.1048],\n",
      "        [ 1.1048],\n",
      "        [ 0.9767],\n",
      "        [-0.3743],\n",
      "        [-1.2651],\n",
      "        [ 0.9767],\n",
      "        [-1.4188],\n",
      "        [ 0.8820],\n",
      "        [-1.3337],\n",
      "        [ 0.5247],\n",
      "        [-0.2716],\n",
      "        [ 0.6987],\n",
      "        [ 0.4943],\n",
      "        [ 1.1048],\n",
      "        [ 0.6987],\n",
      "        [-0.8605],\n",
      "        [-0.1683],\n",
      "        [-1.1670],\n",
      "        [-1.2170],\n",
      "        [ 0.8820],\n",
      "        [-1.0341],\n",
      "        [ 0.5843],\n",
      "        [ 1.1048],\n",
      "        [-1.5501],\n",
      "        [ 0.0378],\n",
      "        [-0.5094],\n",
      "        [-1.3337],\n",
      "        [ 0.9303],\n",
      "        [-0.5094],\n",
      "        [-0.7054],\n",
      "        [-1.4965],\n",
      "        [ 0.5843],\n",
      "        [-1.4188],\n",
      "        [ 0.4011],\n",
      "        [-0.5427],\n",
      "        [ 0.4011],\n",
      "        [ 1.1048],\n",
      "        [ 1.0697]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 168/10000,\n",
      " train_loss: 0.0064,\n",
      " train_mae: 0.0415,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2034],\n",
      "        [-1.2034],\n",
      "        [ 0.6624],\n",
      "        [ 0.6043],\n",
      "        [-0.0125],\n",
      "        [ 0.5142],\n",
      "        [ 0.8772],\n",
      "        [ 0.9263],\n",
      "        [ 0.8772],\n",
      "        [ 0.8001],\n",
      "        [-1.5009],\n",
      "        [-0.9924],\n",
      "        [ 0.5747],\n",
      "        [-0.2201],\n",
      "        [ 0.9020],\n",
      "        [-0.2201],\n",
      "        [ 0.5747],\n",
      "        [-0.7224],\n",
      "        [-0.3236],\n",
      "        [ 0.8520],\n",
      "        [ 0.7463],\n",
      "        [ 0.7734],\n",
      "        [ 1.1435],\n",
      "        [ 1.1435],\n",
      "        [ 1.0188],\n",
      "        [-0.3236],\n",
      "        [-1.2277],\n",
      "        [ 1.0188],\n",
      "        [-1.3846],\n",
      "        [ 0.9263],\n",
      "        [-1.2978],\n",
      "        [ 0.5747],\n",
      "        [-0.2201],\n",
      "        [ 0.7463],\n",
      "        [ 0.5446],\n",
      "        [ 1.1435],\n",
      "        [ 0.7463],\n",
      "        [-0.8158],\n",
      "        [-0.1162],\n",
      "        [-1.1277],\n",
      "        [-1.1787],\n",
      "        [ 0.9263],\n",
      "        [-0.9924],\n",
      "        [ 0.6336],\n",
      "        [ 1.1435],\n",
      "        [-1.5187],\n",
      "        [ 0.0904],\n",
      "        [-0.4600],\n",
      "        [-1.2978],\n",
      "        [ 0.9735],\n",
      "        [-0.4600],\n",
      "        [-0.6584],\n",
      "        [-1.4640],\n",
      "        [ 0.6336],\n",
      "        [-1.3846],\n",
      "        [ 0.4523],\n",
      "        [-0.4936],\n",
      "        [ 0.4523],\n",
      "        [ 1.1435],\n",
      "        [ 1.1094]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 169/10000,\n",
      " train_loss: 0.0028,\n",
      " train_mae: 0.0686,\n",
      " epoch_time_duration: 0.0151\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1444],\n",
      "        [-1.1444],\n",
      "        [ 0.7558],\n",
      "        [ 0.6987],\n",
      "        [ 0.0820],\n",
      "        [ 0.6098],\n",
      "        [ 0.9651],\n",
      "        [ 1.0126],\n",
      "        [ 0.9651],\n",
      "        [ 0.8903],\n",
      "        [-1.4531],\n",
      "        [-0.9251],\n",
      "        [ 0.6695],\n",
      "        [-0.1291],\n",
      "        [ 0.9891],\n",
      "        [-0.1291],\n",
      "        [ 0.6695],\n",
      "        [-0.6454],\n",
      "        [-0.2349],\n",
      "        [ 0.9407],\n",
      "        [ 0.8379],\n",
      "        [ 0.8643],\n",
      "        [ 1.2205],\n",
      "        [ 1.2205],\n",
      "        [ 1.1016],\n",
      "        [-0.2349],\n",
      "        [-1.1696],\n",
      "        [ 1.1016],\n",
      "        [-1.3325],\n",
      "        [ 1.0126],\n",
      "        [-1.2424],\n",
      "        [ 0.6695],\n",
      "        [-0.1291],\n",
      "        [ 0.8379],\n",
      "        [ 0.6398],\n",
      "        [ 1.2205],\n",
      "        [ 0.8379],\n",
      "        [-0.7421],\n",
      "        [-0.0232],\n",
      "        [-1.0657],\n",
      "        [-1.1186],\n",
      "        [ 1.0126],\n",
      "        [-0.9251],\n",
      "        [ 0.7275],\n",
      "        [ 1.2205],\n",
      "        [-1.4715],\n",
      "        [ 0.1861],\n",
      "        [-0.3748],\n",
      "        [-1.2424],\n",
      "        [ 1.0581],\n",
      "        [-0.3748],\n",
      "        [-0.5793],\n",
      "        [-1.4148],\n",
      "        [ 0.7275],\n",
      "        [-1.3325],\n",
      "        [ 0.5484],\n",
      "        [-0.4094],\n",
      "        [ 0.5484],\n",
      "        [ 1.2205],\n",
      "        [ 1.1880]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 170/10000,\n",
      " train_loss: 0.0065,\n",
      " train_mae: 0.0447,\n",
      " epoch_time_duration: 0.0089\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2227],\n",
      "        [-1.2227],\n",
      "        [ 0.6848],\n",
      "        [ 0.6260],\n",
      "        [-0.0037],\n",
      "        [ 0.5345],\n",
      "        [ 0.9015],\n",
      "        [ 0.9507],\n",
      "        [ 0.9015],\n",
      "        [ 0.8238],\n",
      "        [-1.5225],\n",
      "        [-1.0081],\n",
      "        [ 0.5959],\n",
      "        [-0.2167],\n",
      "        [ 0.9264],\n",
      "        [-0.2167],\n",
      "        [ 0.5959],\n",
      "        [-0.7321],\n",
      "        [-0.3230],\n",
      "        [ 0.8761],\n",
      "        [ 0.7696],\n",
      "        [ 0.7970],\n",
      "        [ 1.1674],\n",
      "        [ 1.1674],\n",
      "        [ 1.0433],\n",
      "        [-0.3230],\n",
      "        [-1.2473],\n",
      "        [ 1.0433],\n",
      "        [-1.4058],\n",
      "        [ 0.9507],\n",
      "        [-1.3182],\n",
      "        [ 0.5959],\n",
      "        [-0.2167],\n",
      "        [ 0.7696],\n",
      "        [ 0.5654],\n",
      "        [ 1.1674],\n",
      "        [ 0.7696],\n",
      "        [-0.8278],\n",
      "        [-0.1101],\n",
      "        [-1.1459],\n",
      "        [-1.1976],\n",
      "        [ 0.9507],\n",
      "        [-1.0081],\n",
      "        [ 0.6556],\n",
      "        [ 1.1674],\n",
      "        [-1.5403],\n",
      "        [ 0.1018],\n",
      "        [-0.4630],\n",
      "        [-1.3182],\n",
      "        [ 0.9980],\n",
      "        [-0.4630],\n",
      "        [-0.6666],\n",
      "        [-1.4855],\n",
      "        [ 0.6556],\n",
      "        [-1.4058],\n",
      "        [ 0.4714],\n",
      "        [-0.4975],\n",
      "        [ 0.4714],\n",
      "        [ 1.1674],\n",
      "        [ 1.1335]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 171/10000,\n",
      " train_loss: 0.0027,\n",
      " train_mae: 0.0597,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2402],\n",
      "        [-1.2402],\n",
      "        [ 0.6275],\n",
      "        [ 0.5689],\n",
      "        [-0.0519],\n",
      "        [ 0.4779],\n",
      "        [ 0.8447],\n",
      "        [ 0.8944],\n",
      "        [ 0.8447],\n",
      "        [ 0.7666],\n",
      "        [-1.5350],\n",
      "        [-1.0305],\n",
      "        [ 0.5389],\n",
      "        [-0.2600],\n",
      "        [ 0.8697],\n",
      "        [-0.2600],\n",
      "        [ 0.5389],\n",
      "        [-0.7618],\n",
      "        [-0.3635],\n",
      "        [ 0.8191],\n",
      "        [ 0.7123],\n",
      "        [ 0.7397],\n",
      "        [ 1.1145],\n",
      "        [ 1.1145],\n",
      "        [ 0.9881],\n",
      "        [-0.3635],\n",
      "        [-1.2643],\n",
      "        [ 0.9881],\n",
      "        [-1.4198],\n",
      "        [ 0.8944],\n",
      "        [-1.3338],\n",
      "        [ 0.5389],\n",
      "        [-0.2600],\n",
      "        [ 0.7123],\n",
      "        [ 0.5086],\n",
      "        [ 1.1145],\n",
      "        [ 0.7123],\n",
      "        [-0.8549],\n",
      "        [-0.1559],\n",
      "        [-1.1650],\n",
      "        [-1.2156],\n",
      "        [ 0.8944],\n",
      "        [-1.0305],\n",
      "        [ 0.5984],\n",
      "        [ 1.1145],\n",
      "        [-1.5526],\n",
      "        [ 0.0514],\n",
      "        [-0.4999],\n",
      "        [-1.3338],\n",
      "        [ 0.9422],\n",
      "        [-0.4999],\n",
      "        [-0.6980],\n",
      "        [-1.4984],\n",
      "        [ 0.5984],\n",
      "        [-1.4198],\n",
      "        [ 0.4155],\n",
      "        [-0.5335],\n",
      "        [ 0.4155],\n",
      "        [ 1.1145],\n",
      "        [ 1.0799]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 172/10000,\n",
      " train_loss: 0.0052,\n",
      " train_mae: 0.0510,\n",
      " epoch_time_duration: 0.0076\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1456],\n",
      "        [-1.1456],\n",
      "        [ 0.7129],\n",
      "        [ 0.6565],\n",
      "        [ 0.0523],\n",
      "        [ 0.5689],\n",
      "        [ 0.9207],\n",
      "        [ 0.9681],\n",
      "        [ 0.9207],\n",
      "        [ 0.8462],\n",
      "        [-1.4522],\n",
      "        [-0.9299],\n",
      "        [ 0.6277],\n",
      "        [-0.1534],\n",
      "        [ 0.9446],\n",
      "        [-0.1534],\n",
      "        [ 0.6277],\n",
      "        [-0.6563],\n",
      "        [-0.2564],\n",
      "        [ 0.8963],\n",
      "        [ 0.7942],\n",
      "        [ 0.8204],\n",
      "        [ 1.1769],\n",
      "        [ 1.1769],\n",
      "        [ 1.0572],\n",
      "        [-0.2564],\n",
      "        [-1.1705],\n",
      "        [ 1.0572],\n",
      "        [-1.3319],\n",
      "        [ 0.9681],\n",
      "        [-1.2425],\n",
      "        [ 0.6277],\n",
      "        [-0.1534],\n",
      "        [ 0.7942],\n",
      "        [ 0.5985],\n",
      "        [ 1.1769],\n",
      "        [ 0.7942],\n",
      "        [-0.7507],\n",
      "        [-0.0503],\n",
      "        [-1.0680],\n",
      "        [-1.1202],\n",
      "        [ 0.9681],\n",
      "        [-0.9299],\n",
      "        [ 0.6849],\n",
      "        [ 1.1769],\n",
      "        [-1.4706],\n",
      "        [ 0.1538],\n",
      "        [-0.3926],\n",
      "        [-1.2425],\n",
      "        [ 1.0135],\n",
      "        [-0.3926],\n",
      "        [-0.5919],\n",
      "        [-1.4139],\n",
      "        [ 0.6849],\n",
      "        [-1.3319],\n",
      "        [ 0.5085],\n",
      "        [-0.4263],\n",
      "        [ 0.5085],\n",
      "        [ 1.1769],\n",
      "        [ 1.1442]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 173/10000,\n",
      " train_loss: 0.0038,\n",
      " train_mae: 0.0483,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1758],\n",
      "        [-1.1758],\n",
      "        [ 0.7208],\n",
      "        [ 0.6634],\n",
      "        [ 0.0456],\n",
      "        [ 0.5741],\n",
      "        [ 0.9318],\n",
      "        [ 0.9797],\n",
      "        [ 0.9318],\n",
      "        [ 0.8563],\n",
      "        [-1.4829],\n",
      "        [-0.9578],\n",
      "        [ 0.6341],\n",
      "        [-0.1652],\n",
      "        [ 0.9560],\n",
      "        [-0.1652],\n",
      "        [ 0.6341],\n",
      "        [-0.6795],\n",
      "        [-0.2707],\n",
      "        [ 0.9071],\n",
      "        [ 0.8035],\n",
      "        [ 0.8301],\n",
      "        [ 1.1900],\n",
      "        [ 1.1900],\n",
      "        [ 1.0696],\n",
      "        [-0.2707],\n",
      "        [-1.2009],\n",
      "        [ 1.0696],\n",
      "        [-1.3629],\n",
      "        [ 0.9797],\n",
      "        [-1.2733],\n",
      "        [ 0.6341],\n",
      "        [-0.1652],\n",
      "        [ 0.8035],\n",
      "        [ 0.6043],\n",
      "        [ 1.1900],\n",
      "        [ 0.8035],\n",
      "        [-0.7757],\n",
      "        [-0.0596],\n",
      "        [-1.0975],\n",
      "        [-1.1502],\n",
      "        [ 0.9797],\n",
      "        [-0.9578],\n",
      "        [ 0.6924],\n",
      "        [ 1.1900],\n",
      "        [-1.5013],\n",
      "        [ 0.1496],\n",
      "        [-0.4101],\n",
      "        [-1.2733],\n",
      "        [ 1.0256],\n",
      "        [-0.4101],\n",
      "        [-0.6138],\n",
      "        [-1.4448],\n",
      "        [ 0.6924],\n",
      "        [-1.3629],\n",
      "        [ 0.5124],\n",
      "        [-0.4446],\n",
      "        [ 0.5124],\n",
      "        [ 1.1900],\n",
      "        [ 1.1571]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 174/10000,\n",
      " train_loss: 0.0033,\n",
      " train_mae: 0.0541,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2597],\n",
      "        [-1.2597],\n",
      "        [ 0.6501],\n",
      "        [ 0.5907],\n",
      "        [-0.0425],\n",
      "        [ 0.4985],\n",
      "        [ 0.8691],\n",
      "        [ 0.9190],\n",
      "        [ 0.8691],\n",
      "        [ 0.7906],\n",
      "        [-1.5580],\n",
      "        [-1.0460],\n",
      "        [ 0.5604],\n",
      "        [-0.2559],\n",
      "        [ 0.8943],\n",
      "        [-0.2559],\n",
      "        [ 0.5604],\n",
      "        [-0.7709],\n",
      "        [-0.3622],\n",
      "        [ 0.8434],\n",
      "        [ 0.7358],\n",
      "        [ 0.7634],\n",
      "        [ 1.1386],\n",
      "        [ 1.1386],\n",
      "        [ 1.0128],\n",
      "        [-0.3622],\n",
      "        [-1.2842],\n",
      "        [ 1.0128],\n",
      "        [-1.4418],\n",
      "        [ 0.9190],\n",
      "        [-1.3547],\n",
      "        [ 0.5604],\n",
      "        [-0.2559],\n",
      "        [ 0.7358],\n",
      "        [ 0.5296],\n",
      "        [ 1.1386],\n",
      "        [ 0.7358],\n",
      "        [-0.8663],\n",
      "        [-0.1491],\n",
      "        [-1.1832],\n",
      "        [-1.2347],\n",
      "        [ 0.9190],\n",
      "        [-1.0460],\n",
      "        [ 0.6206],\n",
      "        [ 1.1386],\n",
      "        [-1.5757],\n",
      "        [ 0.0633],\n",
      "        [-0.5022],\n",
      "        [-1.3547],\n",
      "        [ 0.9669],\n",
      "        [-0.5022],\n",
      "        [-0.7055],\n",
      "        [-1.5212],\n",
      "        [ 0.6206],\n",
      "        [-1.4418],\n",
      "        [ 0.4350],\n",
      "        [-0.5367],\n",
      "        [ 0.4350],\n",
      "        [ 1.1386],\n",
      "        [ 1.1042]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 175/10000,\n",
      " train_loss: 0.0047,\n",
      " train_mae: 0.0396,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1958],\n",
      "        [-1.1958],\n",
      "        [ 0.6781],\n",
      "        [ 0.6206],\n",
      "        [ 0.0068],\n",
      "        [ 0.5313],\n",
      "        [ 0.8901],\n",
      "        [ 0.9384],\n",
      "        [ 0.8901],\n",
      "        [ 0.8140],\n",
      "        [-1.4998],\n",
      "        [-0.9810],\n",
      "        [ 0.5913],\n",
      "        [-0.2011],\n",
      "        [ 0.9145],\n",
      "        [-0.2011],\n",
      "        [ 0.5913],\n",
      "        [-0.7073],\n",
      "        [-0.3051],\n",
      "        [ 0.8652],\n",
      "        [ 0.7610],\n",
      "        [ 0.7877],\n",
      "        [ 1.1518],\n",
      "        [ 1.1518],\n",
      "        [ 1.0294],\n",
      "        [-0.3051],\n",
      "        [-1.2206],\n",
      "        [ 1.0294],\n",
      "        [-1.3808],\n",
      "        [ 0.9384],\n",
      "        [-1.2921],\n",
      "        [ 0.5913],\n",
      "        [-0.2011],\n",
      "        [ 0.7610],\n",
      "        [ 0.5615],\n",
      "        [ 1.1518],\n",
      "        [ 0.7610],\n",
      "        [-0.8019],\n",
      "        [-0.0970],\n",
      "        [-1.1187],\n",
      "        [-1.1706],\n",
      "        [ 0.9384],\n",
      "        [-0.9810],\n",
      "        [ 0.6495],\n",
      "        [ 1.1518],\n",
      "        [-1.5180],\n",
      "        [ 0.1097],\n",
      "        [-0.4423],\n",
      "        [-1.2921],\n",
      "        [ 0.9849],\n",
      "        [-0.4423],\n",
      "        [-0.6426],\n",
      "        [-1.4620],\n",
      "        [ 0.6495],\n",
      "        [-1.3808],\n",
      "        [ 0.4699],\n",
      "        [-0.4762],\n",
      "        [ 0.4699],\n",
      "        [ 1.1518],\n",
      "        [ 1.1183]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 176/10000,\n",
      " train_loss: 0.0024,\n",
      " train_mae: 0.0565,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1375],\n",
      "        [-1.1375],\n",
      "        [ 0.7195],\n",
      "        [ 0.6636],\n",
      "        [ 0.0629],\n",
      "        [ 0.5765],\n",
      "        [ 0.9257],\n",
      "        [ 0.9726],\n",
      "        [ 0.9257],\n",
      "        [ 0.8517],\n",
      "        [-1.4479],\n",
      "        [-0.9201],\n",
      "        [ 0.6349],\n",
      "        [-0.1421],\n",
      "        [ 0.9494],\n",
      "        [-0.1421],\n",
      "        [ 0.6349],\n",
      "        [-0.6452],\n",
      "        [-0.2449],\n",
      "        [ 0.9015],\n",
      "        [ 0.8002],\n",
      "        [ 0.8262],\n",
      "        [ 1.1797],\n",
      "        [ 1.1797],\n",
      "        [ 1.0610],\n",
      "        [-0.2449],\n",
      "        [-1.1627],\n",
      "        [ 1.0610],\n",
      "        [-1.3260],\n",
      "        [ 0.9726],\n",
      "        [-1.2354],\n",
      "        [ 0.6349],\n",
      "        [-0.1421],\n",
      "        [ 0.8002],\n",
      "        [ 0.6059],\n",
      "        [ 1.1797],\n",
      "        [ 0.8002],\n",
      "        [-0.7400],\n",
      "        [-0.0393],\n",
      "        [-1.0593],\n",
      "        [-1.1119],\n",
      "        [ 0.9726],\n",
      "        [-0.9201],\n",
      "        [ 0.6917],\n",
      "        [ 1.1797],\n",
      "        [-1.4667],\n",
      "        [ 0.1640],\n",
      "        [-0.3811],\n",
      "        [-1.2354],\n",
      "        [ 1.0177],\n",
      "        [-0.3811],\n",
      "        [-0.5806],\n",
      "        [-1.4091],\n",
      "        [ 0.6917],\n",
      "        [-1.3260],\n",
      "        [ 0.5166],\n",
      "        [-0.4148],\n",
      "        [ 0.5166],\n",
      "        [ 1.1797],\n",
      "        [ 1.1472]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 177/10000,\n",
      " train_loss: 0.0044,\n",
      " train_mae: 0.0437,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2249],\n",
      "        [-1.2249],\n",
      "        [ 0.6726],\n",
      "        [ 0.6143],\n",
      "        [-0.0091],\n",
      "        [ 0.5237],\n",
      "        [ 0.8875],\n",
      "        [ 0.9364],\n",
      "        [ 0.8875],\n",
      "        [ 0.8104],\n",
      "        [-1.5287],\n",
      "        [-1.0090],\n",
      "        [ 0.5845],\n",
      "        [-0.2202],\n",
      "        [ 0.9122],\n",
      "        [-0.2202],\n",
      "        [ 0.5845],\n",
      "        [-0.7329],\n",
      "        [-0.3257],\n",
      "        [ 0.8623],\n",
      "        [ 0.7567],\n",
      "        [ 0.7838],\n",
      "        [ 1.1521],\n",
      "        [ 1.1521],\n",
      "        [ 1.0285],\n",
      "        [-0.3257],\n",
      "        [-1.2497],\n",
      "        [ 1.0285],\n",
      "        [-1.4100],\n",
      "        [ 0.9364],\n",
      "        [-1.3213],\n",
      "        [ 0.5845],\n",
      "        [-0.2202],\n",
      "        [ 0.7567],\n",
      "        [ 0.5543],\n",
      "        [ 1.1521],\n",
      "        [ 0.7567],\n",
      "        [-0.8284],\n",
      "        [-0.1145],\n",
      "        [-1.1474],\n",
      "        [-1.1995],\n",
      "        [ 0.9364],\n",
      "        [-1.0090],\n",
      "        [ 0.6437],\n",
      "        [ 1.1521],\n",
      "        [-1.5468],\n",
      "        [ 0.0954],\n",
      "        [-0.4648],\n",
      "        [-1.3213],\n",
      "        [ 0.9834],\n",
      "        [-0.4648],\n",
      "        [-0.6675],\n",
      "        [-1.4910],\n",
      "        [ 0.6437],\n",
      "        [-1.4100],\n",
      "        [ 0.4613],\n",
      "        [-0.4991],\n",
      "        [ 0.4613],\n",
      "        [ 1.1521],\n",
      "        [ 1.1183]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 178/10000,\n",
      " train_loss: 0.0028,\n",
      " train_mae: 0.0475,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2421],\n",
      "        [-1.2421],\n",
      "        [ 0.6650],\n",
      "        [ 0.6062],\n",
      "        [-0.0219],\n",
      "        [ 0.5149],\n",
      "        [ 0.8817],\n",
      "        [ 0.9310],\n",
      "        [ 0.8817],\n",
      "        [ 0.8040],\n",
      "        [-1.5451],\n",
      "        [-1.0261],\n",
      "        [ 0.5762],\n",
      "        [-0.2344],\n",
      "        [ 0.9066],\n",
      "        [-0.2344],\n",
      "        [ 0.5762],\n",
      "        [-0.7494],\n",
      "        [-0.3404],\n",
      "        [ 0.8563],\n",
      "        [ 0.7498],\n",
      "        [ 0.7771],\n",
      "        [ 1.1483],\n",
      "        [ 1.1483],\n",
      "        [ 1.0238],\n",
      "        [-0.3404],\n",
      "        [-1.2669],\n",
      "        [ 1.0238],\n",
      "        [-1.4269],\n",
      "        [ 0.9310],\n",
      "        [-1.3384],\n",
      "        [ 0.5762],\n",
      "        [-0.2344],\n",
      "        [ 0.7498],\n",
      "        [ 0.5457],\n",
      "        [ 1.1483],\n",
      "        [ 0.7498],\n",
      "        [-0.8452],\n",
      "        [-0.1280],\n",
      "        [-1.1647],\n",
      "        [-1.2167],\n",
      "        [ 0.9310],\n",
      "        [-1.0261],\n",
      "        [ 0.6358],\n",
      "        [ 1.1483],\n",
      "        [-1.5632],\n",
      "        [ 0.0833],\n",
      "        [-0.4802],\n",
      "        [-1.3384],\n",
      "        [ 0.9784],\n",
      "        [-0.4802],\n",
      "        [-0.6838],\n",
      "        [-1.5076],\n",
      "        [ 0.6358],\n",
      "        [-1.4269],\n",
      "        [ 0.4520],\n",
      "        [-0.5148],\n",
      "        [ 0.4520],\n",
      "        [ 1.1483],\n",
      "        [ 1.1143]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 179/10000,\n",
      " train_loss: 0.0034,\n",
      " train_mae: 0.0485,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1576],\n",
      "        [-1.1576],\n",
      "        [ 0.7137],\n",
      "        [ 0.6572],\n",
      "        [ 0.0509],\n",
      "        [ 0.5693],\n",
      "        [ 0.9218],\n",
      "        [ 0.9691],\n",
      "        [ 0.9218],\n",
      "        [ 0.8471],\n",
      "        [-1.4687],\n",
      "        [-0.9393],\n",
      "        [ 0.6283],\n",
      "        [-0.1559],\n",
      "        [ 0.9457],\n",
      "        [-0.1559],\n",
      "        [ 0.6283],\n",
      "        [-0.6628],\n",
      "        [-0.2596],\n",
      "        [ 0.8974],\n",
      "        [ 0.7951],\n",
      "        [ 0.8214],\n",
      "        [ 1.1781],\n",
      "        [ 1.1781],\n",
      "        [ 1.0583],\n",
      "        [-0.2596],\n",
      "        [-1.1828],\n",
      "        [ 1.0583],\n",
      "        [-1.3465],\n",
      "        [ 0.9691],\n",
      "        [-1.2558],\n",
      "        [ 0.6283],\n",
      "        [-0.1559],\n",
      "        [ 0.7951],\n",
      "        [ 0.5990],\n",
      "        [ 1.1781],\n",
      "        [ 0.7951],\n",
      "        [-0.7581],\n",
      "        [-0.0522],\n",
      "        [-1.0790],\n",
      "        [-1.1318],\n",
      "        [ 0.9691],\n",
      "        [-0.9393],\n",
      "        [ 0.6856],\n",
      "        [ 1.1781],\n",
      "        [-1.4874],\n",
      "        [ 0.1529],\n",
      "        [-0.3968],\n",
      "        [-1.2558],\n",
      "        [ 1.0147],\n",
      "        [-0.3968],\n",
      "        [-0.5977],\n",
      "        [-1.4298],\n",
      "        [ 0.6856],\n",
      "        [-1.3465],\n",
      "        [ 0.5088],\n",
      "        [-0.4308],\n",
      "        [ 0.5088],\n",
      "        [ 1.1781],\n",
      "        [ 1.1453]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 180/10000,\n",
      " train_loss: 0.0035,\n",
      " train_mae: 0.0390,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1782],\n",
      "        [-1.1782],\n",
      "        [ 0.6886],\n",
      "        [ 0.6318],\n",
      "        [ 0.0239],\n",
      "        [ 0.5434],\n",
      "        [ 0.8986],\n",
      "        [ 0.9465],\n",
      "        [ 0.8986],\n",
      "        [ 0.8233],\n",
      "        [-1.4867],\n",
      "        [-0.9616],\n",
      "        [ 0.6027],\n",
      "        [-0.1826],\n",
      "        [ 0.9228],\n",
      "        [-0.1826],\n",
      "        [ 0.6027],\n",
      "        [-0.6871],\n",
      "        [-0.2859],\n",
      "        [ 0.8740],\n",
      "        [ 0.7707],\n",
      "        [ 0.7972],\n",
      "        [ 1.1583],\n",
      "        [ 1.1583],\n",
      "        [ 1.0368],\n",
      "        [-0.2859],\n",
      "        [-1.2032],\n",
      "        [ 1.0368],\n",
      "        [-1.3656],\n",
      "        [ 0.9465],\n",
      "        [-1.2756],\n",
      "        [ 0.6027],\n",
      "        [-0.1826],\n",
      "        [ 0.7707],\n",
      "        [ 0.5733],\n",
      "        [ 1.1583],\n",
      "        [ 0.7707],\n",
      "        [-0.7818],\n",
      "        [-0.0791],\n",
      "        [-1.1003],\n",
      "        [-1.1526],\n",
      "        [ 0.9465],\n",
      "        [-0.9616],\n",
      "        [ 0.6604],\n",
      "        [ 1.1583],\n",
      "        [-1.5053],\n",
      "        [ 0.1258],\n",
      "        [-0.4225],\n",
      "        [-1.2756],\n",
      "        [ 0.9926],\n",
      "        [-0.4225],\n",
      "        [-0.6224],\n",
      "        [-1.4482],\n",
      "        [ 0.6604],\n",
      "        [-1.3656],\n",
      "        [ 0.4826],\n",
      "        [-0.4564],\n",
      "        [ 0.4826],\n",
      "        [ 1.1583],\n",
      "        [ 1.1250]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 181/10000,\n",
      " train_loss: 0.0025,\n",
      " train_mae: 0.0480,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2403],\n",
      "        [-1.2403],\n",
      "        [ 0.6548],\n",
      "        [ 0.5962],\n",
      "        [-0.0279],\n",
      "        [ 0.5053],\n",
      "        [ 0.8712],\n",
      "        [ 0.9206],\n",
      "        [ 0.8712],\n",
      "        [ 0.7935],\n",
      "        [-1.5442],\n",
      "        [-1.0249],\n",
      "        [ 0.5663],\n",
      "        [-0.2386],\n",
      "        [ 0.8962],\n",
      "        [-0.2386],\n",
      "        [ 0.5663],\n",
      "        [-0.7496],\n",
      "        [-0.3437],\n",
      "        [ 0.8458],\n",
      "        [ 0.7394],\n",
      "        [ 0.7667],\n",
      "        [ 1.1388],\n",
      "        [ 1.1388],\n",
      "        [ 1.0136],\n",
      "        [-0.3437],\n",
      "        [-1.2651],\n",
      "        [ 1.0136],\n",
      "        [-1.4254],\n",
      "        [ 0.9206],\n",
      "        [-1.3367],\n",
      "        [ 0.5663],\n",
      "        [-0.2386],\n",
      "        [ 0.7394],\n",
      "        [ 0.5360],\n",
      "        [ 1.1388],\n",
      "        [ 0.7394],\n",
      "        [-0.8448],\n",
      "        [-0.1331],\n",
      "        [-1.1630],\n",
      "        [-1.2150],\n",
      "        [ 0.9206],\n",
      "        [-1.0249],\n",
      "        [ 0.6258],\n",
      "        [ 1.1388],\n",
      "        [-1.5624],\n",
      "        [ 0.0765],\n",
      "        [-0.4824],\n",
      "        [-1.3367],\n",
      "        [ 0.9681],\n",
      "        [-0.4824],\n",
      "        [-0.6845],\n",
      "        [-1.5065],\n",
      "        [ 0.6258],\n",
      "        [-1.4254],\n",
      "        [ 0.4427],\n",
      "        [-0.5167],\n",
      "        [ 0.4427],\n",
      "        [ 1.1388],\n",
      "        [ 1.1045]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 182/10000,\n",
      " train_loss: 0.0036,\n",
      " train_mae: 0.0425,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1949],\n",
      "        [-1.1949],\n",
      "        [ 0.7025],\n",
      "        [ 0.6448],\n",
      "        [ 0.0268],\n",
      "        [ 0.5552],\n",
      "        [ 0.9147],\n",
      "        [ 0.9630],\n",
      "        [ 0.9147],\n",
      "        [ 0.8386],\n",
      "        [-1.5053],\n",
      "        [-0.9758],\n",
      "        [ 0.6154],\n",
      "        [-0.1835],\n",
      "        [ 0.9391],\n",
      "        [-0.1835],\n",
      "        [ 0.6154],\n",
      "        [-0.6971],\n",
      "        [-0.2888],\n",
      "        [ 0.8898],\n",
      "        [ 0.7855],\n",
      "        [ 0.8123],\n",
      "        [ 1.1759],\n",
      "        [ 1.1759],\n",
      "        [ 1.0539],\n",
      "        [-0.2888],\n",
      "        [-1.2201],\n",
      "        [ 1.0539],\n",
      "        [-1.3837],\n",
      "        [ 0.9630],\n",
      "        [-1.2931],\n",
      "        [ 0.6154],\n",
      "        [-0.1835],\n",
      "        [ 0.7855],\n",
      "        [ 0.5855],\n",
      "        [ 1.1759],\n",
      "        [ 0.7855],\n",
      "        [-0.7933],\n",
      "        [-0.0781],\n",
      "        [-1.1161],\n",
      "        [-1.1691],\n",
      "        [ 0.9630],\n",
      "        [-0.9758],\n",
      "        [ 0.6739],\n",
      "        [ 1.1759],\n",
      "        [-1.5239],\n",
      "        [ 0.1307],\n",
      "        [-0.4279],\n",
      "        [-1.2931],\n",
      "        [ 1.0094],\n",
      "        [-0.4279],\n",
      "        [-0.6313],\n",
      "        [-1.4666],\n",
      "        [ 0.6739],\n",
      "        [-1.3837],\n",
      "        [ 0.4935],\n",
      "        [-0.4624],\n",
      "        [ 0.4935],\n",
      "        [ 1.1759],\n",
      "        [ 1.1425]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 183/10000,\n",
      " train_loss: 0.0025,\n",
      " train_mae: 0.0443,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1669],\n",
      "        [-1.1669],\n",
      "        [ 0.7072],\n",
      "        [ 0.6505],\n",
      "        [ 0.0425],\n",
      "        [ 0.5622],\n",
      "        [ 0.9166],\n",
      "        [ 0.9644],\n",
      "        [ 0.9166],\n",
      "        [ 0.8415],\n",
      "        [-1.4790],\n",
      "        [-0.9482],\n",
      "        [ 0.6215],\n",
      "        [-0.1645],\n",
      "        [ 0.9407],\n",
      "        [-0.1645],\n",
      "        [ 0.6215],\n",
      "        [-0.6715],\n",
      "        [-0.2682],\n",
      "        [ 0.8921],\n",
      "        [ 0.7891],\n",
      "        [ 0.8156],\n",
      "        [ 1.1753],\n",
      "        [ 1.1753],\n",
      "        [ 1.0543],\n",
      "        [-0.2682],\n",
      "        [-1.1922],\n",
      "        [ 1.0543],\n",
      "        [-1.3564],\n",
      "        [ 0.9644],\n",
      "        [-1.2654],\n",
      "        [ 0.6215],\n",
      "        [-0.1645],\n",
      "        [ 0.7891],\n",
      "        [ 0.5920],\n",
      "        [ 1.1753],\n",
      "        [ 0.7891],\n",
      "        [-0.7669],\n",
      "        [-0.0607],\n",
      "        [-1.0882],\n",
      "        [-1.1411],\n",
      "        [ 0.9644],\n",
      "        [-0.9482],\n",
      "        [ 0.6791],\n",
      "        [ 1.1753],\n",
      "        [-1.4979],\n",
      "        [ 0.1447],\n",
      "        [-0.4054],\n",
      "        [-1.2654],\n",
      "        [ 1.0103],\n",
      "        [-0.4054],\n",
      "        [-0.6064],\n",
      "        [-1.4400],\n",
      "        [ 0.6791],\n",
      "        [-1.3564],\n",
      "        [ 0.5015],\n",
      "        [-0.4394],\n",
      "        [ 0.5015],\n",
      "        [ 1.1753],\n",
      "        [ 1.1422]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 184/10000,\n",
      " train_loss: 0.0030,\n",
      " train_mae: 0.0432,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2182],\n",
      "        [-1.2182],\n",
      "        [ 0.6559],\n",
      "        [ 0.5980],\n",
      "        [-0.0171],\n",
      "        [ 0.5082],\n",
      "        [ 0.8702],\n",
      "        [ 0.9192],\n",
      "        [ 0.8702],\n",
      "        [ 0.7931],\n",
      "        [-1.5243],\n",
      "        [-1.0030],\n",
      "        [ 0.5684],\n",
      "        [-0.2247],\n",
      "        [ 0.8949],\n",
      "        [-0.2247],\n",
      "        [ 0.5684],\n",
      "        [-0.7295],\n",
      "        [-0.3284],\n",
      "        [ 0.8449],\n",
      "        [ 0.7396],\n",
      "        [ 0.7666],\n",
      "        [ 1.1365],\n",
      "        [ 1.1365],\n",
      "        [ 1.0117],\n",
      "        [-0.3284],\n",
      "        [-1.2431],\n",
      "        [ 1.0117],\n",
      "        [-1.4042],\n",
      "        [ 0.9192],\n",
      "        [-1.3149],\n",
      "        [ 0.5684],\n",
      "        [-0.2247],\n",
      "        [ 0.7396],\n",
      "        [ 0.5385],\n",
      "        [ 1.1365],\n",
      "        [ 0.7396],\n",
      "        [-0.8239],\n",
      "        [-0.1208],\n",
      "        [-1.1408],\n",
      "        [-1.1929],\n",
      "        [ 0.9192],\n",
      "        [-1.0030],\n",
      "        [ 0.6271],\n",
      "        [ 1.1365],\n",
      "        [-1.5428],\n",
      "        [ 0.0856],\n",
      "        [-0.4652],\n",
      "        [-1.3149],\n",
      "        [ 0.9664],\n",
      "        [-0.4652],\n",
      "        [-0.6650],\n",
      "        [-1.4861],\n",
      "        [ 0.6271],\n",
      "        [-1.4042],\n",
      "        [ 0.4464],\n",
      "        [-0.4990],\n",
      "        [ 0.4464],\n",
      "        [ 1.1365],\n",
      "        [ 1.1023]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 185/10000,\n",
      " train_loss: 0.0030,\n",
      " train_mae: 0.0403,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2092],\n",
      "        [-1.2092],\n",
      "        [ 0.6808],\n",
      "        [ 0.6228],\n",
      "        [ 0.0048],\n",
      "        [ 0.5329],\n",
      "        [ 0.8947],\n",
      "        [ 0.9435],\n",
      "        [ 0.8947],\n",
      "        [ 0.8179],\n",
      "        [-1.5185],\n",
      "        [-0.9915],\n",
      "        [ 0.5932],\n",
      "        [-0.2046],\n",
      "        [ 0.9194],\n",
      "        [-0.2046],\n",
      "        [ 0.5932],\n",
      "        [-0.7148],\n",
      "        [-0.3092],\n",
      "        [ 0.8696],\n",
      "        [ 0.7644],\n",
      "        [ 0.7914],\n",
      "        [ 1.1595],\n",
      "        [ 1.1595],\n",
      "        [ 1.0356],\n",
      "        [-0.3092],\n",
      "        [-1.2344],\n",
      "        [ 1.0356],\n",
      "        [-1.3972],\n",
      "        [ 0.9435],\n",
      "        [-1.3070],\n",
      "        [ 0.5932],\n",
      "        [-0.2046],\n",
      "        [ 0.7644],\n",
      "        [ 0.5633],\n",
      "        [ 1.1595],\n",
      "        [ 0.7644],\n",
      "        [-0.8103],\n",
      "        [-0.0997],\n",
      "        [-1.1309],\n",
      "        [-1.1836],\n",
      "        [ 0.9435],\n",
      "        [-0.9915],\n",
      "        [ 0.6520],\n",
      "        [ 1.1595],\n",
      "        [-1.5371],\n",
      "        [ 0.1083],\n",
      "        [-0.4475],\n",
      "        [-1.3070],\n",
      "        [ 0.9905],\n",
      "        [-0.4475],\n",
      "        [-0.6495],\n",
      "        [-1.4799],\n",
      "        [ 0.6520],\n",
      "        [-1.3972],\n",
      "        [ 0.4710],\n",
      "        [-0.4817],\n",
      "        [ 0.4710],\n",
      "        [ 1.1595],\n",
      "        [ 1.1256]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 186/10000,\n",
      " train_loss: 0.0023,\n",
      " train_mae: 0.0457,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1744],\n",
      "        [-1.1744],\n",
      "        [ 0.7151],\n",
      "        [ 0.6579],\n",
      "        [ 0.0449],\n",
      "        [ 0.5690],\n",
      "        [ 0.9260],\n",
      "        [ 0.9741],\n",
      "        [ 0.9260],\n",
      "        [ 0.8504],\n",
      "        [-1.4885],\n",
      "        [-0.9541],\n",
      "        [ 0.6287],\n",
      "        [-0.1638],\n",
      "        [ 0.9503],\n",
      "        [-0.1638],\n",
      "        [ 0.6287],\n",
      "        [-0.6752],\n",
      "        [-0.2685],\n",
      "        [ 0.9013],\n",
      "        [ 0.7976],\n",
      "        [ 0.8242],\n",
      "        [ 1.1863],\n",
      "        [ 1.1863],\n",
      "        [ 1.0646],\n",
      "        [-0.2685],\n",
      "        [-1.1999],\n",
      "        [ 1.0646],\n",
      "        [-1.3652],\n",
      "        [ 0.9741],\n",
      "        [-1.2736],\n",
      "        [ 0.6287],\n",
      "        [-0.1638],\n",
      "        [ 0.7976],\n",
      "        [ 0.5990],\n",
      "        [ 1.1863],\n",
      "        [ 0.7976],\n",
      "        [-0.7714],\n",
      "        [-0.0592],\n",
      "        [-1.0951],\n",
      "        [-1.1484],\n",
      "        [ 0.9741],\n",
      "        [-0.9541],\n",
      "        [ 0.6867],\n",
      "        [ 1.1863],\n",
      "        [-1.5075],\n",
      "        [ 0.1479],\n",
      "        [-0.4069],\n",
      "        [-1.2736],\n",
      "        [ 1.0203],\n",
      "        [-0.4069],\n",
      "        [-0.6096],\n",
      "        [-1.4493],\n",
      "        [ 0.6867],\n",
      "        [-1.3652],\n",
      "        [ 0.5077],\n",
      "        [-0.4412],\n",
      "        [ 0.5077],\n",
      "        [ 1.1863],\n",
      "        [ 1.1530]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 187/10000,\n",
      " train_loss: 0.0030,\n",
      " train_mae: 0.0394,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2067e+00],\n",
      "        [-1.2067e+00],\n",
      "        [ 6.7275e-01],\n",
      "        [ 6.1495e-01],\n",
      "        [ 1.4903e-05],\n",
      "        [ 5.2525e-01],\n",
      "        [ 8.8665e-01],\n",
      "        [ 9.3558e-01],\n",
      "        [ 8.8665e-01],\n",
      "        [ 8.0979e-01],\n",
      "        [-1.5162e+00],\n",
      "        [-9.8970e-01],\n",
      "        [ 5.8544e-01],\n",
      "        [-2.0785e-01],\n",
      "        [ 9.1135e-01],\n",
      "        [-2.0785e-01],\n",
      "        [ 5.8544e-01],\n",
      "        [-7.1449e-01],\n",
      "        [-3.1177e-01],\n",
      "        [ 8.6149e-01],\n",
      "        [ 7.5629e-01],\n",
      "        [ 7.8326e-01],\n",
      "        [ 1.1525e+00],\n",
      "        [ 1.1525e+00],\n",
      "        [ 1.0279e+00],\n",
      "        [-3.1177e-01],\n",
      "        [-1.2318e+00],\n",
      "        [ 1.0279e+00],\n",
      "        [-1.3946e+00],\n",
      "        [ 9.3558e-01],\n",
      "        [-1.3044e+00],\n",
      "        [ 5.8544e-01],\n",
      "        [-2.0785e-01],\n",
      "        [ 7.5629e-01],\n",
      "        [ 5.5553e-01],\n",
      "        [ 1.1525e+00],\n",
      "        [ 7.5629e-01],\n",
      "        [-8.0946e-01],\n",
      "        [-1.0373e-01],\n",
      "        [-1.1287e+00],\n",
      "        [-1.1812e+00],\n",
      "        [ 9.3558e-01],\n",
      "        [-9.8970e-01],\n",
      "        [ 6.4406e-01],\n",
      "        [ 1.1525e+00],\n",
      "        [-1.5349e+00],\n",
      "        [ 1.0284e-01],\n",
      "        [-4.4904e-01],\n",
      "        [-1.3044e+00],\n",
      "        [ 9.8266e-01],\n",
      "        [-4.4904e-01],\n",
      "        [-6.4963e-01],\n",
      "        [-1.4775e+00],\n",
      "        [ 6.4406e-01],\n",
      "        [-1.3946e+00],\n",
      "        [ 4.6357e-01],\n",
      "        [-4.8300e-01],\n",
      "        [ 4.6357e-01],\n",
      "        [ 1.1525e+00],\n",
      "        [ 1.1183e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 188/10000,\n",
      " train_loss: 0.0023,\n",
      " train_mae: 0.0406,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2113],\n",
      "        [-1.2113],\n",
      "        [ 0.6637],\n",
      "        [ 0.6058],\n",
      "        [-0.0086],\n",
      "        [ 0.5160],\n",
      "        [ 0.8781],\n",
      "        [ 0.9272],\n",
      "        [ 0.8781],\n",
      "        [ 0.8010],\n",
      "        [-1.5202],\n",
      "        [-0.9949],\n",
      "        [ 0.5763],\n",
      "        [-0.2159],\n",
      "        [ 0.9029],\n",
      "        [-0.2159],\n",
      "        [ 0.5763],\n",
      "        [-0.7207],\n",
      "        [-0.3194],\n",
      "        [ 0.8529],\n",
      "        [ 0.7474],\n",
      "        [ 0.7744],\n",
      "        [ 1.1452],\n",
      "        [ 1.1452],\n",
      "        [ 1.0199],\n",
      "        [-0.3194],\n",
      "        [-1.2363],\n",
      "        [ 1.0199],\n",
      "        [-1.3988],\n",
      "        [ 0.9272],\n",
      "        [-1.3087],\n",
      "        [ 0.5763],\n",
      "        [-0.2159],\n",
      "        [ 0.7474],\n",
      "        [ 0.5463],\n",
      "        [ 1.1452],\n",
      "        [ 0.7474],\n",
      "        [-0.8153],\n",
      "        [-0.1120],\n",
      "        [-1.1334],\n",
      "        [-1.1858],\n",
      "        [ 0.9272],\n",
      "        [-0.9949],\n",
      "        [ 0.6350],\n",
      "        [ 1.1452],\n",
      "        [-1.5388],\n",
      "        [ 0.0940],\n",
      "        [-0.4562],\n",
      "        [-1.3087],\n",
      "        [ 0.9745],\n",
      "        [-0.4562],\n",
      "        [-0.6561],\n",
      "        [-1.4815],\n",
      "        [ 0.6350],\n",
      "        [-1.3988],\n",
      "        [ 0.4544],\n",
      "        [-0.4901],\n",
      "        [ 0.4544],\n",
      "        [ 1.1452],\n",
      "        [ 1.1109]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 189/10000,\n",
      " train_loss: 0.0025,\n",
      " train_mae: 0.0429,\n",
      " epoch_time_duration: 0.0154\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1768],\n",
      "        [-1.1768],\n",
      "        [ 0.7072],\n",
      "        [ 0.6499],\n",
      "        [ 0.0379],\n",
      "        [ 0.5609],\n",
      "        [ 0.9189],\n",
      "        [ 0.9673],\n",
      "        [ 0.9189],\n",
      "        [ 0.8429],\n",
      "        [-1.4915],\n",
      "        [-0.9569],\n",
      "        [ 0.6206],\n",
      "        [-0.1700],\n",
      "        [ 0.9433],\n",
      "        [-0.1700],\n",
      "        [ 0.6206],\n",
      "        [-0.6790],\n",
      "        [-0.2741],\n",
      "        [ 0.8940],\n",
      "        [ 0.7900],\n",
      "        [ 0.8167],\n",
      "        [ 1.1813],\n",
      "        [ 1.1813],\n",
      "        [ 1.0584],\n",
      "        [-0.2741],\n",
      "        [-1.2023],\n",
      "        [ 1.0584],\n",
      "        [-1.3677],\n",
      "        [ 0.9673],\n",
      "        [-1.2760],\n",
      "        [ 0.6206],\n",
      "        [-0.1700],\n",
      "        [ 0.7900],\n",
      "        [ 0.5910],\n",
      "        [ 1.1813],\n",
      "        [ 0.7900],\n",
      "        [-0.7748],\n",
      "        [-0.0658],\n",
      "        [-1.0977],\n",
      "        [-1.1509],\n",
      "        [ 0.9673],\n",
      "        [-0.9569],\n",
      "        [ 0.6788],\n",
      "        [ 1.1813],\n",
      "        [-1.5105],\n",
      "        [ 0.1406],\n",
      "        [-0.4119],\n",
      "        [-1.2760],\n",
      "        [ 1.0138],\n",
      "        [-0.4119],\n",
      "        [-0.6137],\n",
      "        [-1.4521],\n",
      "        [ 0.6788],\n",
      "        [-1.3677],\n",
      "        [ 0.4997],\n",
      "        [-0.4460],\n",
      "        [ 0.4997],\n",
      "        [ 1.1813],\n",
      "        [ 1.1476]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 190/10000,\n",
      " train_loss: 0.0027,\n",
      " train_mae: 0.0395,\n",
      " epoch_time_duration: 0.0080\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2016],\n",
      "        [-1.2016],\n",
      "        [ 0.6888],\n",
      "        [ 0.6309],\n",
      "        [ 0.0137],\n",
      "        [ 0.5409],\n",
      "        [ 0.9031],\n",
      "        [ 0.9522],\n",
      "        [ 0.9031],\n",
      "        [ 0.8261],\n",
      "        [-1.5141],\n",
      "        [-0.9826],\n",
      "        [ 0.6013],\n",
      "        [-0.1952],\n",
      "        [ 0.9279],\n",
      "        [-0.1952],\n",
      "        [ 0.6013],\n",
      "        [-0.7052],\n",
      "        [-0.2997],\n",
      "        [ 0.8779],\n",
      "        [ 0.7725],\n",
      "        [ 0.7996],\n",
      "        [ 1.1693],\n",
      "        [ 1.1693],\n",
      "        [ 1.0446],\n",
      "        [-0.2997],\n",
      "        [-1.2269],\n",
      "        [ 1.0446],\n",
      "        [-1.3913],\n",
      "        [ 0.9522],\n",
      "        [-1.3002],\n",
      "        [ 0.6013],\n",
      "        [-0.1952],\n",
      "        [ 0.7725],\n",
      "        [ 0.5713],\n",
      "        [ 1.1693],\n",
      "        [ 0.7725],\n",
      "        [-0.8009],\n",
      "        [-0.0906],\n",
      "        [-1.1228],\n",
      "        [-1.1758],\n",
      "        [ 0.9522],\n",
      "        [-0.9826],\n",
      "        [ 0.6601],\n",
      "        [ 1.1693],\n",
      "        [-1.5330],\n",
      "        [ 0.1170],\n",
      "        [-0.4379],\n",
      "        [-1.3002],\n",
      "        [ 0.9993],\n",
      "        [-0.4379],\n",
      "        [-0.6398],\n",
      "        [-1.4750],\n",
      "        [ 0.6601],\n",
      "        [-1.3913],\n",
      "        [ 0.4791],\n",
      "        [-0.4720],\n",
      "        [ 0.4791],\n",
      "        [ 1.1693],\n",
      "        [ 1.1351]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 191/10000,\n",
      " train_loss: 0.0022,\n",
      " train_mae: 0.0410,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2162],\n",
      "        [-1.2162],\n",
      "        [ 0.6616],\n",
      "        [ 0.6035],\n",
      "        [-0.0124],\n",
      "        [ 0.5134],\n",
      "        [ 0.8772],\n",
      "        [ 0.9267],\n",
      "        [ 0.8772],\n",
      "        [ 0.7996],\n",
      "        [-1.5261],\n",
      "        [-0.9994],\n",
      "        [ 0.5738],\n",
      "        [-0.2199],\n",
      "        [ 0.9022],\n",
      "        [-0.2199],\n",
      "        [ 0.5738],\n",
      "        [-0.7250],\n",
      "        [-0.3235],\n",
      "        [ 0.8518],\n",
      "        [ 0.7457],\n",
      "        [ 0.7729],\n",
      "        [ 1.1465],\n",
      "        [ 1.1465],\n",
      "        [ 1.0201],\n",
      "        [-0.3235],\n",
      "        [-1.2413],\n",
      "        [ 1.0201],\n",
      "        [-1.4042],\n",
      "        [ 0.9267],\n",
      "        [-1.3138],\n",
      "        [ 0.5738],\n",
      "        [-0.2199],\n",
      "        [ 0.7457],\n",
      "        [ 0.5438],\n",
      "        [ 1.1465],\n",
      "        [ 0.7457],\n",
      "        [-0.8197],\n",
      "        [-0.1160],\n",
      "        [-1.1382],\n",
      "        [-1.1906],\n",
      "        [ 0.9267],\n",
      "        [-0.9994],\n",
      "        [ 0.6327],\n",
      "        [ 1.1465],\n",
      "        [-1.5448],\n",
      "        [ 0.0903],\n",
      "        [-0.4604],\n",
      "        [-1.3138],\n",
      "        [ 0.9743],\n",
      "        [-0.4604],\n",
      "        [-0.6603],\n",
      "        [-1.4872],\n",
      "        [ 0.6327],\n",
      "        [-1.4042],\n",
      "        [ 0.4515],\n",
      "        [-0.4942],\n",
      "        [ 0.4515],\n",
      "        [ 1.1465],\n",
      "        [ 1.1118]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 192/10000,\n",
      " train_loss: 0.0026,\n",
      " train_mae: 0.0390,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1777],\n",
      "        [-1.1777],\n",
      "        [ 0.6948],\n",
      "        [ 0.6375],\n",
      "        [ 0.0282],\n",
      "        [ 0.5486],\n",
      "        [ 0.9070],\n",
      "        [ 0.9556],\n",
      "        [ 0.9070],\n",
      "        [ 0.8307],\n",
      "        [-1.4924],\n",
      "        [-0.9588],\n",
      "        [ 0.6082],\n",
      "        [-0.1781],\n",
      "        [ 0.9315],\n",
      "        [-0.1781],\n",
      "        [ 0.6082],\n",
      "        [-0.6828],\n",
      "        [-0.2813],\n",
      "        [ 0.8820],\n",
      "        [ 0.7776],\n",
      "        [ 0.8044],\n",
      "        [ 1.1716],\n",
      "        [ 1.1716],\n",
      "        [ 1.0474],\n",
      "        [-0.2813],\n",
      "        [-1.2031],\n",
      "        [ 1.0474],\n",
      "        [-1.3684],\n",
      "        [ 0.9556],\n",
      "        [-1.2767],\n",
      "        [ 0.6082],\n",
      "        [-0.1781],\n",
      "        [ 0.7776],\n",
      "        [ 0.5786],\n",
      "        [ 1.1716],\n",
      "        [ 0.7776],\n",
      "        [-0.7779],\n",
      "        [-0.0747],\n",
      "        [-1.0988],\n",
      "        [-1.1519],\n",
      "        [ 0.9556],\n",
      "        [-0.9588],\n",
      "        [ 0.6663],\n",
      "        [ 1.1716],\n",
      "        [-1.5115],\n",
      "        [ 0.1301],\n",
      "        [-0.4179],\n",
      "        [-1.2767],\n",
      "        [ 1.0024],\n",
      "        [-0.4179],\n",
      "        [-0.6180],\n",
      "        [-1.4528],\n",
      "        [ 0.6663],\n",
      "        [-1.3684],\n",
      "        [ 0.4875],\n",
      "        [-0.4518],\n",
      "        [ 0.4875],\n",
      "        [ 1.1716],\n",
      "        [ 1.1375]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 193/10000,\n",
      " train_loss: 0.0023,\n",
      " train_mae: 0.0391,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1927],\n",
      "        [-1.1927],\n",
      "        [ 0.6940],\n",
      "        [ 0.6362],\n",
      "        [ 0.0211],\n",
      "        [ 0.5464],\n",
      "        [ 0.9082],\n",
      "        [ 0.9572],\n",
      "        [ 0.9082],\n",
      "        [ 0.8312],\n",
      "        [-1.5074],\n",
      "        [-0.9731],\n",
      "        [ 0.6066],\n",
      "        [-0.1871],\n",
      "        [ 0.9329],\n",
      "        [-0.1871],\n",
      "        [ 0.6066],\n",
      "        [-0.6957],\n",
      "        [-0.2912],\n",
      "        [ 0.8830],\n",
      "        [ 0.7776],\n",
      "        [ 0.8046],\n",
      "        [ 1.1748],\n",
      "        [ 1.1748],\n",
      "        [ 1.0498],\n",
      "        [-0.2912],\n",
      "        [-1.2181],\n",
      "        [ 1.0498],\n",
      "        [-1.3835],\n",
      "        [ 0.9572],\n",
      "        [-1.2918],\n",
      "        [ 0.6066],\n",
      "        [-0.1871],\n",
      "        [ 0.7776],\n",
      "        [ 0.5767],\n",
      "        [ 1.1748],\n",
      "        [ 0.7776],\n",
      "        [-0.7913],\n",
      "        [-0.0828],\n",
      "        [-1.1136],\n",
      "        [-1.1668],\n",
      "        [ 0.9572],\n",
      "        [-0.9731],\n",
      "        [ 0.6653],\n",
      "        [ 1.1748],\n",
      "        [-1.5264],\n",
      "        [ 0.1239],\n",
      "        [-0.4289],\n",
      "        [-1.2918],\n",
      "        [ 1.0044],\n",
      "        [-0.4289],\n",
      "        [-0.6304],\n",
      "        [-1.4679],\n",
      "        [ 0.6653],\n",
      "        [-1.3835],\n",
      "        [ 0.4847],\n",
      "        [-0.4630],\n",
      "        [ 0.4847],\n",
      "        [ 1.1748],\n",
      "        [ 1.1405]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 194/10000,\n",
      " train_loss: 0.0022,\n",
      " train_mae: 0.0407,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2204],\n",
      "        [-1.2204],\n",
      "        [ 0.6668],\n",
      "        [ 0.6083],\n",
      "        [-0.0108],\n",
      "        [ 0.5177],\n",
      "        [ 0.8837],\n",
      "        [ 0.9335],\n",
      "        [ 0.8837],\n",
      "        [ 0.8057],\n",
      "        [-1.5320],\n",
      "        [-1.0025],\n",
      "        [ 0.5785],\n",
      "        [-0.2193],\n",
      "        [ 0.9088],\n",
      "        [-0.2193],\n",
      "        [ 0.5785],\n",
      "        [-0.7267],\n",
      "        [-0.3234],\n",
      "        [ 0.8582],\n",
      "        [ 0.7514],\n",
      "        [ 0.7788],\n",
      "        [ 1.1549],\n",
      "        [ 1.1549],\n",
      "        [ 1.0276],\n",
      "        [-0.3234],\n",
      "        [-1.2456],\n",
      "        [ 1.0276],\n",
      "        [-1.4094],\n",
      "        [ 0.9335],\n",
      "        [-1.3186],\n",
      "        [ 0.5785],\n",
      "        [-0.2193],\n",
      "        [ 0.7514],\n",
      "        [ 0.5483],\n",
      "        [ 1.1549],\n",
      "        [ 0.7514],\n",
      "        [-0.8219],\n",
      "        [-0.1149],\n",
      "        [-1.1420],\n",
      "        [-1.1947],\n",
      "        [ 0.9335],\n",
      "        [-1.0025],\n",
      "        [ 0.6378],\n",
      "        [ 1.1549],\n",
      "        [-1.5509],\n",
      "        [ 0.0924],\n",
      "        [-0.4609],\n",
      "        [-1.3186],\n",
      "        [ 0.9815],\n",
      "        [-0.4609],\n",
      "        [-0.6618],\n",
      "        [-1.4930],\n",
      "        [ 0.6378],\n",
      "        [-1.4094],\n",
      "        [ 0.4555],\n",
      "        [-0.4949],\n",
      "        [ 0.4555],\n",
      "        [ 1.1549],\n",
      "        [ 1.1199]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 195/10000,\n",
      " train_loss: 0.0025,\n",
      " train_mae: 0.0372,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1844],\n",
      "        [-1.1844],\n",
      "        [ 0.6878],\n",
      "        [ 0.6302],\n",
      "        [ 0.0199],\n",
      "        [ 0.5410],\n",
      "        [ 0.9013],\n",
      "        [ 0.9503],\n",
      "        [ 0.9013],\n",
      "        [ 0.8245],\n",
      "        [-1.4994],\n",
      "        [-0.9656],\n",
      "        [ 0.6008],\n",
      "        [-0.1861],\n",
      "        [ 0.9261],\n",
      "        [-0.1861],\n",
      "        [ 0.6008],\n",
      "        [-0.6900],\n",
      "        [-0.2893],\n",
      "        [ 0.8762],\n",
      "        [ 0.7711],\n",
      "        [ 0.7980],\n",
      "        [ 1.1685],\n",
      "        [ 1.1685],\n",
      "        [ 1.0430],\n",
      "        [-0.2893],\n",
      "        [-1.2098],\n",
      "        [ 1.0430],\n",
      "        [-1.3752],\n",
      "        [ 0.9503],\n",
      "        [-1.2833],\n",
      "        [ 0.6008],\n",
      "        [-0.1861],\n",
      "        [ 0.7711],\n",
      "        [ 0.5711],\n",
      "        [ 1.1685],\n",
      "        [ 0.7711],\n",
      "        [-0.7849],\n",
      "        [-0.0829],\n",
      "        [-1.1055],\n",
      "        [-1.1585],\n",
      "        [ 0.9503],\n",
      "        [-0.9656],\n",
      "        [ 0.6592],\n",
      "        [ 1.1685],\n",
      "        [-1.5186],\n",
      "        [ 0.1219],\n",
      "        [-0.4256],\n",
      "        [-1.2833],\n",
      "        [ 0.9976],\n",
      "        [-0.4256],\n",
      "        [-0.6253],\n",
      "        [-1.4598],\n",
      "        [ 0.6592],\n",
      "        [-1.3752],\n",
      "        [ 0.4797],\n",
      "        [-0.4594],\n",
      "        [ 0.4797],\n",
      "        [ 1.1685],\n",
      "        [ 1.1341]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 196/10000,\n",
      " train_loss: 0.0021,\n",
      " train_mae: 0.0381,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1834],\n",
      "        [-1.1834],\n",
      "        [ 0.6925],\n",
      "        [ 0.6349],\n",
      "        [ 0.0237],\n",
      "        [ 0.5455],\n",
      "        [ 0.9063],\n",
      "        [ 0.9554],\n",
      "        [ 0.9063],\n",
      "        [ 0.8294],\n",
      "        [-1.4994],\n",
      "        [-0.9640],\n",
      "        [ 0.6055],\n",
      "        [-0.1828],\n",
      "        [ 0.9311],\n",
      "        [-0.1828],\n",
      "        [ 0.6055],\n",
      "        [-0.6877],\n",
      "        [-0.2861],\n",
      "        [ 0.8811],\n",
      "        [ 0.7759],\n",
      "        [ 0.8029],\n",
      "        [ 1.1738],\n",
      "        [ 1.1738],\n",
      "        [ 1.0482],\n",
      "        [-0.2861],\n",
      "        [-1.2089],\n",
      "        [ 1.0482],\n",
      "        [-1.3747],\n",
      "        [ 0.9554],\n",
      "        [-1.2827],\n",
      "        [ 0.6055],\n",
      "        [-0.1828],\n",
      "        [ 0.7759],\n",
      "        [ 0.5757],\n",
      "        [ 1.1738],\n",
      "        [ 0.7759],\n",
      "        [-0.7829],\n",
      "        [-0.0793],\n",
      "        [-1.1043],\n",
      "        [-1.1574],\n",
      "        [ 0.9554],\n",
      "        [-0.9640],\n",
      "        [ 0.6639],\n",
      "        [ 1.1738],\n",
      "        [-1.5187],\n",
      "        [ 0.1258],\n",
      "        [-0.4227],\n",
      "        [-1.2827],\n",
      "        [ 1.0027],\n",
      "        [-0.4227],\n",
      "        [-0.6229],\n",
      "        [-1.4596],\n",
      "        [ 0.6639],\n",
      "        [-1.3747],\n",
      "        [ 0.4842],\n",
      "        [-0.4566],\n",
      "        [ 0.4842],\n",
      "        [ 1.1738],\n",
      "        [ 1.1393]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 197/10000,\n",
      " train_loss: 0.0022,\n",
      " train_mae: 0.0396,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2182],\n",
      "        [-1.2182],\n",
      "        [ 0.6700],\n",
      "        [ 0.6114],\n",
      "        [-0.0079],\n",
      "        [ 0.5207],\n",
      "        [ 0.8875],\n",
      "        [ 0.9375],\n",
      "        [ 0.8875],\n",
      "        [ 0.8092],\n",
      "        [-1.5316],\n",
      "        [-0.9997],\n",
      "        [ 0.5815],\n",
      "        [-0.2162],\n",
      "        [ 0.9127],\n",
      "        [-0.2162],\n",
      "        [ 0.5815],\n",
      "        [-0.7236],\n",
      "        [-0.3203],\n",
      "        [ 0.8618],\n",
      "        [ 0.7548],\n",
      "        [ 0.7822],\n",
      "        [ 1.1600],\n",
      "        [ 1.1600],\n",
      "        [ 1.0319],\n",
      "        [-0.3203],\n",
      "        [-1.2436],\n",
      "        [ 1.0319],\n",
      "        [-1.4082],\n",
      "        [ 0.9375],\n",
      "        [-1.3169],\n",
      "        [ 0.5815],\n",
      "        [-0.2162],\n",
      "        [ 0.7548],\n",
      "        [ 0.5513],\n",
      "        [ 1.1600],\n",
      "        [ 0.7548],\n",
      "        [-0.8188],\n",
      "        [-0.1119],\n",
      "        [-1.1395],\n",
      "        [-1.1924],\n",
      "        [ 0.9375],\n",
      "        [-0.9997],\n",
      "        [ 0.6409],\n",
      "        [ 1.1600],\n",
      "        [-1.5506],\n",
      "        [ 0.0953],\n",
      "        [-0.4577],\n",
      "        [-1.3169],\n",
      "        [ 0.9856],\n",
      "        [-0.4577],\n",
      "        [-0.6586],\n",
      "        [-1.4923],\n",
      "        [ 0.6409],\n",
      "        [-1.4082],\n",
      "        [ 0.4584],\n",
      "        [-0.4917],\n",
      "        [ 0.4584],\n",
      "        [ 1.1600],\n",
      "        [ 1.1248]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 198/10000,\n",
      " train_loss: 0.0023,\n",
      " train_mae: 0.0370,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1932],\n",
      "        [-1.1932],\n",
      "        [ 0.6861],\n",
      "        [ 0.6282],\n",
      "        [ 0.0146],\n",
      "        [ 0.5383],\n",
      "        [ 0.9014],\n",
      "        [ 0.9509],\n",
      "        [ 0.9014],\n",
      "        [ 0.8239],\n",
      "        [-1.5092],\n",
      "        [-0.9738],\n",
      "        [ 0.5986],\n",
      "        [-0.1922],\n",
      "        [ 0.9264],\n",
      "        [-0.1922],\n",
      "        [ 0.5986],\n",
      "        [-0.6976],\n",
      "        [-0.2957],\n",
      "        [ 0.8760],\n",
      "        [ 0.7701],\n",
      "        [ 0.7972],\n",
      "        [ 1.1713],\n",
      "        [ 1.1713],\n",
      "        [ 1.0444],\n",
      "        [-0.2957],\n",
      "        [-1.2186],\n",
      "        [ 1.0444],\n",
      "        [-1.3845],\n",
      "        [ 0.9509],\n",
      "        [-1.2924],\n",
      "        [ 0.5986],\n",
      "        [-0.1922],\n",
      "        [ 0.7701],\n",
      "        [ 0.5686],\n",
      "        [ 1.1713],\n",
      "        [ 0.7701],\n",
      "        [-0.7927],\n",
      "        [-0.0886],\n",
      "        [-1.1141],\n",
      "        [-1.1672],\n",
      "        [ 0.9509],\n",
      "        [-0.9738],\n",
      "        [ 0.6573],\n",
      "        [ 1.1713],\n",
      "        [-1.5285],\n",
      "        [ 0.1170],\n",
      "        [-0.4325],\n",
      "        [-1.2924],\n",
      "        [ 0.9985],\n",
      "        [-0.4325],\n",
      "        [-0.6327],\n",
      "        [-1.4694],\n",
      "        [ 0.6573],\n",
      "        [-1.3845],\n",
      "        [ 0.4767],\n",
      "        [-0.4663],\n",
      "        [ 0.4767],\n",
      "        [ 1.1713],\n",
      "        [ 1.1364]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 199/10000,\n",
      " train_loss: 0.0020,\n",
      " train_mae: 0.0372,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1799],\n",
      "        [-1.1799],\n",
      "        [ 0.6907],\n",
      "        [ 0.6330],\n",
      "        [ 0.0236],\n",
      "        [ 0.5438],\n",
      "        [ 0.9048],\n",
      "        [ 0.9540],\n",
      "        [ 0.9048],\n",
      "        [ 0.8277],\n",
      "        [-1.4971],\n",
      "        [-0.9605],\n",
      "        [ 0.6037],\n",
      "        [-0.1820],\n",
      "        [ 0.9296],\n",
      "        [-0.1820],\n",
      "        [ 0.6037],\n",
      "        [-0.6849],\n",
      "        [-0.2849],\n",
      "        [ 0.8795],\n",
      "        [ 0.7741],\n",
      "        [ 0.8011],\n",
      "        [ 1.1737],\n",
      "        [ 1.1737],\n",
      "        [ 1.0472],\n",
      "        [-0.2849],\n",
      "        [-1.2054],\n",
      "        [ 1.0472],\n",
      "        [-1.3718],\n",
      "        [ 0.9540],\n",
      "        [-1.2794],\n",
      "        [ 0.6037],\n",
      "        [-0.1820],\n",
      "        [ 0.7741],\n",
      "        [ 0.5739],\n",
      "        [ 1.1737],\n",
      "        [ 0.7741],\n",
      "        [-0.7797],\n",
      "        [-0.0790],\n",
      "        [-1.1007],\n",
      "        [-1.1539],\n",
      "        [ 0.9540],\n",
      "        [-0.9605],\n",
      "        [ 0.6621],\n",
      "        [ 1.1737],\n",
      "        [-1.5165],\n",
      "        [ 0.1252],\n",
      "        [-0.4209],\n",
      "        [-1.2794],\n",
      "        [ 1.0015],\n",
      "        [-0.4209],\n",
      "        [-0.6202],\n",
      "        [-1.4571],\n",
      "        [ 0.6621],\n",
      "        [-1.3718],\n",
      "        [ 0.4825],\n",
      "        [-0.4546],\n",
      "        [ 0.4825],\n",
      "        [ 1.1737],\n",
      "        [ 1.1389]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 200/10000,\n",
      " train_loss: 0.0021,\n",
      " train_mae: 0.0381,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2118],\n",
      "        [-1.2118],\n",
      "        [ 0.6698],\n",
      "        [ 0.6114],\n",
      "        [-0.0054],\n",
      "        [ 0.5208],\n",
      "        [ 0.8873],\n",
      "        [ 0.9374],\n",
      "        [ 0.8873],\n",
      "        [ 0.8090],\n",
      "        [-1.5267],\n",
      "        [-0.9932],\n",
      "        [ 0.5816],\n",
      "        [-0.2126],\n",
      "        [ 0.9126],\n",
      "        [-0.2126],\n",
      "        [ 0.5816],\n",
      "        [-0.7176],\n",
      "        [-0.3161],\n",
      "        [ 0.8616],\n",
      "        [ 0.7546],\n",
      "        [ 0.7820],\n",
      "        [ 1.1608],\n",
      "        [ 1.1608],\n",
      "        [ 1.0321],\n",
      "        [-0.3161],\n",
      "        [-1.2372],\n",
      "        [ 1.0321],\n",
      "        [-1.4024],\n",
      "        [ 0.9374],\n",
      "        [-1.3107],\n",
      "        [ 0.5816],\n",
      "        [-0.2126],\n",
      "        [ 0.7546],\n",
      "        [ 0.5514],\n",
      "        [ 1.1608],\n",
      "        [ 0.7546],\n",
      "        [-0.8126],\n",
      "        [-0.1088],\n",
      "        [-1.1330],\n",
      "        [-1.1859],\n",
      "        [ 0.9374],\n",
      "        [-0.9932],\n",
      "        [ 0.6408],\n",
      "        [ 1.1608],\n",
      "        [-1.5459],\n",
      "        [ 0.0973],\n",
      "        [-0.4529],\n",
      "        [-1.3107],\n",
      "        [ 0.9857],\n",
      "        [-0.4529],\n",
      "        [-0.6529],\n",
      "        [-1.4870],\n",
      "        [ 0.6408],\n",
      "        [-1.4024],\n",
      "        [ 0.4587],\n",
      "        [-0.4867],\n",
      "        [ 0.4587],\n",
      "        [ 1.1608],\n",
      "        [ 1.1255]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 201/10000,\n",
      " train_loss: 0.0021,\n",
      " train_mae: 0.0371,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1980],\n",
      "        [-1.1980],\n",
      "        [ 0.6859],\n",
      "        [ 0.6276],\n",
      "        [ 0.0120],\n",
      "        [ 0.5374],\n",
      "        [ 0.9024],\n",
      "        [ 0.9523],\n",
      "        [ 0.9024],\n",
      "        [ 0.8244],\n",
      "        [-1.5154],\n",
      "        [-0.9781],\n",
      "        [ 0.5979],\n",
      "        [-0.1952],\n",
      "        [ 0.9276],\n",
      "        [-0.1952],\n",
      "        [ 0.5979],\n",
      "        [-0.7014],\n",
      "        [-0.2988],\n",
      "        [ 0.8769],\n",
      "        [ 0.7703],\n",
      "        [ 0.7976],\n",
      "        [ 1.1745],\n",
      "        [ 1.1745],\n",
      "        [ 1.0465],\n",
      "        [-0.2988],\n",
      "        [-1.2236],\n",
      "        [ 1.0465],\n",
      "        [-1.3901],\n",
      "        [ 0.9523],\n",
      "        [-1.2977],\n",
      "        [ 0.5979],\n",
      "        [-0.1952],\n",
      "        [ 0.7703],\n",
      "        [ 0.5678],\n",
      "        [ 1.1745],\n",
      "        [ 0.7703],\n",
      "        [-0.7966],\n",
      "        [-0.0914],\n",
      "        [-1.1187],\n",
      "        [-1.1720],\n",
      "        [ 0.9523],\n",
      "        [-0.9781],\n",
      "        [ 0.6569],\n",
      "        [ 1.1745],\n",
      "        [-1.5348],\n",
      "        [ 0.1146],\n",
      "        [-0.4358],\n",
      "        [-1.2977],\n",
      "        [ 1.0003],\n",
      "        [-0.4358],\n",
      "        [-0.6364],\n",
      "        [-1.4754],\n",
      "        [ 0.6569],\n",
      "        [-1.3901],\n",
      "        [ 0.4755],\n",
      "        [-0.4697],\n",
      "        [ 0.4755],\n",
      "        [ 1.1745],\n",
      "        [ 1.1393]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 202/10000,\n",
      " train_loss: 0.0019,\n",
      " train_mae: 0.0368,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1823],\n",
      "        [-1.1823],\n",
      "        [ 0.6902],\n",
      "        [ 0.6323],\n",
      "        [ 0.0219],\n",
      "        [ 0.5428],\n",
      "        [ 0.9053],\n",
      "        [ 0.9548],\n",
      "        [ 0.9053],\n",
      "        [ 0.8277],\n",
      "        [-1.5008],\n",
      "        [-0.9624],\n",
      "        [ 0.6029],\n",
      "        [-0.1837],\n",
      "        [ 0.9303],\n",
      "        [-0.1837],\n",
      "        [ 0.6029],\n",
      "        [-0.6866],\n",
      "        [-0.2865],\n",
      "        [ 0.8799],\n",
      "        [ 0.7739],\n",
      "        [ 0.8011],\n",
      "        [ 1.1760],\n",
      "        [ 1.1760],\n",
      "        [ 1.0486],\n",
      "        [-0.2865],\n",
      "        [-1.2079],\n",
      "        [ 1.0486],\n",
      "        [-1.3748],\n",
      "        [ 0.9548],\n",
      "        [-1.2821],\n",
      "        [ 0.6029],\n",
      "        [-0.1837],\n",
      "        [ 0.7739],\n",
      "        [ 0.5730],\n",
      "        [ 1.1760],\n",
      "        [ 0.7739],\n",
      "        [-0.7815],\n",
      "        [-0.0807],\n",
      "        [-1.1029],\n",
      "        [-1.1562],\n",
      "        [ 0.9548],\n",
      "        [-0.9624],\n",
      "        [ 0.6614],\n",
      "        [ 1.1760],\n",
      "        [-1.5203],\n",
      "        [ 0.1236],\n",
      "        [-0.4226],\n",
      "        [-1.2821],\n",
      "        [ 1.0026],\n",
      "        [-0.4226],\n",
      "        [-0.6219],\n",
      "        [-1.4606],\n",
      "        [ 0.6614],\n",
      "        [-1.3748],\n",
      "        [ 0.4814],\n",
      "        [-0.4563],\n",
      "        [ 0.4814],\n",
      "        [ 1.1760],\n",
      "        [ 1.1410]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 203/10000,\n",
      " train_loss: 0.0020,\n",
      " train_mae: 0.0367,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2060],\n",
      "        [-1.2060],\n",
      "        [ 0.6688],\n",
      "        [ 0.6105],\n",
      "        [-0.0038],\n",
      "        [ 0.5202],\n",
      "        [ 0.8863],\n",
      "        [ 0.9364],\n",
      "        [ 0.8863],\n",
      "        [ 0.8079],\n",
      "        [-1.5223],\n",
      "        [-0.9873],\n",
      "        [ 0.5807],\n",
      "        [-0.2098],\n",
      "        [ 0.9116],\n",
      "        [-0.2098],\n",
      "        [ 0.5807],\n",
      "        [-0.7125],\n",
      "        [-0.3128],\n",
      "        [ 0.8606],\n",
      "        [ 0.7535],\n",
      "        [ 0.7809],\n",
      "        [ 1.1608],\n",
      "        [ 1.1608],\n",
      "        [ 1.0315],\n",
      "        [-0.3128],\n",
      "        [-1.2314],\n",
      "        [ 1.0315],\n",
      "        [-1.3973],\n",
      "        [ 0.9364],\n",
      "        [-1.3051],\n",
      "        [ 0.5807],\n",
      "        [-0.2098],\n",
      "        [ 0.7535],\n",
      "        [ 0.5506],\n",
      "        [ 1.1608],\n",
      "        [ 0.7535],\n",
      "        [-0.8071],\n",
      "        [-0.1067],\n",
      "        [-1.1271],\n",
      "        [-1.1801],\n",
      "        [ 0.9364],\n",
      "        [-0.9873],\n",
      "        [ 0.6398],\n",
      "        [ 1.1608],\n",
      "        [-1.5416],\n",
      "        [ 0.0983],\n",
      "        [-0.4488],\n",
      "        [-1.3051],\n",
      "        [ 0.9848],\n",
      "        [-0.4488],\n",
      "        [-0.6479],\n",
      "        [-1.4824],\n",
      "        [ 0.6398],\n",
      "        [-1.3973],\n",
      "        [ 0.4583],\n",
      "        [-0.4825],\n",
      "        [ 0.4583],\n",
      "        [ 1.1608],\n",
      "        [ 1.1252]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 204/10000,\n",
      " train_loss: 0.0020,\n",
      " train_mae: 0.0366,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1978],\n",
      "        [-1.1978],\n",
      "        [ 0.6853],\n",
      "        [ 0.6269],\n",
      "        [ 0.0115],\n",
      "        [ 0.5366],\n",
      "        [ 0.9025],\n",
      "        [ 0.9525],\n",
      "        [ 0.9025],\n",
      "        [ 0.8242],\n",
      "        [-1.5166],\n",
      "        [-0.9775],\n",
      "        [ 0.5972],\n",
      "        [-0.1954],\n",
      "        [ 0.9277],\n",
      "        [-0.1954],\n",
      "        [ 0.5972],\n",
      "        [-0.7008],\n",
      "        [-0.2988],\n",
      "        [ 0.8768],\n",
      "        [ 0.7698],\n",
      "        [ 0.7972],\n",
      "        [ 1.1761],\n",
      "        [ 1.1761],\n",
      "        [ 1.0472],\n",
      "        [-0.2988],\n",
      "        [-1.2235],\n",
      "        [ 1.0472],\n",
      "        [-1.3906],\n",
      "        [ 0.9525],\n",
      "        [-1.2977],\n",
      "        [ 0.5972],\n",
      "        [-0.1954],\n",
      "        [ 0.7698],\n",
      "        [ 0.5670],\n",
      "        [ 1.1761],\n",
      "        [ 0.7698],\n",
      "        [-0.7960],\n",
      "        [-0.0918],\n",
      "        [-1.1183],\n",
      "        [-1.1717],\n",
      "        [ 0.9525],\n",
      "        [-0.9775],\n",
      "        [ 0.6563],\n",
      "        [ 1.1761],\n",
      "        [-1.5361],\n",
      "        [ 0.1140],\n",
      "        [-0.4356],\n",
      "        [-1.2977],\n",
      "        [ 1.0008],\n",
      "        [-0.4356],\n",
      "        [-0.6359],\n",
      "        [-1.4763],\n",
      "        [ 0.6563],\n",
      "        [-1.3906],\n",
      "        [ 0.4746],\n",
      "        [-0.4695],\n",
      "        [ 0.4746],\n",
      "        [ 1.1761],\n",
      "        [ 1.1407]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 205/10000,\n",
      " train_loss: 0.0019,\n",
      " train_mae: 0.0364,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1868],\n",
      "        [-1.1868],\n",
      "        [ 0.6897],\n",
      "        [ 0.6316],\n",
      "        [ 0.0194],\n",
      "        [ 0.5417],\n",
      "        [ 0.9059],\n",
      "        [ 0.9558],\n",
      "        [ 0.9059],\n",
      "        [ 0.8280],\n",
      "        [-1.5067],\n",
      "        [-0.9664],\n",
      "        [ 0.6020],\n",
      "        [-0.1865],\n",
      "        [ 0.9311],\n",
      "        [-0.1865],\n",
      "        [ 0.6020],\n",
      "        [-0.6900],\n",
      "        [-0.2895],\n",
      "        [ 0.8804],\n",
      "        [ 0.7739],\n",
      "        [ 0.8011],\n",
      "        [ 1.1788],\n",
      "        [ 1.1788],\n",
      "        [ 1.0502],\n",
      "        [-0.2895],\n",
      "        [-1.2125],\n",
      "        [ 1.0502],\n",
      "        [-1.3801],\n",
      "        [ 0.9558],\n",
      "        [-1.2869],\n",
      "        [ 0.6020],\n",
      "        [-0.1865],\n",
      "        [ 0.7739],\n",
      "        [ 0.5720],\n",
      "        [ 1.1788],\n",
      "        [ 0.7739],\n",
      "        [-0.7851],\n",
      "        [-0.0833],\n",
      "        [-1.1072],\n",
      "        [-1.1607],\n",
      "        [ 0.9558],\n",
      "        [-0.9664],\n",
      "        [ 0.6608],\n",
      "        [ 1.1788],\n",
      "        [-1.5263],\n",
      "        [ 0.1213],\n",
      "        [-0.4257],\n",
      "        [-1.2869],\n",
      "        [ 1.0039],\n",
      "        [-0.4257],\n",
      "        [-0.6253],\n",
      "        [-1.4662],\n",
      "        [ 0.6608],\n",
      "        [-1.3801],\n",
      "        [ 0.4801],\n",
      "        [-0.4594],\n",
      "        [ 0.4801],\n",
      "        [ 1.1788],\n",
      "        [ 1.1434]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 206/10000,\n",
      " train_loss: 0.0019,\n",
      " train_mae: 0.0358,\n",
      " epoch_time_duration: 0.0102\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2031],\n",
      "        [-1.2031],\n",
      "        [ 0.6686],\n",
      "        [ 0.6102],\n",
      "        [-0.0028],\n",
      "        [ 0.5200],\n",
      "        [ 0.8863],\n",
      "        [ 0.9366],\n",
      "        [ 0.8863],\n",
      "        [ 0.8077],\n",
      "        [-1.5209],\n",
      "        [-0.9841],\n",
      "        [ 0.5805],\n",
      "        [-0.2082],\n",
      "        [ 0.9117],\n",
      "        [-0.2082],\n",
      "        [ 0.5805],\n",
      "        [-0.7095],\n",
      "        [-0.3108],\n",
      "        [ 0.8605],\n",
      "        [ 0.7533],\n",
      "        [ 0.7807],\n",
      "        [ 1.1620],\n",
      "        [ 1.1620],\n",
      "        [ 1.0319],\n",
      "        [-0.3108],\n",
      "        [-1.2286],\n",
      "        [ 1.0319],\n",
      "        [-1.3951],\n",
      "        [ 0.9366],\n",
      "        [-1.3025],\n",
      "        [ 0.5805],\n",
      "        [-0.2082],\n",
      "        [ 0.7533],\n",
      "        [ 0.5504],\n",
      "        [ 1.1620],\n",
      "        [ 0.7533],\n",
      "        [-0.8040],\n",
      "        [-0.1053],\n",
      "        [-1.1240],\n",
      "        [-1.1771],\n",
      "        [ 0.9366],\n",
      "        [-0.9841],\n",
      "        [ 0.6396],\n",
      "        [ 1.1620],\n",
      "        [-1.5404],\n",
      "        [ 0.0990],\n",
      "        [-0.4465],\n",
      "        [-1.3025],\n",
      "        [ 0.9851],\n",
      "        [-0.4465],\n",
      "        [-0.6451],\n",
      "        [-1.4807],\n",
      "        [ 0.6396],\n",
      "        [-1.3951],\n",
      "        [ 0.4581],\n",
      "        [-0.4801],\n",
      "        [ 0.4581],\n",
      "        [ 1.1620],\n",
      "        [ 1.1262]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 207/10000,\n",
      " train_loss: 0.0020,\n",
      " train_mae: 0.0357,\n",
      " epoch_time_duration: 0.0091\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1946],\n",
      "        [-1.1946],\n",
      "        [ 0.6847],\n",
      "        [ 0.6264],\n",
      "        [ 0.0125],\n",
      "        [ 0.5361],\n",
      "        [ 0.9022],\n",
      "        [ 0.9523],\n",
      "        [ 0.9022],\n",
      "        [ 0.8237],\n",
      "        [-1.5149],\n",
      "        [-0.9741],\n",
      "        [ 0.5967],\n",
      "        [-0.1937],\n",
      "        [ 0.9275],\n",
      "        [-0.1937],\n",
      "        [ 0.5967],\n",
      "        [-0.6976],\n",
      "        [-0.2968],\n",
      "        [ 0.8765],\n",
      "        [ 0.7693],\n",
      "        [ 0.7967],\n",
      "        [ 1.1770],\n",
      "        [ 1.1770],\n",
      "        [ 1.0474],\n",
      "        [-0.2968],\n",
      "        [-1.2203],\n",
      "        [ 1.0474],\n",
      "        [-1.3881],\n",
      "        [ 0.9523],\n",
      "        [-1.2949],\n",
      "        [ 0.5967],\n",
      "        [-0.1937],\n",
      "        [ 0.7693],\n",
      "        [ 0.5666],\n",
      "        [ 1.1770],\n",
      "        [ 0.7693],\n",
      "        [-0.7927],\n",
      "        [-0.0905],\n",
      "        [-1.1150],\n",
      "        [-1.1685],\n",
      "        [ 0.9523],\n",
      "        [-0.9741],\n",
      "        [ 0.6558],\n",
      "        [ 1.1770],\n",
      "        [-1.5346],\n",
      "        [ 0.1146],\n",
      "        [-0.4331],\n",
      "        [-1.2949],\n",
      "        [ 1.0008],\n",
      "        [-0.4331],\n",
      "        [-0.6329],\n",
      "        [-1.4744],\n",
      "        [ 0.6558],\n",
      "        [-1.3881],\n",
      "        [ 0.4743],\n",
      "        [-0.4669],\n",
      "        [ 0.4743],\n",
      "        [ 1.1770],\n",
      "        [ 1.1413]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 208/10000,\n",
      " train_loss: 0.0018,\n",
      " train_mae: 0.0358,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1910],\n",
      "        [-1.1910],\n",
      "        [ 0.6880],\n",
      "        [ 0.6297],\n",
      "        [ 0.0164],\n",
      "        [ 0.5395],\n",
      "        [ 0.9052],\n",
      "        [ 0.9554],\n",
      "        [ 0.9052],\n",
      "        [ 0.8268],\n",
      "        [-1.5122],\n",
      "        [-0.9701],\n",
      "        [ 0.6000],\n",
      "        [-0.1896],\n",
      "        [ 0.9305],\n",
      "        [-0.1896],\n",
      "        [ 0.6000],\n",
      "        [-0.6934],\n",
      "        [-0.2927],\n",
      "        [ 0.8795],\n",
      "        [ 0.7725],\n",
      "        [ 0.7999],\n",
      "        [ 1.1800],\n",
      "        [ 1.1800],\n",
      "        [ 1.0505],\n",
      "        [-0.2927],\n",
      "        [-1.2167],\n",
      "        [ 1.0505],\n",
      "        [-1.3849],\n",
      "        [ 0.9554],\n",
      "        [-1.2914],\n",
      "        [ 0.6000],\n",
      "        [-0.1896],\n",
      "        [ 0.7725],\n",
      "        [ 0.5699],\n",
      "        [ 1.1800],\n",
      "        [ 0.7725],\n",
      "        [-0.7885],\n",
      "        [-0.0865],\n",
      "        [-1.1112],\n",
      "        [-1.1648],\n",
      "        [ 0.9554],\n",
      "        [-0.9701],\n",
      "        [ 0.6590],\n",
      "        [ 1.1800],\n",
      "        [-1.5319],\n",
      "        [ 0.1184],\n",
      "        [-0.4289],\n",
      "        [-1.2914],\n",
      "        [ 1.0038],\n",
      "        [-0.4289],\n",
      "        [-0.6286],\n",
      "        [-1.4715],\n",
      "        [ 0.6590],\n",
      "        [-1.3849],\n",
      "        [ 0.4777],\n",
      "        [-0.4627],\n",
      "        [ 0.4777],\n",
      "        [ 1.1800],\n",
      "        [ 1.1444]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 209/10000,\n",
      " train_loss: 0.0018,\n",
      " train_mae: 0.0352,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2023],\n",
      "        [-1.2023],\n",
      "        [ 0.6692],\n",
      "        [ 0.6108],\n",
      "        [-0.0019],\n",
      "        [ 0.5205],\n",
      "        [ 0.8874],\n",
      "        [ 0.9379],\n",
      "        [ 0.8874],\n",
      "        [ 0.8086],\n",
      "        [-1.5218],\n",
      "        [-0.9828],\n",
      "        [ 0.5810],\n",
      "        [-0.2071],\n",
      "        [ 0.9129],\n",
      "        [-0.2071],\n",
      "        [ 0.5810],\n",
      "        [-0.7080],\n",
      "        [-0.3096],\n",
      "        [ 0.8616],\n",
      "        [ 0.7540],\n",
      "        [ 0.7815],\n",
      "        [ 1.1644],\n",
      "        [ 1.1644],\n",
      "        [ 1.0336],\n",
      "        [-0.3096],\n",
      "        [-1.2279],\n",
      "        [ 1.0336],\n",
      "        [-1.3951],\n",
      "        [ 0.9379],\n",
      "        [-1.3021],\n",
      "        [ 0.5810],\n",
      "        [-0.2071],\n",
      "        [ 0.7540],\n",
      "        [ 0.5509],\n",
      "        [ 1.1644],\n",
      "        [ 0.7540],\n",
      "        [-0.8025],\n",
      "        [-0.1044],\n",
      "        [-1.1230],\n",
      "        [-1.1763],\n",
      "        [ 0.9379],\n",
      "        [-0.9828],\n",
      "        [ 0.6402],\n",
      "        [ 1.1644],\n",
      "        [-1.5414],\n",
      "        [ 0.0997],\n",
      "        [-0.4451],\n",
      "        [-1.3021],\n",
      "        [ 0.9866],\n",
      "        [-0.4451],\n",
      "        [-0.6436],\n",
      "        [-1.4813],\n",
      "        [ 0.6402],\n",
      "        [-1.3951],\n",
      "        [ 0.4586],\n",
      "        [-0.4787],\n",
      "        [ 0.4586],\n",
      "        [ 1.1644],\n",
      "        [ 1.1284]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 210/10000,\n",
      " train_loss: 0.0019,\n",
      " train_mae: 0.0350,\n",
      " epoch_time_duration: 0.0073\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1909],\n",
      "        [-1.1909],\n",
      "        [ 0.6849],\n",
      "        [ 0.6266],\n",
      "        [ 0.0142],\n",
      "        [ 0.5365],\n",
      "        [ 0.9024],\n",
      "        [ 0.9527],\n",
      "        [ 0.9024],\n",
      "        [ 0.8239],\n",
      "        [-1.5128],\n",
      "        [-0.9700],\n",
      "        [ 0.5969],\n",
      "        [-0.1913],\n",
      "        [ 0.9278],\n",
      "        [-0.1913],\n",
      "        [ 0.5969],\n",
      "        [-0.6938],\n",
      "        [-0.2940],\n",
      "        [ 0.8767],\n",
      "        [ 0.7695],\n",
      "        [ 0.7969],\n",
      "        [ 1.1782],\n",
      "        [ 1.1782],\n",
      "        [ 1.0481],\n",
      "        [-0.2940],\n",
      "        [-1.2167],\n",
      "        [ 1.0481],\n",
      "        [-1.3852],\n",
      "        [ 0.9527],\n",
      "        [-1.2915],\n",
      "        [ 0.5969],\n",
      "        [-0.1913],\n",
      "        [ 0.7695],\n",
      "        [ 0.5668],\n",
      "        [ 1.1782],\n",
      "        [ 0.7695],\n",
      "        [-0.7887],\n",
      "        [-0.0884],\n",
      "        [-1.1111],\n",
      "        [-1.1647],\n",
      "        [ 0.9527],\n",
      "        [-0.9700],\n",
      "        [ 0.6559],\n",
      "        [ 1.1782],\n",
      "        [-1.5326],\n",
      "        [ 0.1160],\n",
      "        [-0.4299],\n",
      "        [-1.2915],\n",
      "        [ 1.0012],\n",
      "        [-0.4299],\n",
      "        [-0.6291],\n",
      "        [-1.4720],\n",
      "        [ 0.6559],\n",
      "        [-1.3852],\n",
      "        [ 0.4747],\n",
      "        [-0.4635],\n",
      "        [ 0.4747],\n",
      "        [ 1.1782],\n",
      "        [ 1.1423]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 211/10000,\n",
      " train_loss: 0.0018,\n",
      " train_mae: 0.0352,\n",
      " epoch_time_duration: 0.0074\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1944],\n",
      "        [-1.1944],\n",
      "        [ 0.6848],\n",
      "        [ 0.6263],\n",
      "        [ 0.0127],\n",
      "        [ 0.5360],\n",
      "        [ 0.9029],\n",
      "        [ 0.9533],\n",
      "        [ 0.9029],\n",
      "        [ 0.8242],\n",
      "        [-1.5168],\n",
      "        [-0.9732],\n",
      "        [ 0.5966],\n",
      "        [-0.1932],\n",
      "        [ 0.9283],\n",
      "        [-0.1932],\n",
      "        [ 0.5966],\n",
      "        [-0.6965],\n",
      "        [-0.2961],\n",
      "        [ 0.8771],\n",
      "        [ 0.7696],\n",
      "        [ 0.7971],\n",
      "        [ 1.1795],\n",
      "        [ 1.1795],\n",
      "        [ 1.0489],\n",
      "        [-0.2961],\n",
      "        [-1.2202],\n",
      "        [ 1.0489],\n",
      "        [-1.3889],\n",
      "        [ 0.9533],\n",
      "        [-1.2951],\n",
      "        [ 0.5966],\n",
      "        [-0.1932],\n",
      "        [ 0.7696],\n",
      "        [ 0.5664],\n",
      "        [ 1.1795],\n",
      "        [ 0.7696],\n",
      "        [-0.7916],\n",
      "        [-0.0901],\n",
      "        [-1.1144],\n",
      "        [-1.1681],\n",
      "        [ 0.9533],\n",
      "        [-0.9732],\n",
      "        [ 0.6557],\n",
      "        [ 1.1795],\n",
      "        [-1.5366],\n",
      "        [ 0.1146],\n",
      "        [-0.4322],\n",
      "        [-1.2951],\n",
      "        [ 1.0020],\n",
      "        [-0.4322],\n",
      "        [-0.6318],\n",
      "        [-1.4759],\n",
      "        [ 0.6557],\n",
      "        [-1.3889],\n",
      "        [ 0.4741],\n",
      "        [-0.4659],\n",
      "        [ 0.4741],\n",
      "        [ 1.1795],\n",
      "        [ 1.1435]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 212/10000,\n",
      " train_loss: 0.0018,\n",
      " train_mae: 0.0347,\n",
      " epoch_time_duration: 0.0074\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2017e+00],\n",
      "        [-1.2017e+00],\n",
      "        [ 6.7070e-01],\n",
      "        [ 6.1219e-01],\n",
      "        [-5.6214e-04],\n",
      "        [ 5.2180e-01],\n",
      "        [ 8.8941e-01],\n",
      "        [ 9.4002e-01],\n",
      "        [ 8.8941e-01],\n",
      "        [ 8.1039e-01],\n",
      "        [-1.5230e+00],\n",
      "        [-9.8161e-01],\n",
      "        [ 5.8240e-01],\n",
      "        [-2.0560e-01],\n",
      "        [ 9.1492e-01],\n",
      "        [-2.0560e-01],\n",
      "        [ 5.8240e-01],\n",
      "        [-7.0642e-01],\n",
      "        [-3.0806e-01],\n",
      "        [ 8.6348e-01],\n",
      "        [ 7.5568e-01],\n",
      "        [ 7.8323e-01],\n",
      "        [ 1.1676e+00],\n",
      "        [ 1.1676e+00],\n",
      "        [ 1.0362e+00],\n",
      "        [-3.0806e-01],\n",
      "        [-1.2274e+00],\n",
      "        [ 1.0362e+00],\n",
      "        [-1.3955e+00],\n",
      "        [ 9.4002e-01],\n",
      "        [-1.3020e+00],\n",
      "        [ 5.8240e-01],\n",
      "        [-2.0560e-01],\n",
      "        [ 7.5568e-01],\n",
      "        [ 5.5227e-01],\n",
      "        [ 1.1676e+00],\n",
      "        [ 7.5568e-01],\n",
      "        [-8.0099e-01],\n",
      "        [-1.0294e-01],\n",
      "        [-1.1222e+00],\n",
      "        [-1.1756e+00],\n",
      "        [ 9.4002e-01],\n",
      "        [-9.8161e-01],\n",
      "        [ 6.4163e-01],\n",
      "        [ 1.1676e+00],\n",
      "        [-1.5427e+00],\n",
      "        [ 1.0106e-01],\n",
      "        [-4.4351e-01],\n",
      "        [-1.3020e+00],\n",
      "        [ 9.8894e-01],\n",
      "        [-4.4351e-01],\n",
      "        [-6.4201e-01],\n",
      "        [-1.4822e+00],\n",
      "        [ 6.4163e-01],\n",
      "        [-1.3955e+00],\n",
      "        [ 4.5991e-01],\n",
      "        [-4.7705e-01],\n",
      "        [ 4.5991e-01],\n",
      "        [ 1.1676e+00],\n",
      "        [ 1.1314e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 213/10000,\n",
      " train_loss: 0.0018,\n",
      " train_mae: 0.0345,\n",
      " epoch_time_duration: 0.0080\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1880],\n",
      "        [-1.1880],\n",
      "        [ 0.6855],\n",
      "        [ 0.6272],\n",
      "        [ 0.0160],\n",
      "        [ 0.5371],\n",
      "        [ 0.9032],\n",
      "        [ 0.9535],\n",
      "        [ 0.9032],\n",
      "        [ 0.8245],\n",
      "        [-1.5116],\n",
      "        [-0.9667],\n",
      "        [ 0.5975],\n",
      "        [-0.1890],\n",
      "        [ 0.9286],\n",
      "        [-0.1890],\n",
      "        [ 0.5975],\n",
      "        [-0.6905],\n",
      "        [-0.2914],\n",
      "        [ 0.8774],\n",
      "        [ 0.7701],\n",
      "        [ 0.7975],\n",
      "        [ 1.1799],\n",
      "        [ 1.1799],\n",
      "        [ 1.0492],\n",
      "        [-0.2914],\n",
      "        [-1.2138],\n",
      "        [ 1.0492],\n",
      "        [-1.3831],\n",
      "        [ 0.9535],\n",
      "        [-1.2889],\n",
      "        [ 0.5975],\n",
      "        [-0.1890],\n",
      "        [ 0.7701],\n",
      "        [ 0.5675],\n",
      "        [ 1.1799],\n",
      "        [ 0.7701],\n",
      "        [-0.7853],\n",
      "        [-0.0863],\n",
      "        [-1.1079],\n",
      "        [-1.1617],\n",
      "        [ 0.9535],\n",
      "        [-0.9667],\n",
      "        [ 0.6565],\n",
      "        [ 1.1799],\n",
      "        [-1.5315],\n",
      "        [ 0.1174],\n",
      "        [-0.4270],\n",
      "        [-1.2889],\n",
      "        [ 1.0022],\n",
      "        [-0.4270],\n",
      "        [-0.6259],\n",
      "        [-1.4704],\n",
      "        [ 0.6565],\n",
      "        [-1.3831],\n",
      "        [ 0.4754],\n",
      "        [-0.4606],\n",
      "        [ 0.4754],\n",
      "        [ 1.1799],\n",
      "        [ 1.1439]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 214/10000,\n",
      " train_loss: 0.0018,\n",
      " train_mae: 0.0345,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1975],\n",
      "        [-1.1975],\n",
      "        [ 0.6806],\n",
      "        [ 0.6220],\n",
      "        [ 0.0084],\n",
      "        [ 0.5315],\n",
      "        [ 0.8994],\n",
      "        [ 0.9501],\n",
      "        [ 0.8994],\n",
      "        [ 0.8204],\n",
      "        [-1.5210],\n",
      "        [-0.9761],\n",
      "        [ 0.5922],\n",
      "        [-0.1971],\n",
      "        [ 0.9250],\n",
      "        [-0.1971],\n",
      "        [ 0.5922],\n",
      "        [-0.6996],\n",
      "        [-0.2999],\n",
      "        [ 0.8735],\n",
      "        [ 0.7656],\n",
      "        [ 0.7932],\n",
      "        [ 1.1777],\n",
      "        [ 1.1777],\n",
      "        [ 1.0463],\n",
      "        [-0.2999],\n",
      "        [-1.2234],\n",
      "        [ 1.0463],\n",
      "        [-1.3926],\n",
      "        [ 0.9501],\n",
      "        [-1.2984],\n",
      "        [ 0.5922],\n",
      "        [-0.1971],\n",
      "        [ 0.7656],\n",
      "        [ 0.5620],\n",
      "        [ 1.1777],\n",
      "        [ 0.7656],\n",
      "        [-0.7946],\n",
      "        [-0.0942],\n",
      "        [-1.1175],\n",
      "        [-1.1712],\n",
      "        [ 0.9501],\n",
      "        [-0.9761],\n",
      "        [ 0.6515],\n",
      "        [ 1.1777],\n",
      "        [-1.5409],\n",
      "        [ 0.1102],\n",
      "        [-0.4357],\n",
      "        [-1.2984],\n",
      "        [ 0.9990],\n",
      "        [-0.4357],\n",
      "        [-0.6350],\n",
      "        [-1.4799],\n",
      "        [ 0.6515],\n",
      "        [-1.3926],\n",
      "        [ 0.4696],\n",
      "        [-0.4694],\n",
      "        [ 0.4696],\n",
      "        [ 1.1777],\n",
      "        [ 1.1415]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 215/10000,\n",
      " train_loss: 0.0017,\n",
      " train_mae: 0.0341,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2000],\n",
      "        [-1.2000],\n",
      "        [ 0.6732],\n",
      "        [ 0.6147],\n",
      "        [ 0.0021],\n",
      "        [ 0.5242],\n",
      "        [ 0.8923],\n",
      "        [ 0.9430],\n",
      "        [ 0.8923],\n",
      "        [ 0.8131],\n",
      "        [-1.5231],\n",
      "        [-0.9792],\n",
      "        [ 0.5849],\n",
      "        [-0.2028],\n",
      "        [ 0.9179],\n",
      "        [-0.2028],\n",
      "        [ 0.5849],\n",
      "        [-0.7035],\n",
      "        [-0.3052],\n",
      "        [ 0.8663],\n",
      "        [ 0.7583],\n",
      "        [ 0.7859],\n",
      "        [ 1.1714],\n",
      "        [ 1.1714],\n",
      "        [ 1.0395],\n",
      "        [-0.3052],\n",
      "        [-1.2258],\n",
      "        [ 1.0395],\n",
      "        [-1.3948],\n",
      "        [ 0.9430],\n",
      "        [-1.3008],\n",
      "        [ 0.5849],\n",
      "        [-0.2028],\n",
      "        [ 0.7583],\n",
      "        [ 0.5547],\n",
      "        [ 1.1714],\n",
      "        [ 0.7583],\n",
      "        [-0.7982],\n",
      "        [-0.1002],\n",
      "        [-1.1201],\n",
      "        [-1.1738],\n",
      "        [ 0.9430],\n",
      "        [-0.9792],\n",
      "        [ 0.6441],\n",
      "        [ 1.1714],\n",
      "        [-1.5431],\n",
      "        [ 0.1037],\n",
      "        [-0.4406],\n",
      "        [-1.3008],\n",
      "        [ 0.9921],\n",
      "        [-0.4406],\n",
      "        [-0.6391],\n",
      "        [-1.4820],\n",
      "        [ 0.6441],\n",
      "        [-1.3948],\n",
      "        [ 0.4623],\n",
      "        [-0.4741],\n",
      "        [ 0.4623],\n",
      "        [ 1.1714],\n",
      "        [ 1.1350]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 216/10000,\n",
      " train_loss: 0.0017,\n",
      " train_mae: 0.0341,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1868],\n",
      "        [-1.1868],\n",
      "        [ 0.6854],\n",
      "        [ 0.6271],\n",
      "        [ 0.0165],\n",
      "        [ 0.5370],\n",
      "        [ 0.9035],\n",
      "        [ 0.9540],\n",
      "        [ 0.9035],\n",
      "        [ 0.8246],\n",
      "        [-1.5119],\n",
      "        [-0.9650],\n",
      "        [ 0.5974],\n",
      "        [-0.1880],\n",
      "        [ 0.9289],\n",
      "        [-0.1880],\n",
      "        [ 0.5974],\n",
      "        [-0.6888],\n",
      "        [-0.2903],\n",
      "        [ 0.8776],\n",
      "        [ 0.7701],\n",
      "        [ 0.7976],\n",
      "        [ 1.1812],\n",
      "        [ 1.1812],\n",
      "        [ 1.0499],\n",
      "        [-0.2903],\n",
      "        [-1.2127],\n",
      "        [ 1.0499],\n",
      "        [-1.3827],\n",
      "        [ 0.9540],\n",
      "        [-1.2881],\n",
      "        [ 0.5974],\n",
      "        [-0.1880],\n",
      "        [ 0.7701],\n",
      "        [ 0.5674],\n",
      "        [ 1.1812],\n",
      "        [ 0.7701],\n",
      "        [-0.7836],\n",
      "        [-0.0856],\n",
      "        [-1.1065],\n",
      "        [-1.1604],\n",
      "        [ 0.9540],\n",
      "        [-0.9650],\n",
      "        [ 0.6564],\n",
      "        [ 1.1812],\n",
      "        [-1.5320],\n",
      "        [ 0.1178],\n",
      "        [-0.4256],\n",
      "        [-1.2881],\n",
      "        [ 1.0028],\n",
      "        [-0.4256],\n",
      "        [-0.6242],\n",
      "        [-1.4705],\n",
      "        [ 0.6564],\n",
      "        [-1.3827],\n",
      "        [ 0.4754],\n",
      "        [-0.4592],\n",
      "        [ 0.4754],\n",
      "        [ 1.1812],\n",
      "        [ 1.1450]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 217/10000,\n",
      " train_loss: 0.0017,\n",
      " train_mae: 0.0340,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2003],\n",
      "        [-1.2003],\n",
      "        [ 0.6763],\n",
      "        [ 0.6177],\n",
      "        [ 0.0042],\n",
      "        [ 0.5271],\n",
      "        [ 0.8958],\n",
      "        [ 0.9467],\n",
      "        [ 0.8958],\n",
      "        [ 0.8165],\n",
      "        [-1.5249],\n",
      "        [-0.9788],\n",
      "        [ 0.5878],\n",
      "        [-0.2009],\n",
      "        [ 0.9215],\n",
      "        [-0.2009],\n",
      "        [ 0.5878],\n",
      "        [-0.7025],\n",
      "        [-0.3034],\n",
      "        [ 0.8698],\n",
      "        [ 0.7616],\n",
      "        [ 0.7892],\n",
      "        [ 1.1757],\n",
      "        [ 1.1757],\n",
      "        [ 1.0434],\n",
      "        [-0.3034],\n",
      "        [-1.2262],\n",
      "        [ 1.0434],\n",
      "        [-1.3959],\n",
      "        [ 0.9467],\n",
      "        [-1.3015],\n",
      "        [ 0.5878],\n",
      "        [-0.2009],\n",
      "        [ 0.7616],\n",
      "        [ 0.5576],\n",
      "        [ 1.1757],\n",
      "        [ 0.7616],\n",
      "        [-0.7974],\n",
      "        [-0.0982],\n",
      "        [-1.1202],\n",
      "        [-1.1740],\n",
      "        [ 0.9467],\n",
      "        [-0.9788],\n",
      "        [ 0.6472],\n",
      "        [ 1.1757],\n",
      "        [-1.5449],\n",
      "        [ 0.1059],\n",
      "        [-0.4391],\n",
      "        [-1.3015],\n",
      "        [ 0.9959],\n",
      "        [-0.4391],\n",
      "        [-0.6379],\n",
      "        [-1.4836],\n",
      "        [ 0.6472],\n",
      "        [-1.3959],\n",
      "        [ 0.4651],\n",
      "        [-0.4726],\n",
      "        [ 0.4651],\n",
      "        [ 1.1757],\n",
      "        [ 1.1392]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 218/10000,\n",
      " train_loss: 0.0017,\n",
      " train_mae: 0.0334,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1967],\n",
      "        [-1.1967],\n",
      "        [ 0.6769],\n",
      "        [ 0.6184],\n",
      "        [ 0.0063],\n",
      "        [ 0.5280],\n",
      "        [ 0.8962],\n",
      "        [ 0.9470],\n",
      "        [ 0.8962],\n",
      "        [ 0.8169],\n",
      "        [-1.5218],\n",
      "        [-0.9751],\n",
      "        [ 0.5886],\n",
      "        [-0.1984],\n",
      "        [ 0.9218],\n",
      "        [-0.1984],\n",
      "        [ 0.5886],\n",
      "        [-0.6990],\n",
      "        [-0.3007],\n",
      "        [ 0.8702],\n",
      "        [ 0.7621],\n",
      "        [ 0.7897],\n",
      "        [ 1.1761],\n",
      "        [ 1.1761],\n",
      "        [ 1.0437],\n",
      "        [-0.3007],\n",
      "        [-1.2226],\n",
      "        [ 1.0437],\n",
      "        [-1.3925],\n",
      "        [ 0.9470],\n",
      "        [-1.2979],\n",
      "        [ 0.5886],\n",
      "        [-0.1984],\n",
      "        [ 0.7621],\n",
      "        [ 0.5584],\n",
      "        [ 1.1761],\n",
      "        [ 0.7621],\n",
      "        [-0.7938],\n",
      "        [-0.0959],\n",
      "        [-1.1165],\n",
      "        [-1.1703],\n",
      "        [ 0.9470],\n",
      "        [-0.9751],\n",
      "        [ 0.6478],\n",
      "        [ 1.1761],\n",
      "        [-1.5419],\n",
      "        [ 0.1077],\n",
      "        [-0.4360],\n",
      "        [-1.2979],\n",
      "        [ 0.9962],\n",
      "        [-0.4360],\n",
      "        [-0.6345],\n",
      "        [-1.4804],\n",
      "        [ 0.6478],\n",
      "        [-1.3925],\n",
      "        [ 0.4661],\n",
      "        [-0.4695],\n",
      "        [ 0.4661],\n",
      "        [ 1.1761],\n",
      "        [ 1.1395]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 219/10000,\n",
      " train_loss: 0.0016,\n",
      " train_mae: 0.0334,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1879],\n",
      "        [-1.1879],\n",
      "        [ 0.6835],\n",
      "        [ 0.6251],\n",
      "        [ 0.0147],\n",
      "        [ 0.5349],\n",
      "        [ 0.9020],\n",
      "        [ 0.9527],\n",
      "        [ 0.9020],\n",
      "        [ 0.8230],\n",
      "        [-1.5143],\n",
      "        [-0.9659],\n",
      "        [ 0.5954],\n",
      "        [-0.1895],\n",
      "        [ 0.9276],\n",
      "        [-0.1895],\n",
      "        [ 0.5954],\n",
      "        [-0.6896],\n",
      "        [-0.2916],\n",
      "        [ 0.8761],\n",
      "        [ 0.7683],\n",
      "        [ 0.7958],\n",
      "        [ 1.1812],\n",
      "        [ 1.1812],\n",
      "        [ 1.0491],\n",
      "        [-0.2916],\n",
      "        [-1.2139],\n",
      "        [ 1.0491],\n",
      "        [-1.3844],\n",
      "        [ 0.9527],\n",
      "        [-1.2895],\n",
      "        [ 0.5954],\n",
      "        [-0.1895],\n",
      "        [ 0.7683],\n",
      "        [ 0.5653],\n",
      "        [ 1.1812],\n",
      "        [ 0.7683],\n",
      "        [-0.7844],\n",
      "        [-0.0872],\n",
      "        [-1.1075],\n",
      "        [-1.1615],\n",
      "        [ 0.9527],\n",
      "        [-0.9659],\n",
      "        [ 0.6544],\n",
      "        [ 1.1812],\n",
      "        [-1.5345],\n",
      "        [ 0.1159],\n",
      "        [-0.4268],\n",
      "        [-1.2895],\n",
      "        [ 1.0017],\n",
      "        [-0.4268],\n",
      "        [-0.6252],\n",
      "        [-1.4727],\n",
      "        [ 0.6544],\n",
      "        [-1.3844],\n",
      "        [ 0.4733],\n",
      "        [-0.4603],\n",
      "        [ 0.4733],\n",
      "        [ 1.1812],\n",
      "        [ 1.1447]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 220/10000,\n",
      " train_loss: 0.0017,\n",
      " train_mae: 0.0337,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2018],\n",
      "        [-1.2018],\n",
      "        [ 0.6735],\n",
      "        [ 0.6148],\n",
      "        [ 0.0017],\n",
      "        [ 0.5242],\n",
      "        [ 0.8936],\n",
      "        [ 0.9446],\n",
      "        [ 0.8936],\n",
      "        [ 0.8140],\n",
      "        [-1.5275],\n",
      "        [-0.9800],\n",
      "        [ 0.5849],\n",
      "        [-0.2030],\n",
      "        [ 0.9193],\n",
      "        [-0.2030],\n",
      "        [ 0.5849],\n",
      "        [-0.7039],\n",
      "        [-0.3054],\n",
      "        [ 0.8674],\n",
      "        [ 0.7589],\n",
      "        [ 0.7866],\n",
      "        [ 1.1748],\n",
      "        [ 1.1748],\n",
      "        [ 1.0417],\n",
      "        [-0.3054],\n",
      "        [-1.2277],\n",
      "        [ 1.0417],\n",
      "        [-1.3979],\n",
      "        [ 0.9446],\n",
      "        [-1.3032],\n",
      "        [ 0.5849],\n",
      "        [-0.2030],\n",
      "        [ 0.7589],\n",
      "        [ 0.5547],\n",
      "        [ 1.1748],\n",
      "        [ 0.7589],\n",
      "        [-0.7987],\n",
      "        [-0.1005],\n",
      "        [-1.1215],\n",
      "        [-1.1754],\n",
      "        [ 0.9446],\n",
      "        [-0.9800],\n",
      "        [ 0.6444],\n",
      "        [ 1.1748],\n",
      "        [-1.5476],\n",
      "        [ 0.1033],\n",
      "        [-0.4408],\n",
      "        [-1.3032],\n",
      "        [ 0.9940],\n",
      "        [-0.4408],\n",
      "        [-0.6393],\n",
      "        [-1.4860],\n",
      "        [ 0.6444],\n",
      "        [-1.3979],\n",
      "        [ 0.4622],\n",
      "        [-0.4743],\n",
      "        [ 0.4622],\n",
      "        [ 1.1748],\n",
      "        [ 1.1381]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 221/10000,\n",
      " train_loss: 0.0016,\n",
      " train_mae: 0.0332,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1927],\n",
      "        [-1.1927],\n",
      "        [ 0.6810],\n",
      "        [ 0.6225],\n",
      "        [ 0.0110],\n",
      "        [ 0.5321],\n",
      "        [ 0.9003],\n",
      "        [ 0.9512],\n",
      "        [ 0.9003],\n",
      "        [ 0.8210],\n",
      "        [-1.5198],\n",
      "        [-0.9704],\n",
      "        [ 0.5927],\n",
      "        [-0.1934],\n",
      "        [ 0.9260],\n",
      "        [-0.1934],\n",
      "        [ 0.5927],\n",
      "        [-0.6939],\n",
      "        [-0.2956],\n",
      "        [ 0.8743],\n",
      "        [ 0.7661],\n",
      "        [ 0.7938],\n",
      "        [ 1.1808],\n",
      "        [ 1.1808],\n",
      "        [ 1.0481],\n",
      "        [-0.2956],\n",
      "        [-1.2187],\n",
      "        [ 1.0481],\n",
      "        [-1.3895],\n",
      "        [ 0.9512],\n",
      "        [-1.2944],\n",
      "        [ 0.5927],\n",
      "        [-0.1934],\n",
      "        [ 0.7661],\n",
      "        [ 0.5625],\n",
      "        [ 1.1808],\n",
      "        [ 0.7661],\n",
      "        [-0.7888],\n",
      "        [-0.0910],\n",
      "        [-1.1122],\n",
      "        [-1.1662],\n",
      "        [ 0.9512],\n",
      "        [-0.9704],\n",
      "        [ 0.6519],\n",
      "        [ 1.1808],\n",
      "        [-1.5400],\n",
      "        [ 0.1123],\n",
      "        [-0.4309],\n",
      "        [-1.2944],\n",
      "        [ 1.0005],\n",
      "        [-0.4309],\n",
      "        [-0.6294],\n",
      "        [-1.4780],\n",
      "        [ 0.6519],\n",
      "        [-1.3895],\n",
      "        [ 0.4703],\n",
      "        [-0.4644],\n",
      "        [ 0.4703],\n",
      "        [ 1.1808],\n",
      "        [ 1.1442]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 222/10000,\n",
      " train_loss: 0.0016,\n",
      " train_mae: 0.0326,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1915],\n",
      "        [-1.1915],\n",
      "        [ 0.6792],\n",
      "        [ 0.6207],\n",
      "        [ 0.0102],\n",
      "        [ 0.5304],\n",
      "        [ 0.8984],\n",
      "        [ 0.9493],\n",
      "        [ 0.8984],\n",
      "        [ 0.8191],\n",
      "        [-1.5188],\n",
      "        [-0.9693],\n",
      "        [ 0.5909],\n",
      "        [-0.1937],\n",
      "        [ 0.9241],\n",
      "        [-0.1937],\n",
      "        [ 0.5909],\n",
      "        [-0.6932],\n",
      "        [-0.2957],\n",
      "        [ 0.8724],\n",
      "        [ 0.7642],\n",
      "        [ 0.7918],\n",
      "        [ 1.1792],\n",
      "        [ 1.1792],\n",
      "        [ 1.0462],\n",
      "        [-0.2957],\n",
      "        [-1.2175],\n",
      "        [ 1.0462],\n",
      "        [-1.3884],\n",
      "        [ 0.9493],\n",
      "        [-1.2932],\n",
      "        [ 0.5909],\n",
      "        [-0.1937],\n",
      "        [ 0.7642],\n",
      "        [ 0.5608],\n",
      "        [ 1.1792],\n",
      "        [ 0.7642],\n",
      "        [-0.7879],\n",
      "        [-0.0916],\n",
      "        [-1.1110],\n",
      "        [-1.1650],\n",
      "        [ 0.9493],\n",
      "        [-0.9693],\n",
      "        [ 0.6501],\n",
      "        [ 1.1792],\n",
      "        [-1.5391],\n",
      "        [ 0.1113],\n",
      "        [-0.4307],\n",
      "        [-1.2932],\n",
      "        [ 0.9986],\n",
      "        [-0.4307],\n",
      "        [-0.6288],\n",
      "        [-1.4770],\n",
      "        [ 0.6501],\n",
      "        [-1.3884],\n",
      "        [ 0.4686],\n",
      "        [-0.4641],\n",
      "        [ 0.4686],\n",
      "        [ 1.1792],\n",
      "        [ 1.1425]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 223/10000,\n",
      " train_loss: 0.0016,\n",
      " train_mae: 0.0331,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2005],\n",
      "        [-1.2005],\n",
      "        [ 0.6740],\n",
      "        [ 0.6152],\n",
      "        [ 0.0027],\n",
      "        [ 0.5246],\n",
      "        [ 0.8943],\n",
      "        [ 0.9454],\n",
      "        [ 0.8943],\n",
      "        [ 0.8145],\n",
      "        [-1.5276],\n",
      "        [-0.9782],\n",
      "        [ 0.5853],\n",
      "        [-0.2018],\n",
      "        [ 0.9200],\n",
      "        [-0.2018],\n",
      "        [ 0.5853],\n",
      "        [-0.7020],\n",
      "        [-0.3039],\n",
      "        [ 0.8681],\n",
      "        [ 0.7594],\n",
      "        [ 0.7872],\n",
      "        [ 1.1765],\n",
      "        [ 1.1765],\n",
      "        [ 1.0428],\n",
      "        [-0.3039],\n",
      "        [-1.2265],\n",
      "        [ 1.0428],\n",
      "        [-1.3973],\n",
      "        [ 0.9454],\n",
      "        [-1.3022],\n",
      "        [ 0.5853],\n",
      "        [-0.2018],\n",
      "        [ 0.7594],\n",
      "        [ 0.5551],\n",
      "        [ 1.1765],\n",
      "        [ 0.7594],\n",
      "        [-0.7968],\n",
      "        [-0.0994],\n",
      "        [-1.1200],\n",
      "        [-1.1740],\n",
      "        [ 0.9454],\n",
      "        [-0.9782],\n",
      "        [ 0.6448],\n",
      "        [ 1.1765],\n",
      "        [-1.5479],\n",
      "        [ 0.1041],\n",
      "        [-0.4391],\n",
      "        [-1.3022],\n",
      "        [ 0.9949],\n",
      "        [-0.4391],\n",
      "        [-0.6375],\n",
      "        [-1.4858],\n",
      "        [ 0.6448],\n",
      "        [-1.3973],\n",
      "        [ 0.4626],\n",
      "        [-0.4726],\n",
      "        [ 0.4626],\n",
      "        [ 1.1765],\n",
      "        [ 1.1396]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 224/10000,\n",
      " train_loss: 0.0016,\n",
      " train_mae: 0.0330,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1902],\n",
      "        [-1.1902],\n",
      "        [ 0.6831],\n",
      "        [ 0.6245],\n",
      "        [ 0.0136],\n",
      "        [ 0.5342],\n",
      "        [ 0.9026],\n",
      "        [ 0.9536],\n",
      "        [ 0.9026],\n",
      "        [ 0.8232],\n",
      "        [-1.5189],\n",
      "        [-0.9674],\n",
      "        [ 0.5947],\n",
      "        [-0.1905],\n",
      "        [ 0.9283],\n",
      "        [-0.1905],\n",
      "        [ 0.5947],\n",
      "        [-0.6907],\n",
      "        [-0.2926],\n",
      "        [ 0.8765],\n",
      "        [ 0.7683],\n",
      "        [ 0.7959],\n",
      "        [ 1.1839],\n",
      "        [ 1.1839],\n",
      "        [ 1.0507],\n",
      "        [-0.2926],\n",
      "        [-1.2163],\n",
      "        [ 1.0507],\n",
      "        [-1.3879],\n",
      "        [ 0.9536],\n",
      "        [-1.2923],\n",
      "        [ 0.5947],\n",
      "        [-0.1905],\n",
      "        [ 0.7683],\n",
      "        [ 0.5646],\n",
      "        [ 1.1839],\n",
      "        [ 0.7683],\n",
      "        [-0.7856],\n",
      "        [-0.0883],\n",
      "        [-1.1095],\n",
      "        [-1.1637],\n",
      "        [ 0.9536],\n",
      "        [-0.9674],\n",
      "        [ 0.6540],\n",
      "        [ 1.1839],\n",
      "        [-1.5393],\n",
      "        [ 0.1148],\n",
      "        [-0.4277],\n",
      "        [-1.2923],\n",
      "        [ 1.0030],\n",
      "        [-0.4277],\n",
      "        [-0.6261],\n",
      "        [-1.4769],\n",
      "        [ 0.6540],\n",
      "        [-1.3879],\n",
      "        [ 0.4724],\n",
      "        [-0.4612],\n",
      "        [ 0.4724],\n",
      "        [ 1.1839],\n",
      "        [ 1.1471]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 225/10000,\n",
      " train_loss: 0.0016,\n",
      " train_mae: 0.0322,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1957],\n",
      "        [-1.1957],\n",
      "        [ 0.6742],\n",
      "        [ 0.6155],\n",
      "        [ 0.0049],\n",
      "        [ 0.5251],\n",
      "        [ 0.8942],\n",
      "        [ 0.9453],\n",
      "        [ 0.8942],\n",
      "        [ 0.8145],\n",
      "        [-1.5238],\n",
      "        [-0.9735],\n",
      "        [ 0.5857],\n",
      "        [-0.1988],\n",
      "        [ 0.9200],\n",
      "        [-0.1988],\n",
      "        [ 0.5857],\n",
      "        [-0.6976],\n",
      "        [-0.3006],\n",
      "        [ 0.8680],\n",
      "        [ 0.7595],\n",
      "        [ 0.7872],\n",
      "        [ 1.1766],\n",
      "        [ 1.1766],\n",
      "        [ 1.0428],\n",
      "        [-0.3006],\n",
      "        [-1.2218],\n",
      "        [ 1.0428],\n",
      "        [-1.3930],\n",
      "        [ 0.9453],\n",
      "        [-1.2976],\n",
      "        [ 0.5857],\n",
      "        [-0.1988],\n",
      "        [ 0.7595],\n",
      "        [ 0.5556],\n",
      "        [ 1.1766],\n",
      "        [ 0.7595],\n",
      "        [-0.7922],\n",
      "        [-0.0968],\n",
      "        [-1.1152],\n",
      "        [-1.1693],\n",
      "        [ 0.9453],\n",
      "        [-0.9735],\n",
      "        [ 0.6450],\n",
      "        [ 1.1766],\n",
      "        [-1.5442],\n",
      "        [ 0.1059],\n",
      "        [-0.4354],\n",
      "        [-1.2976],\n",
      "        [ 0.9949],\n",
      "        [-0.4354],\n",
      "        [-0.6333],\n",
      "        [-1.4818],\n",
      "        [ 0.6450],\n",
      "        [-1.3930],\n",
      "        [ 0.4633],\n",
      "        [-0.4688],\n",
      "        [ 0.4633],\n",
      "        [ 1.1766],\n",
      "        [ 1.1397]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 226/10000,\n",
      " train_loss: 0.0015,\n",
      " train_mae: 0.0326,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1963],\n",
      "        [-1.1963],\n",
      "        [ 0.6778],\n",
      "        [ 0.6191],\n",
      "        [ 0.0073],\n",
      "        [ 0.5285],\n",
      "        [ 0.8981],\n",
      "        [ 0.9494],\n",
      "        [ 0.8981],\n",
      "        [ 0.8184],\n",
      "        [-1.5253],\n",
      "        [-0.9735],\n",
      "        [ 0.5892],\n",
      "        [-0.1969],\n",
      "        [ 0.9240],\n",
      "        [-0.1969],\n",
      "        [ 0.5892],\n",
      "        [-0.6969],\n",
      "        [-0.2990],\n",
      "        [ 0.8719],\n",
      "        [ 0.7632],\n",
      "        [ 0.7910],\n",
      "        [ 1.1810],\n",
      "        [ 1.1810],\n",
      "        [ 1.0469],\n",
      "        [-0.2990],\n",
      "        [-1.2225],\n",
      "        [ 1.0469],\n",
      "        [-1.3941],\n",
      "        [ 0.9494],\n",
      "        [-1.2985],\n",
      "        [ 0.5892],\n",
      "        [-0.1969],\n",
      "        [ 0.7632],\n",
      "        [ 0.5590],\n",
      "        [ 1.1810],\n",
      "        [ 0.7632],\n",
      "        [-0.7917],\n",
      "        [-0.0947],\n",
      "        [-1.1156],\n",
      "        [-1.1698],\n",
      "        [ 0.9494],\n",
      "        [-0.9735],\n",
      "        [ 0.6486],\n",
      "        [ 1.1810],\n",
      "        [-1.5457],\n",
      "        [ 0.1085],\n",
      "        [-0.4341],\n",
      "        [-1.2985],\n",
      "        [ 0.9990],\n",
      "        [-0.4341],\n",
      "        [-0.6324],\n",
      "        [-1.4832],\n",
      "        [ 0.6486],\n",
      "        [-1.3941],\n",
      "        [ 0.4665],\n",
      "        [-0.4675],\n",
      "        [ 0.4665],\n",
      "        [ 1.1810],\n",
      "        [ 1.1439]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 227/10000,\n",
      " train_loss: 0.0015,\n",
      " train_mae: 0.0324,\n",
      " epoch_time_duration: 0.0086\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1914],\n",
      "        [-1.1914],\n",
      "        [ 0.6807],\n",
      "        [ 0.6221],\n",
      "        [ 0.0115],\n",
      "        [ 0.5317],\n",
      "        [ 0.9008],\n",
      "        [ 0.9519],\n",
      "        [ 0.9008],\n",
      "        [ 0.8211],\n",
      "        [-1.5211],\n",
      "        [-0.9683],\n",
      "        [ 0.5923],\n",
      "        [-0.1923],\n",
      "        [ 0.9265],\n",
      "        [-0.1923],\n",
      "        [ 0.5923],\n",
      "        [-0.6918],\n",
      "        [-0.2943],\n",
      "        [ 0.8746],\n",
      "        [ 0.7661],\n",
      "        [ 0.7938],\n",
      "        [ 1.1833],\n",
      "        [ 1.1833],\n",
      "        [ 1.0494],\n",
      "        [-0.2943],\n",
      "        [-1.2176],\n",
      "        [ 1.0494],\n",
      "        [-1.3896],\n",
      "        [ 0.9519],\n",
      "        [-1.2937],\n",
      "        [ 0.5923],\n",
      "        [-0.1923],\n",
      "        [ 0.7661],\n",
      "        [ 0.5622],\n",
      "        [ 1.1833],\n",
      "        [ 0.7661],\n",
      "        [-0.7866],\n",
      "        [-0.0903],\n",
      "        [-1.1105],\n",
      "        [-1.1648],\n",
      "        [ 0.9519],\n",
      "        [-0.9683],\n",
      "        [ 0.6516],\n",
      "        [ 1.1833],\n",
      "        [-1.5416],\n",
      "        [ 0.1125],\n",
      "        [-0.4292],\n",
      "        [-1.2937],\n",
      "        [ 1.0014],\n",
      "        [-0.4292],\n",
      "        [-0.6273],\n",
      "        [-1.4789],\n",
      "        [ 0.6516],\n",
      "        [-1.3896],\n",
      "        [ 0.4699],\n",
      "        [-0.4626],\n",
      "        [ 0.4699],\n",
      "        [ 1.1833],\n",
      "        [ 1.1463]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 228/10000,\n",
      " train_loss: 0.0015,\n",
      " train_mae: 0.0320,\n",
      " epoch_time_duration: 0.0084\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1973],\n",
      "        [-1.1973],\n",
      "        [ 0.6723],\n",
      "        [ 0.6136],\n",
      "        [ 0.0030],\n",
      "        [ 0.5231],\n",
      "        [ 0.8928],\n",
      "        [ 0.9442],\n",
      "        [ 0.8928],\n",
      "        [ 0.8129],\n",
      "        [-1.5264],\n",
      "        [-0.9748],\n",
      "        [ 0.5837],\n",
      "        [-0.2006],\n",
      "        [ 0.9187],\n",
      "        [-0.2006],\n",
      "        [ 0.5837],\n",
      "        [-0.6989],\n",
      "        [-0.3023],\n",
      "        [ 0.8666],\n",
      "        [ 0.7578],\n",
      "        [ 0.7855],\n",
      "        [ 1.1766],\n",
      "        [ 1.1766],\n",
      "        [ 1.0420],\n",
      "        [-0.3023],\n",
      "        [-1.2234],\n",
      "        [ 1.0420],\n",
      "        [-1.3951],\n",
      "        [ 0.9442],\n",
      "        [-1.2994],\n",
      "        [ 0.5837],\n",
      "        [-0.2006],\n",
      "        [ 0.7578],\n",
      "        [ 0.5535],\n",
      "        [ 1.1766],\n",
      "        [ 0.7578],\n",
      "        [-0.7935],\n",
      "        [-0.0987],\n",
      "        [-1.1166],\n",
      "        [-1.1708],\n",
      "        [ 0.9442],\n",
      "        [-0.9748],\n",
      "        [ 0.6431],\n",
      "        [ 1.1766],\n",
      "        [-1.5469],\n",
      "        [ 0.1039],\n",
      "        [-0.4369],\n",
      "        [-1.2994],\n",
      "        [ 0.9939],\n",
      "        [-0.4369],\n",
      "        [-0.6346],\n",
      "        [-1.4843],\n",
      "        [ 0.6431],\n",
      "        [-1.3951],\n",
      "        [ 0.4612],\n",
      "        [-0.4703],\n",
      "        [ 0.4612],\n",
      "        [ 1.1766],\n",
      "        [ 1.1394]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 229/10000,\n",
      " train_loss: 0.0015,\n",
      " train_mae: 0.0324,\n",
      " epoch_time_duration: 0.0092\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1924],\n",
      "        [-1.1924],\n",
      "        [ 0.6816],\n",
      "        [ 0.6229],\n",
      "        [ 0.0117],\n",
      "        [ 0.5324],\n",
      "        [ 0.9021],\n",
      "        [ 0.9533],\n",
      "        [ 0.9021],\n",
      "        [ 0.8222],\n",
      "        [-1.5231],\n",
      "        [-0.9689],\n",
      "        [ 0.5930],\n",
      "        [-0.1922],\n",
      "        [ 0.9279],\n",
      "        [-0.1922],\n",
      "        [ 0.5930],\n",
      "        [-0.6920],\n",
      "        [-0.2942],\n",
      "        [ 0.8758],\n",
      "        [ 0.7671],\n",
      "        [ 0.7948],\n",
      "        [ 1.1855],\n",
      "        [ 1.1855],\n",
      "        [ 1.0511],\n",
      "        [-0.2942],\n",
      "        [-1.2186],\n",
      "        [ 1.0511],\n",
      "        [-1.3911],\n",
      "        [ 0.9533],\n",
      "        [-1.2950],\n",
      "        [ 0.5930],\n",
      "        [-0.1922],\n",
      "        [ 0.7671],\n",
      "        [ 0.5629],\n",
      "        [ 1.1855],\n",
      "        [ 0.7671],\n",
      "        [-0.7869],\n",
      "        [-0.0901],\n",
      "        [-1.1114],\n",
      "        [-1.1658],\n",
      "        [ 0.9533],\n",
      "        [-0.9689],\n",
      "        [ 0.6524],\n",
      "        [ 1.1855],\n",
      "        [-1.5437],\n",
      "        [ 0.1128],\n",
      "        [-0.4292],\n",
      "        [-1.2950],\n",
      "        [ 1.0030],\n",
      "        [-0.4292],\n",
      "        [-0.6275],\n",
      "        [-1.4807],\n",
      "        [ 0.6524],\n",
      "        [-1.3911],\n",
      "        [ 0.4705],\n",
      "        [-0.4627],\n",
      "        [ 0.4705],\n",
      "        [ 1.1855],\n",
      "        [ 1.1483]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 230/10000,\n",
      " train_loss: 0.0015,\n",
      " train_mae: 0.0316,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1952],\n",
      "        [-1.1952],\n",
      "        [ 0.6751],\n",
      "        [ 0.6164],\n",
      "        [ 0.0059],\n",
      "        [ 0.5259],\n",
      "        [ 0.8958],\n",
      "        [ 0.9471],\n",
      "        [ 0.8958],\n",
      "        [ 0.8158],\n",
      "        [-1.5255],\n",
      "        [-0.9722],\n",
      "        [ 0.5865],\n",
      "        [-0.1976],\n",
      "        [ 0.9217],\n",
      "        [-0.1976],\n",
      "        [ 0.5865],\n",
      "        [-0.6960],\n",
      "        [-0.2993],\n",
      "        [ 0.8695],\n",
      "        [ 0.7606],\n",
      "        [ 0.7884],\n",
      "        [ 1.1799],\n",
      "        [ 1.1799],\n",
      "        [ 1.0451],\n",
      "        [-0.2993],\n",
      "        [-1.2214],\n",
      "        [ 1.0451],\n",
      "        [-1.3936],\n",
      "        [ 0.9471],\n",
      "        [-1.2976],\n",
      "        [ 0.5865],\n",
      "        [-0.1976],\n",
      "        [ 0.7606],\n",
      "        [ 0.5563],\n",
      "        [ 1.1799],\n",
      "        [ 0.7606],\n",
      "        [-0.7907],\n",
      "        [-0.0957],\n",
      "        [-1.1143],\n",
      "        [-1.1686],\n",
      "        [ 0.9471],\n",
      "        [-0.9722],\n",
      "        [ 0.6459],\n",
      "        [ 1.1799],\n",
      "        [-1.5460],\n",
      "        [ 0.1068],\n",
      "        [-0.4339],\n",
      "        [-1.2976],\n",
      "        [ 0.9969],\n",
      "        [-0.4339],\n",
      "        [-0.6317],\n",
      "        [-1.4831],\n",
      "        [ 0.6459],\n",
      "        [-1.3936],\n",
      "        [ 0.4640],\n",
      "        [-0.4673],\n",
      "        [ 0.4640],\n",
      "        [ 1.1799],\n",
      "        [ 1.1426]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 231/10000,\n",
      " train_loss: 0.0015,\n",
      " train_mae: 0.0315,\n",
      " epoch_time_duration: 0.0083\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1944],\n",
      "        [-1.1944],\n",
      "        [ 0.6758],\n",
      "        [ 0.6171],\n",
      "        [ 0.0067],\n",
      "        [ 0.5265],\n",
      "        [ 0.8965],\n",
      "        [ 0.9479],\n",
      "        [ 0.8965],\n",
      "        [ 0.8165],\n",
      "        [-1.5252],\n",
      "        [-0.9713],\n",
      "        [ 0.5872],\n",
      "        [-0.1967],\n",
      "        [ 0.9224],\n",
      "        [-0.1967],\n",
      "        [ 0.5872],\n",
      "        [-0.6950],\n",
      "        [-0.2984],\n",
      "        [ 0.8702],\n",
      "        [ 0.7613],\n",
      "        [ 0.7891],\n",
      "        [ 1.1809],\n",
      "        [ 1.1809],\n",
      "        [ 1.0460],\n",
      "        [-0.2984],\n",
      "        [-1.2206],\n",
      "        [ 1.0460],\n",
      "        [-1.3931],\n",
      "        [ 0.9479],\n",
      "        [-1.2969],\n",
      "        [ 0.5872],\n",
      "        [-0.1967],\n",
      "        [ 0.7613],\n",
      "        [ 0.5570],\n",
      "        [ 1.1809],\n",
      "        [ 0.7613],\n",
      "        [-0.7897],\n",
      "        [-0.0948],\n",
      "        [-1.1135],\n",
      "        [-1.1678],\n",
      "        [ 0.9479],\n",
      "        [-0.9713],\n",
      "        [ 0.6466],\n",
      "        [ 1.1809],\n",
      "        [-1.5458],\n",
      "        [ 0.1076],\n",
      "        [-0.4329],\n",
      "        [-1.2969],\n",
      "        [ 0.9977],\n",
      "        [-0.4329],\n",
      "        [-0.6307],\n",
      "        [-1.4828],\n",
      "        [ 0.6466],\n",
      "        [-1.3931],\n",
      "        [ 0.4647],\n",
      "        [-0.4663],\n",
      "        [ 0.4647],\n",
      "        [ 1.1809],\n",
      "        [ 1.1436]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 232/10000,\n",
      " train_loss: 0.0015,\n",
      " train_mae: 0.0319,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1929],\n",
      "        [-1.1929],\n",
      "        [ 0.6802],\n",
      "        [ 0.6214],\n",
      "        [ 0.0105],\n",
      "        [ 0.5308],\n",
      "        [ 0.9010],\n",
      "        [ 0.9525],\n",
      "        [ 0.9010],\n",
      "        [ 0.8210],\n",
      "        [-1.5246],\n",
      "        [-0.9692],\n",
      "        [ 0.5915],\n",
      "        [-0.1932],\n",
      "        [ 0.9269],\n",
      "        [-0.1932],\n",
      "        [ 0.5915],\n",
      "        [-0.6923],\n",
      "        [-0.2950],\n",
      "        [ 0.8747],\n",
      "        [ 0.7658],\n",
      "        [ 0.7936],\n",
      "        [ 1.1855],\n",
      "        [ 1.1855],\n",
      "        [ 1.0505],\n",
      "        [-0.2950],\n",
      "        [-1.2192],\n",
      "        [ 1.0505],\n",
      "        [-1.3921],\n",
      "        [ 0.9525],\n",
      "        [-1.2957],\n",
      "        [ 0.5915],\n",
      "        [-0.1932],\n",
      "        [ 0.7658],\n",
      "        [ 0.5613],\n",
      "        [ 1.1855],\n",
      "        [ 0.7658],\n",
      "        [-0.7872],\n",
      "        [-0.0912],\n",
      "        [-1.1117],\n",
      "        [-1.1662],\n",
      "        [ 0.9525],\n",
      "        [-0.9692],\n",
      "        [ 0.6510],\n",
      "        [ 1.1855],\n",
      "        [-1.5453],\n",
      "        [ 0.1115],\n",
      "        [-0.4298],\n",
      "        [-1.2957],\n",
      "        [ 1.0023],\n",
      "        [-0.4298],\n",
      "        [-0.6279],\n",
      "        [-1.4821],\n",
      "        [ 0.6510],\n",
      "        [-1.3921],\n",
      "        [ 0.4689],\n",
      "        [-0.4632],\n",
      "        [ 0.4689],\n",
      "        [ 1.1855],\n",
      "        [ 1.1482]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 233/10000,\n",
      " train_loss: 0.0015,\n",
      " train_mae: 0.0313,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1964],\n",
      "        [-1.1964],\n",
      "        [ 0.6725],\n",
      "        [ 0.6137],\n",
      "        [ 0.0035],\n",
      "        [ 0.5231],\n",
      "        [ 0.8936],\n",
      "        [ 0.9451],\n",
      "        [ 0.8936],\n",
      "        [ 0.8134],\n",
      "        [-1.5276],\n",
      "        [-0.9733],\n",
      "        [ 0.5838],\n",
      "        [-0.1997],\n",
      "        [ 0.9195],\n",
      "        [-0.1997],\n",
      "        [ 0.5838],\n",
      "        [-0.6973],\n",
      "        [-0.3012],\n",
      "        [ 0.8672],\n",
      "        [ 0.7581],\n",
      "        [ 0.7859],\n",
      "        [ 1.1789],\n",
      "        [ 1.1789],\n",
      "        [ 1.0434],\n",
      "        [-0.3012],\n",
      "        [-1.2227],\n",
      "        [ 1.0434],\n",
      "        [-1.3952],\n",
      "        [ 0.9451],\n",
      "        [-1.2990],\n",
      "        [ 0.5838],\n",
      "        [-0.1997],\n",
      "        [ 0.7581],\n",
      "        [ 0.5536],\n",
      "        [ 1.1789],\n",
      "        [ 0.7581],\n",
      "        [-0.7919],\n",
      "        [-0.0980],\n",
      "        [-1.1155],\n",
      "        [-1.1698],\n",
      "        [ 0.9451],\n",
      "        [-0.9733],\n",
      "        [ 0.6432],\n",
      "        [ 1.1789],\n",
      "        [-1.5482],\n",
      "        [ 0.1042],\n",
      "        [-0.4356],\n",
      "        [-1.2990],\n",
      "        [ 0.9950],\n",
      "        [-0.4356],\n",
      "        [-0.6331],\n",
      "        [-1.4851],\n",
      "        [ 0.6432],\n",
      "        [-1.3952],\n",
      "        [ 0.4612],\n",
      "        [-0.4689],\n",
      "        [ 0.4612],\n",
      "        [ 1.1789],\n",
      "        [ 1.1414]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 234/10000,\n",
      " train_loss: 0.0015,\n",
      " train_mae: 0.0315,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1912],\n",
      "        [-1.1912],\n",
      "        [ 0.6801],\n",
      "        [ 0.6213],\n",
      "        [ 0.0112],\n",
      "        [ 0.5308],\n",
      "        [ 0.9010],\n",
      "        [ 0.9525],\n",
      "        [ 0.9010],\n",
      "        [ 0.8209],\n",
      "        [-1.5236],\n",
      "        [-0.9674],\n",
      "        [ 0.5915],\n",
      "        [-0.1922],\n",
      "        [ 0.9269],\n",
      "        [-0.1922],\n",
      "        [ 0.5915],\n",
      "        [-0.6906],\n",
      "        [-0.2938],\n",
      "        [ 0.8747],\n",
      "        [ 0.7657],\n",
      "        [ 0.7935],\n",
      "        [ 1.1860],\n",
      "        [ 1.1860],\n",
      "        [ 1.0507],\n",
      "        [-0.2938],\n",
      "        [-1.2175],\n",
      "        [ 1.0507],\n",
      "        [-1.3907],\n",
      "        [ 0.9525],\n",
      "        [-1.2941],\n",
      "        [ 0.5915],\n",
      "        [-0.1922],\n",
      "        [ 0.7657],\n",
      "        [ 0.5613],\n",
      "        [ 1.1860],\n",
      "        [ 0.7657],\n",
      "        [-0.7854],\n",
      "        [-0.0904],\n",
      "        [-1.1100],\n",
      "        [-1.1645],\n",
      "        [ 0.9525],\n",
      "        [-0.9674],\n",
      "        [ 0.6509],\n",
      "        [ 1.1860],\n",
      "        [-1.5444],\n",
      "        [ 0.1120],\n",
      "        [-0.4284],\n",
      "        [-1.2941],\n",
      "        [ 1.0024],\n",
      "        [-0.4284],\n",
      "        [-0.6262],\n",
      "        [-1.4809],\n",
      "        [ 0.6509],\n",
      "        [-1.3907],\n",
      "        [ 0.4690],\n",
      "        [-0.4618],\n",
      "        [ 0.4690],\n",
      "        [ 1.1860],\n",
      "        [ 1.1485]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 235/10000,\n",
      " train_loss: 0.0014,\n",
      " train_mae: 0.0312,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1966],\n",
      "        [-1.1966],\n",
      "        [ 0.6744],\n",
      "        [ 0.6155],\n",
      "        [ 0.0048],\n",
      "        [ 0.5248],\n",
      "        [ 0.8958],\n",
      "        [ 0.9474],\n",
      "        [ 0.8958],\n",
      "        [ 0.8155],\n",
      "        [-1.5287],\n",
      "        [-0.9730],\n",
      "        [ 0.5856],\n",
      "        [-0.1985],\n",
      "        [ 0.9218],\n",
      "        [-0.1985],\n",
      "        [ 0.5856],\n",
      "        [-0.6966],\n",
      "        [-0.3001],\n",
      "        [ 0.8694],\n",
      "        [ 0.7601],\n",
      "        [ 0.7880],\n",
      "        [ 1.1818],\n",
      "        [ 1.1818],\n",
      "        [ 1.0460],\n",
      "        [-0.3001],\n",
      "        [-1.2229],\n",
      "        [ 1.0460],\n",
      "        [-1.3959],\n",
      "        [ 0.9474],\n",
      "        [-1.2994],\n",
      "        [ 0.5856],\n",
      "        [-0.1985],\n",
      "        [ 0.7601],\n",
      "        [ 0.5553],\n",
      "        [ 1.1818],\n",
      "        [ 0.7601],\n",
      "        [-0.7913],\n",
      "        [-0.0967],\n",
      "        [-1.1155],\n",
      "        [-1.1699],\n",
      "        [ 0.9474],\n",
      "        [-0.9730],\n",
      "        [ 0.6451],\n",
      "        [ 1.1818],\n",
      "        [-1.5495],\n",
      "        [ 0.1056],\n",
      "        [-0.4347],\n",
      "        [-1.2994],\n",
      "        [ 0.9975],\n",
      "        [-0.4347],\n",
      "        [-0.6323],\n",
      "        [-1.4861],\n",
      "        [ 0.6451],\n",
      "        [-1.3959],\n",
      "        [ 0.4629],\n",
      "        [-0.4680],\n",
      "        [ 0.4629],\n",
      "        [ 1.1818],\n",
      "        [ 1.1442]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 236/10000,\n",
      " train_loss: 0.0014,\n",
      " train_mae: 0.0309,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1925],\n",
      "        [-1.1925],\n",
      "        [ 0.6765],\n",
      "        [ 0.6177],\n",
      "        [ 0.0080],\n",
      "        [ 0.5272],\n",
      "        [ 0.8977],\n",
      "        [ 0.9493],\n",
      "        [ 0.8977],\n",
      "        [ 0.8175],\n",
      "        [-1.5252],\n",
      "        [-0.9688],\n",
      "        [ 0.5878],\n",
      "        [-0.1949],\n",
      "        [ 0.9237],\n",
      "        [-0.1949],\n",
      "        [ 0.5878],\n",
      "        [-0.6925],\n",
      "        [-0.2964],\n",
      "        [ 0.8713],\n",
      "        [ 0.7622],\n",
      "        [ 0.7900],\n",
      "        [ 1.1835],\n",
      "        [ 1.1835],\n",
      "        [ 1.0477],\n",
      "        [-0.2964],\n",
      "        [-1.2188],\n",
      "        [ 1.0477],\n",
      "        [-1.3921],\n",
      "        [ 0.9493],\n",
      "        [-1.2955],\n",
      "        [ 0.5878],\n",
      "        [-0.1949],\n",
      "        [ 0.7622],\n",
      "        [ 0.5577],\n",
      "        [ 1.1835],\n",
      "        [ 0.7622],\n",
      "        [-0.7871],\n",
      "        [-0.0933],\n",
      "        [-1.1113],\n",
      "        [-1.1658],\n",
      "        [ 0.9493],\n",
      "        [-0.9688],\n",
      "        [ 0.6473],\n",
      "        [ 1.1835],\n",
      "        [-1.5460],\n",
      "        [ 0.1087],\n",
      "        [-0.4307],\n",
      "        [-1.2955],\n",
      "        [ 0.9993],\n",
      "        [-0.4307],\n",
      "        [-0.6282],\n",
      "        [-1.4825],\n",
      "        [ 0.6473],\n",
      "        [-1.3921],\n",
      "        [ 0.4654],\n",
      "        [-0.4640],\n",
      "        [ 0.4654],\n",
      "        [ 1.1835],\n",
      "        [ 1.1459]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 237/10000,\n",
      " train_loss: 0.0014,\n",
      " train_mae: 0.0311,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1936],\n",
      "        [-1.1936],\n",
      "        [ 0.6774],\n",
      "        [ 0.6185],\n",
      "        [ 0.0082],\n",
      "        [ 0.5279],\n",
      "        [ 0.8989],\n",
      "        [ 0.9505],\n",
      "        [ 0.8989],\n",
      "        [ 0.8185],\n",
      "        [-1.5268],\n",
      "        [-0.9696],\n",
      "        [ 0.5886],\n",
      "        [-0.1950],\n",
      "        [ 0.9249],\n",
      "        [-0.1950],\n",
      "        [ 0.5886],\n",
      "        [-0.6930],\n",
      "        [-0.2965],\n",
      "        [ 0.8725],\n",
      "        [ 0.7631],\n",
      "        [ 0.7910],\n",
      "        [ 1.1851],\n",
      "        [ 1.1851],\n",
      "        [ 1.0491],\n",
      "        [-0.2965],\n",
      "        [-1.2199],\n",
      "        [ 1.0491],\n",
      "        [-1.3935],\n",
      "        [ 0.9505],\n",
      "        [-1.2967],\n",
      "        [ 0.5886],\n",
      "        [-0.1950],\n",
      "        [ 0.7631],\n",
      "        [ 0.5584],\n",
      "        [ 1.1851],\n",
      "        [ 0.7631],\n",
      "        [-0.7877],\n",
      "        [-0.0932],\n",
      "        [-1.1123],\n",
      "        [-1.1668],\n",
      "        [ 0.9505],\n",
      "        [-0.9696],\n",
      "        [ 0.6481],\n",
      "        [ 1.1851],\n",
      "        [-1.5476],\n",
      "        [ 0.1090],\n",
      "        [-0.4310],\n",
      "        [-1.2967],\n",
      "        [ 1.0006],\n",
      "        [-0.4310],\n",
      "        [-0.6286],\n",
      "        [-1.4840],\n",
      "        [ 0.6481],\n",
      "        [-1.3935],\n",
      "        [ 0.4660],\n",
      "        [-0.4643],\n",
      "        [ 0.4660],\n",
      "        [ 1.1851],\n",
      "        [ 1.1475]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 238/10000,\n",
      " train_loss: 0.0014,\n",
      " train_mae: 0.0308,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1958],\n",
      "        [-1.1958],\n",
      "        [ 0.6737],\n",
      "        [ 0.6148],\n",
      "        [ 0.0047],\n",
      "        [ 0.5241],\n",
      "        [ 0.8954],\n",
      "        [ 0.9472],\n",
      "        [ 0.8954],\n",
      "        [ 0.8150],\n",
      "        [-1.5289],\n",
      "        [-0.9720],\n",
      "        [ 0.5849],\n",
      "        [-0.1983],\n",
      "        [ 0.9215],\n",
      "        [-0.1983],\n",
      "        [ 0.5849],\n",
      "        [-0.6957],\n",
      "        [-0.2998],\n",
      "        [ 0.8690],\n",
      "        [ 0.7595],\n",
      "        [ 0.7875],\n",
      "        [ 1.1823],\n",
      "        [ 1.1823],\n",
      "        [ 1.0460],\n",
      "        [-0.2998],\n",
      "        [-1.2221],\n",
      "        [ 1.0460],\n",
      "        [-1.3956],\n",
      "        [ 0.9472],\n",
      "        [-1.2988],\n",
      "        [ 0.5849],\n",
      "        [-0.1983],\n",
      "        [ 0.7595],\n",
      "        [ 0.5547],\n",
      "        [ 1.1823],\n",
      "        [ 0.7595],\n",
      "        [-0.7904],\n",
      "        [-0.0967],\n",
      "        [-1.1146],\n",
      "        [-1.1691],\n",
      "        [ 0.9472],\n",
      "        [-0.9720],\n",
      "        [ 0.6444],\n",
      "        [ 1.1823],\n",
      "        [-1.5498],\n",
      "        [ 0.1053],\n",
      "        [-0.4341],\n",
      "        [-1.2988],\n",
      "        [ 0.9974],\n",
      "        [-0.4341],\n",
      "        [-0.6315],\n",
      "        [-1.4861],\n",
      "        [ 0.6444],\n",
      "        [-1.3956],\n",
      "        [ 0.4622],\n",
      "        [-0.4674],\n",
      "        [ 0.4622],\n",
      "        [ 1.1823],\n",
      "        [ 1.1446]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 239/10000,\n",
      " train_loss: 0.0014,\n",
      " train_mae: 0.0308,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1907],\n",
      "        [-1.1907],\n",
      "        [ 0.6787],\n",
      "        [ 0.6199],\n",
      "        [ 0.0104],\n",
      "        [ 0.5293],\n",
      "        [ 0.9001],\n",
      "        [ 0.9518],\n",
      "        [ 0.9001],\n",
      "        [ 0.8198],\n",
      "        [-1.5247],\n",
      "        [-0.9665],\n",
      "        [ 0.5900],\n",
      "        [-0.1925],\n",
      "        [ 0.9262],\n",
      "        [-0.1925],\n",
      "        [ 0.5900],\n",
      "        [-0.6899],\n",
      "        [-0.2939],\n",
      "        [ 0.8737],\n",
      "        [ 0.7644],\n",
      "        [ 0.7923],\n",
      "        [ 1.1867],\n",
      "        [ 1.1867],\n",
      "        [ 1.0505],\n",
      "        [-0.2939],\n",
      "        [-1.2171],\n",
      "        [ 1.0505],\n",
      "        [-1.3910],\n",
      "        [ 0.9518],\n",
      "        [-1.2939],\n",
      "        [ 0.5900],\n",
      "        [-0.1925],\n",
      "        [ 0.7644],\n",
      "        [ 0.5598],\n",
      "        [ 1.1867],\n",
      "        [ 0.7644],\n",
      "        [-0.7846],\n",
      "        [-0.0909],\n",
      "        [-1.1093],\n",
      "        [-1.1639],\n",
      "        [ 0.9518],\n",
      "        [-0.9665],\n",
      "        [ 0.6495],\n",
      "        [ 1.1867],\n",
      "        [-1.5456],\n",
      "        [ 0.1110],\n",
      "        [-0.4282],\n",
      "        [-1.2939],\n",
      "        [ 1.0019],\n",
      "        [-0.4282],\n",
      "        [-0.6256],\n",
      "        [-1.4817],\n",
      "        [ 0.6495],\n",
      "        [-1.3910],\n",
      "        [ 0.4675],\n",
      "        [-0.4615],\n",
      "        [ 0.4675],\n",
      "        [ 1.1867],\n",
      "        [ 1.1490]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 240/10000,\n",
      " train_loss: 0.0014,\n",
      " train_mae: 0.0307,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1970],\n",
      "        [-1.1970],\n",
      "        [ 0.6728],\n",
      "        [ 0.6138],\n",
      "        [ 0.0035],\n",
      "        [ 0.5231],\n",
      "        [ 0.8948],\n",
      "        [ 0.9467],\n",
      "        [ 0.8948],\n",
      "        [ 0.8143],\n",
      "        [-1.5307],\n",
      "        [-0.9731],\n",
      "        [ 0.5839],\n",
      "        [-0.1994],\n",
      "        [ 0.9210],\n",
      "        [-0.1994],\n",
      "        [ 0.5839],\n",
      "        [-0.6967],\n",
      "        [-0.3009],\n",
      "        [ 0.8684],\n",
      "        [ 0.7587],\n",
      "        [ 0.7867],\n",
      "        [ 1.1825],\n",
      "        [ 1.1825],\n",
      "        [ 1.0457],\n",
      "        [-0.3009],\n",
      "        [-1.2234],\n",
      "        [ 1.0457],\n",
      "        [-1.3971],\n",
      "        [ 0.9467],\n",
      "        [-1.3002],\n",
      "        [ 0.5839],\n",
      "        [-0.1994],\n",
      "        [ 0.7587],\n",
      "        [ 0.5536],\n",
      "        [ 1.1825],\n",
      "        [ 0.7587],\n",
      "        [-0.7913],\n",
      "        [-0.0979],\n",
      "        [-1.1157],\n",
      "        [-1.1703],\n",
      "        [ 0.9467],\n",
      "        [-0.9731],\n",
      "        [ 0.6435],\n",
      "        [ 1.1825],\n",
      "        [-1.5516],\n",
      "        [ 0.1041],\n",
      "        [-0.4351],\n",
      "        [-1.3002],\n",
      "        [ 0.9970],\n",
      "        [-0.4351],\n",
      "        [-0.6325],\n",
      "        [-1.4878],\n",
      "        [ 0.6435],\n",
      "        [-1.3971],\n",
      "        [ 0.4611],\n",
      "        [-0.4684],\n",
      "        [ 0.4611],\n",
      "        [ 1.1825],\n",
      "        [ 1.1447]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 241/10000,\n",
      " train_loss: 0.0014,\n",
      " train_mae: 0.0306,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1909],\n",
      "        [-1.1909],\n",
      "        [ 0.6785],\n",
      "        [ 0.6196],\n",
      "        [ 0.0101],\n",
      "        [ 0.5290],\n",
      "        [ 0.9002],\n",
      "        [ 0.9519],\n",
      "        [ 0.9002],\n",
      "        [ 0.8197],\n",
      "        [-1.5256],\n",
      "        [-0.9666],\n",
      "        [ 0.5897],\n",
      "        [-0.1927],\n",
      "        [ 0.9262],\n",
      "        [-0.1927],\n",
      "        [ 0.5897],\n",
      "        [-0.6899],\n",
      "        [-0.2940],\n",
      "        [ 0.8737],\n",
      "        [ 0.7643],\n",
      "        [ 0.7922],\n",
      "        [ 1.1874],\n",
      "        [ 1.1874],\n",
      "        [ 1.0508],\n",
      "        [-0.2940],\n",
      "        [-1.2173],\n",
      "        [ 1.0508],\n",
      "        [-1.3916],\n",
      "        [ 0.9519],\n",
      "        [-1.2943],\n",
      "        [ 0.5897],\n",
      "        [-0.1927],\n",
      "        [ 0.7643],\n",
      "        [ 0.5595],\n",
      "        [ 1.1874],\n",
      "        [ 0.7643],\n",
      "        [-0.7846],\n",
      "        [-0.0911],\n",
      "        [-1.1094],\n",
      "        [-1.1641],\n",
      "        [ 0.9519],\n",
      "        [-0.9666],\n",
      "        [ 0.6492],\n",
      "        [ 1.1874],\n",
      "        [-1.5466],\n",
      "        [ 0.1106],\n",
      "        [-0.4282],\n",
      "        [-1.2943],\n",
      "        [ 1.0021],\n",
      "        [-0.4282],\n",
      "        [-0.6256],\n",
      "        [-1.4825],\n",
      "        [ 0.6492],\n",
      "        [-1.3916],\n",
      "        [ 0.4671],\n",
      "        [-0.4615],\n",
      "        [ 0.4671],\n",
      "        [ 1.1874],\n",
      "        [ 1.1496]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 242/10000,\n",
      " train_loss: 0.0014,\n",
      " train_mae: 0.0303,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1953],\n",
      "        [-1.1953],\n",
      "        [ 0.6731],\n",
      "        [ 0.6141],\n",
      "        [ 0.0044],\n",
      "        [ 0.5234],\n",
      "        [ 0.8952],\n",
      "        [ 0.9471],\n",
      "        [ 0.8952],\n",
      "        [ 0.8146],\n",
      "        [-1.5297],\n",
      "        [-0.9712],\n",
      "        [ 0.5842],\n",
      "        [-0.1982],\n",
      "        [ 0.9213],\n",
      "        [-0.1982],\n",
      "        [ 0.5842],\n",
      "        [-0.6950],\n",
      "        [-0.2995],\n",
      "        [ 0.8687],\n",
      "        [ 0.7590],\n",
      "        [ 0.7870],\n",
      "        [ 1.1833],\n",
      "        [ 1.1833],\n",
      "        [ 1.0462],\n",
      "        [-0.2995],\n",
      "        [-1.2217],\n",
      "        [ 1.0462],\n",
      "        [-1.3957],\n",
      "        [ 0.9471],\n",
      "        [-1.2986],\n",
      "        [ 0.5842],\n",
      "        [-0.1982],\n",
      "        [ 0.7590],\n",
      "        [ 0.5539],\n",
      "        [ 1.1833],\n",
      "        [ 0.7590],\n",
      "        [-0.7895],\n",
      "        [-0.0968],\n",
      "        [-1.1139],\n",
      "        [-1.1685],\n",
      "        [ 0.9471],\n",
      "        [-0.9712],\n",
      "        [ 0.6438],\n",
      "        [ 1.1833],\n",
      "        [-1.5506],\n",
      "        [ 0.1049],\n",
      "        [-0.4336],\n",
      "        [-1.2986],\n",
      "        [ 0.9974],\n",
      "        [-0.4336],\n",
      "        [-0.6307],\n",
      "        [-1.4866],\n",
      "        [ 0.6438],\n",
      "        [-1.3957],\n",
      "        [ 0.4615],\n",
      "        [-0.4668],\n",
      "        [ 0.4615],\n",
      "        [ 1.1833],\n",
      "        [ 1.1453]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 243/10000,\n",
      " train_loss: 0.0013,\n",
      " train_mae: 0.0305,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1929],\n",
      "        [-1.1929],\n",
      "        [ 0.6772],\n",
      "        [ 0.6183],\n",
      "        [ 0.0084],\n",
      "        [ 0.5276],\n",
      "        [ 0.8994],\n",
      "        [ 0.9513],\n",
      "        [ 0.8994],\n",
      "        [ 0.8187],\n",
      "        [-1.5282],\n",
      "        [-0.9684],\n",
      "        [ 0.5884],\n",
      "        [-0.1944],\n",
      "        [ 0.9255],\n",
      "        [-0.1944],\n",
      "        [ 0.5884],\n",
      "        [-0.6917],\n",
      "        [-0.2958],\n",
      "        [ 0.8729],\n",
      "        [ 0.7632],\n",
      "        [ 0.7911],\n",
      "        [ 1.1875],\n",
      "        [ 1.1875],\n",
      "        [ 1.0504],\n",
      "        [-0.2958],\n",
      "        [-1.2194],\n",
      "        [ 1.0504],\n",
      "        [-1.3938],\n",
      "        [ 0.9513],\n",
      "        [-1.2964],\n",
      "        [ 0.5884],\n",
      "        [-0.1944],\n",
      "        [ 0.7632],\n",
      "        [ 0.5581],\n",
      "        [ 1.1875],\n",
      "        [ 0.7632],\n",
      "        [-0.7864],\n",
      "        [-0.0929],\n",
      "        [-1.1114],\n",
      "        [-1.1661],\n",
      "        [ 0.9513],\n",
      "        [-0.9684],\n",
      "        [ 0.6479],\n",
      "        [ 1.1875],\n",
      "        [-1.5492],\n",
      "        [ 0.1089],\n",
      "        [-0.4300],\n",
      "        [-1.2964],\n",
      "        [ 1.0016],\n",
      "        [-0.4300],\n",
      "        [-0.6274],\n",
      "        [-1.4850],\n",
      "        [ 0.6479],\n",
      "        [-1.3938],\n",
      "        [ 0.4657],\n",
      "        [-0.4633],\n",
      "        [ 0.4657],\n",
      "        [ 1.1875],\n",
      "        [ 1.1495]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 244/10000,\n",
      " train_loss: 0.0013,\n",
      " train_mae: 0.0300,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1932],\n",
      "        [-1.1932],\n",
      "        [ 0.6742],\n",
      "        [ 0.6152],\n",
      "        [ 0.0060],\n",
      "        [ 0.5246],\n",
      "        [ 0.8963],\n",
      "        [ 0.9482],\n",
      "        [ 0.8963],\n",
      "        [ 0.8157],\n",
      "        [-1.5284],\n",
      "        [-0.9690],\n",
      "        [ 0.5853],\n",
      "        [-0.1964],\n",
      "        [ 0.9225],\n",
      "        [-0.1964],\n",
      "        [ 0.5853],\n",
      "        [-0.6927],\n",
      "        [-0.2975],\n",
      "        [ 0.8698],\n",
      "        [ 0.7601],\n",
      "        [ 0.7881],\n",
      "        [ 1.1848],\n",
      "        [ 1.1848],\n",
      "        [ 1.0475],\n",
      "        [-0.2975],\n",
      "        [-1.2197],\n",
      "        [ 1.0475],\n",
      "        [-1.3941],\n",
      "        [ 0.9482],\n",
      "        [-1.2967],\n",
      "        [ 0.5853],\n",
      "        [-0.1964],\n",
      "        [ 0.7601],\n",
      "        [ 0.5551],\n",
      "        [ 1.1848],\n",
      "        [ 0.7601],\n",
      "        [-0.7872],\n",
      "        [-0.0950],\n",
      "        [-1.1118],\n",
      "        [-1.1664],\n",
      "        [ 0.9482],\n",
      "        [-0.9690],\n",
      "        [ 0.6449],\n",
      "        [ 1.1848],\n",
      "        [-1.5494],\n",
      "        [ 0.1064],\n",
      "        [-0.4315],\n",
      "        [-1.2967],\n",
      "        [ 0.9986],\n",
      "        [-0.4315],\n",
      "        [-0.6285],\n",
      "        [-1.4852],\n",
      "        [ 0.6449],\n",
      "        [-1.3941],\n",
      "        [ 0.4627],\n",
      "        [-0.4647],\n",
      "        [ 0.4627],\n",
      "        [ 1.1848],\n",
      "        [ 1.1467]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 245/10000,\n",
      " train_loss: 0.0013,\n",
      " train_mae: 0.0302,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1941],\n",
      "        [-1.1941],\n",
      "        [ 0.6754],\n",
      "        [ 0.6164],\n",
      "        [ 0.0066],\n",
      "        [ 0.5257],\n",
      "        [ 0.8979],\n",
      "        [ 0.9498],\n",
      "        [ 0.8979],\n",
      "        [ 0.8171],\n",
      "        [-1.5298],\n",
      "        [-0.9696],\n",
      "        [ 0.5865],\n",
      "        [-0.1961],\n",
      "        [ 0.9240],\n",
      "        [-0.1961],\n",
      "        [ 0.5865],\n",
      "        [-0.6929],\n",
      "        [-0.2974],\n",
      "        [ 0.8713],\n",
      "        [ 0.7615],\n",
      "        [ 0.7895],\n",
      "        [ 1.1867],\n",
      "        [ 1.1867],\n",
      "        [ 1.0492],\n",
      "        [-0.2974],\n",
      "        [-1.2206],\n",
      "        [ 1.0492],\n",
      "        [-1.3952],\n",
      "        [ 0.9498],\n",
      "        [-1.2977],\n",
      "        [ 0.5865],\n",
      "        [-0.1961],\n",
      "        [ 0.7615],\n",
      "        [ 0.5562],\n",
      "        [ 1.1867],\n",
      "        [ 0.7615],\n",
      "        [-0.7876],\n",
      "        [-0.0946],\n",
      "        [-1.1125],\n",
      "        [-1.1673],\n",
      "        [ 0.9498],\n",
      "        [-0.9696],\n",
      "        [ 0.6461],\n",
      "        [ 1.1867],\n",
      "        [-1.5508],\n",
      "        [ 0.1071],\n",
      "        [-0.4315],\n",
      "        [-1.2977],\n",
      "        [ 1.0003],\n",
      "        [-0.4315],\n",
      "        [-0.6287],\n",
      "        [-1.4865],\n",
      "        [ 0.6461],\n",
      "        [-1.3952],\n",
      "        [ 0.4637],\n",
      "        [-0.4647],\n",
      "        [ 0.4637],\n",
      "        [ 1.1867],\n",
      "        [ 1.1486]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 246/10000,\n",
      " train_loss: 0.0013,\n",
      " train_mae: 0.0300,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1925],\n",
      "        [-1.1925],\n",
      "        [ 0.6756],\n",
      "        [ 0.6167],\n",
      "        [ 0.0074],\n",
      "        [ 0.5260],\n",
      "        [ 0.8980],\n",
      "        [ 0.9500],\n",
      "        [ 0.8980],\n",
      "        [ 0.8172],\n",
      "        [-1.5285],\n",
      "        [-0.9679],\n",
      "        [ 0.5867],\n",
      "        [-0.1950],\n",
      "        [ 0.9242],\n",
      "        [-0.1950],\n",
      "        [ 0.5867],\n",
      "        [-0.6914],\n",
      "        [-0.2962],\n",
      "        [ 0.8714],\n",
      "        [ 0.7616],\n",
      "        [ 0.7896],\n",
      "        [ 1.1869],\n",
      "        [ 1.1869],\n",
      "        [ 1.0494],\n",
      "        [-0.2962],\n",
      "        [-1.2190],\n",
      "        [ 1.0494],\n",
      "        [-1.3937],\n",
      "        [ 0.9500],\n",
      "        [-1.2961],\n",
      "        [ 0.5867],\n",
      "        [-0.1950],\n",
      "        [ 0.7616],\n",
      "        [ 0.5565],\n",
      "        [ 1.1869],\n",
      "        [ 0.7616],\n",
      "        [-0.7860],\n",
      "        [-0.0937],\n",
      "        [-1.1109],\n",
      "        [-1.1656],\n",
      "        [ 0.9500],\n",
      "        [-0.9679],\n",
      "        [ 0.6463],\n",
      "        [ 1.1869],\n",
      "        [-1.5496],\n",
      "        [ 0.1078],\n",
      "        [-0.4301],\n",
      "        [-1.2961],\n",
      "        [ 1.0005],\n",
      "        [-0.4301],\n",
      "        [-0.6272],\n",
      "        [-1.4851],\n",
      "        [ 0.6463],\n",
      "        [-1.3937],\n",
      "        [ 0.4641],\n",
      "        [-0.4634],\n",
      "        [ 0.4641],\n",
      "        [ 1.1869],\n",
      "        [ 1.1488]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 247/10000,\n",
      " train_loss: 0.0013,\n",
      " train_mae: 0.0298,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1942],\n",
      "        [-1.1942],\n",
      "        [ 0.6735],\n",
      "        [ 0.6145],\n",
      "        [ 0.0052],\n",
      "        [ 0.5237],\n",
      "        [ 0.8961],\n",
      "        [ 0.9482],\n",
      "        [ 0.8961],\n",
      "        [ 0.8153],\n",
      "        [-1.5302],\n",
      "        [-0.9697],\n",
      "        [ 0.5846],\n",
      "        [-0.1972],\n",
      "        [ 0.9223],\n",
      "        [-0.1972],\n",
      "        [ 0.5846],\n",
      "        [-0.6933],\n",
      "        [-0.2983],\n",
      "        [ 0.8695],\n",
      "        [ 0.7596],\n",
      "        [ 0.7876],\n",
      "        [ 1.1855],\n",
      "        [ 1.1855],\n",
      "        [ 1.0477],\n",
      "        [-0.2983],\n",
      "        [-1.2207],\n",
      "        [ 1.0477],\n",
      "        [-1.3955],\n",
      "        [ 0.9482],\n",
      "        [-1.2979],\n",
      "        [ 0.5846],\n",
      "        [-0.1972],\n",
      "        [ 0.7596],\n",
      "        [ 0.5543],\n",
      "        [ 1.1855],\n",
      "        [ 0.7596],\n",
      "        [-0.7879],\n",
      "        [-0.0959],\n",
      "        [-1.1126],\n",
      "        [-1.1674],\n",
      "        [ 0.9482],\n",
      "        [-0.9697],\n",
      "        [ 0.6442],\n",
      "        [ 1.1855],\n",
      "        [-1.5513],\n",
      "        [ 0.1055],\n",
      "        [-0.4322],\n",
      "        [-1.2979],\n",
      "        [ 0.9987],\n",
      "        [-0.4322],\n",
      "        [-0.6291],\n",
      "        [-1.4869],\n",
      "        [ 0.6442],\n",
      "        [-1.3955],\n",
      "        [ 0.4618],\n",
      "        [-0.4654],\n",
      "        [ 0.4618],\n",
      "        [ 1.1855],\n",
      "        [ 1.1473]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 248/10000,\n",
      " train_loss: 0.0013,\n",
      " train_mae: 0.0300,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1923],\n",
      "        [-1.1923],\n",
      "        [ 0.6770],\n",
      "        [ 0.6180],\n",
      "        [ 0.0084],\n",
      "        [ 0.5272],\n",
      "        [ 0.8996],\n",
      "        [ 0.9517],\n",
      "        [ 0.8996],\n",
      "        [ 0.8188],\n",
      "        [-1.5291],\n",
      "        [-0.9674],\n",
      "        [ 0.5880],\n",
      "        [-0.1940],\n",
      "        [ 0.9258],\n",
      "        [-0.1940],\n",
      "        [ 0.5880],\n",
      "        [-0.6906],\n",
      "        [-0.2952],\n",
      "        [ 0.8730],\n",
      "        [ 0.7631],\n",
      "        [ 0.7911],\n",
      "        [ 1.1891],\n",
      "        [ 1.1891],\n",
      "        [ 1.0513],\n",
      "        [-0.2952],\n",
      "        [-1.2189],\n",
      "        [ 1.0513],\n",
      "        [-1.3940],\n",
      "        [ 0.9517],\n",
      "        [-1.2962],\n",
      "        [ 0.5880],\n",
      "        [-0.1940],\n",
      "        [ 0.7631],\n",
      "        [ 0.5578],\n",
      "        [ 1.1891],\n",
      "        [ 0.7631],\n",
      "        [-0.7853],\n",
      "        [-0.0927],\n",
      "        [-1.1106],\n",
      "        [-1.1654],\n",
      "        [ 0.9517],\n",
      "        [-0.9674],\n",
      "        [ 0.6477],\n",
      "        [ 1.1891],\n",
      "        [-1.5503],\n",
      "        [ 0.1088],\n",
      "        [-0.4292],\n",
      "        [-1.2962],\n",
      "        [ 1.0022],\n",
      "        [-0.4292],\n",
      "        [-0.6264],\n",
      "        [-1.4856],\n",
      "        [ 0.6477],\n",
      "        [-1.3940],\n",
      "        [ 0.4653],\n",
      "        [-0.4625],\n",
      "        [ 0.4653],\n",
      "        [ 1.1891],\n",
      "        [ 1.1509]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 249/10000,\n",
      " train_loss: 0.0013,\n",
      " train_mae: 0.0295,\n",
      " epoch_time_duration: 0.0124\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1943],\n",
      "        [-1.1943],\n",
      "        [ 0.6718],\n",
      "        [ 0.6128],\n",
      "        [ 0.0039],\n",
      "        [ 0.5220],\n",
      "        [ 0.8945],\n",
      "        [ 0.9467],\n",
      "        [ 0.8945],\n",
      "        [ 0.8136],\n",
      "        [-1.5307],\n",
      "        [-0.9698],\n",
      "        [ 0.5828],\n",
      "        [-0.1981],\n",
      "        [ 0.9208],\n",
      "        [-0.1981],\n",
      "        [ 0.5828],\n",
      "        [-0.6936],\n",
      "        [-0.2991],\n",
      "        [ 0.8679],\n",
      "        [ 0.7579],\n",
      "        [ 0.7859],\n",
      "        [ 1.1846],\n",
      "        [ 1.1846],\n",
      "        [ 1.0464],\n",
      "        [-0.2991],\n",
      "        [-1.2208],\n",
      "        [ 1.0464],\n",
      "        [-1.3957],\n",
      "        [ 0.9467],\n",
      "        [-1.2980],\n",
      "        [ 0.5828],\n",
      "        [-0.1981],\n",
      "        [ 0.7579],\n",
      "        [ 0.5526],\n",
      "        [ 1.1846],\n",
      "        [ 0.7579],\n",
      "        [-0.7881],\n",
      "        [-0.0970],\n",
      "        [-1.1127],\n",
      "        [-1.1675],\n",
      "        [ 0.9467],\n",
      "        [-0.9698],\n",
      "        [ 0.6425],\n",
      "        [ 1.1846],\n",
      "        [-1.5519],\n",
      "        [ 0.1042],\n",
      "        [-0.4328],\n",
      "        [-1.2980],\n",
      "        [ 0.9973],\n",
      "        [-0.4328],\n",
      "        [-0.6295],\n",
      "        [-1.4873],\n",
      "        [ 0.6425],\n",
      "        [-1.3957],\n",
      "        [ 0.4602],\n",
      "        [-0.4660],\n",
      "        [ 0.4602],\n",
      "        [ 1.1846],\n",
      "        [ 1.1463]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 250/10000,\n",
      " train_loss: 0.0013,\n",
      " train_mae: 0.0300,\n",
      " epoch_time_duration: 0.0143\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1916],\n",
      "        [-1.1916],\n",
      "        [ 0.6783],\n",
      "        [ 0.6192],\n",
      "        [ 0.0096],\n",
      "        [ 0.5284],\n",
      "        [ 0.9011],\n",
      "        [ 0.9532],\n",
      "        [ 0.9011],\n",
      "        [ 0.8201],\n",
      "        [-1.5292],\n",
      "        [-0.9664],\n",
      "        [ 0.5892],\n",
      "        [-0.1928],\n",
      "        [ 0.9273],\n",
      "        [-0.1928],\n",
      "        [ 0.5892],\n",
      "        [-0.6895],\n",
      "        [-0.2940],\n",
      "        [ 0.8745],\n",
      "        [ 0.7644],\n",
      "        [ 0.7924],\n",
      "        [ 1.1910],\n",
      "        [ 1.1910],\n",
      "        [ 1.0529],\n",
      "        [-0.2940],\n",
      "        [-1.2182],\n",
      "        [ 1.0529],\n",
      "        [-1.3937],\n",
      "        [ 0.9532],\n",
      "        [-1.2957],\n",
      "        [ 0.5892],\n",
      "        [-0.1928],\n",
      "        [ 0.7644],\n",
      "        [ 0.5590],\n",
      "        [ 1.1910],\n",
      "        [ 0.7644],\n",
      "        [-0.7842],\n",
      "        [-0.0915],\n",
      "        [-1.1097],\n",
      "        [-1.1647],\n",
      "        [ 0.9532],\n",
      "        [-0.9664],\n",
      "        [ 0.6489],\n",
      "        [ 1.1910],\n",
      "        [-1.5504],\n",
      "        [ 0.1100],\n",
      "        [-0.4280],\n",
      "        [-1.2957],\n",
      "        [ 1.0038],\n",
      "        [-0.4280],\n",
      "        [-0.6252],\n",
      "        [-1.4856],\n",
      "        [ 0.6489],\n",
      "        [-1.3937],\n",
      "        [ 0.4665],\n",
      "        [-0.4612],\n",
      "        [ 0.4665],\n",
      "        [ 1.1910],\n",
      "        [ 1.1528]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 251/10000,\n",
      " train_loss: 0.0013,\n",
      " train_mae: 0.0294,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1953],\n",
      "        [-1.1953],\n",
      "        [ 0.6699],\n",
      "        [ 0.6109],\n",
      "        [ 0.0021],\n",
      "        [ 0.5201],\n",
      "        [ 0.8929],\n",
      "        [ 0.9451],\n",
      "        [ 0.8929],\n",
      "        [ 0.8119],\n",
      "        [-1.5321],\n",
      "        [-0.9708],\n",
      "        [ 0.5809],\n",
      "        [-0.1997],\n",
      "        [ 0.9192],\n",
      "        [-0.1997],\n",
      "        [ 0.5809],\n",
      "        [-0.6948],\n",
      "        [-0.3006],\n",
      "        [ 0.8663],\n",
      "        [ 0.7561],\n",
      "        [ 0.7842],\n",
      "        [ 1.1836],\n",
      "        [ 1.1836],\n",
      "        [ 1.0451],\n",
      "        [-0.3006],\n",
      "        [-1.2218],\n",
      "        [ 1.0451],\n",
      "        [-1.3969],\n",
      "        [ 0.9451],\n",
      "        [-1.2991],\n",
      "        [ 0.5809],\n",
      "        [-0.1997],\n",
      "        [ 0.7561],\n",
      "        [ 0.5506],\n",
      "        [ 1.1836],\n",
      "        [ 0.7561],\n",
      "        [-0.7892],\n",
      "        [-0.0987],\n",
      "        [-1.1137],\n",
      "        [-1.1685],\n",
      "        [ 0.9451],\n",
      "        [-0.9708],\n",
      "        [ 0.6406],\n",
      "        [ 1.1836],\n",
      "        [-1.5533],\n",
      "        [ 0.1023],\n",
      "        [-0.4342],\n",
      "        [-1.2991],\n",
      "        [ 0.9959],\n",
      "        [-0.4342],\n",
      "        [-0.6307],\n",
      "        [-1.4886],\n",
      "        [ 0.6406],\n",
      "        [-1.3969],\n",
      "        [ 0.4582],\n",
      "        [-0.4673],\n",
      "        [ 0.4582],\n",
      "        [ 1.1836],\n",
      "        [ 1.1452]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 252/10000,\n",
      " train_loss: 0.0013,\n",
      " train_mae: 0.0300,\n",
      " epoch_time_duration: 0.0088\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1898],\n",
      "        [-1.1898],\n",
      "        [ 0.6802],\n",
      "        [ 0.6211],\n",
      "        [ 0.0117],\n",
      "        [ 0.5303],\n",
      "        [ 0.9031],\n",
      "        [ 0.9553],\n",
      "        [ 0.9031],\n",
      "        [ 0.8221],\n",
      "        [-1.5282],\n",
      "        [-0.9643],\n",
      "        [ 0.5912],\n",
      "        [-0.1906],\n",
      "        [ 0.9294],\n",
      "        [-0.1906],\n",
      "        [ 0.5912],\n",
      "        [-0.6872],\n",
      "        [-0.2918],\n",
      "        [ 0.8765],\n",
      "        [ 0.7664],\n",
      "        [ 0.7944],\n",
      "        [ 1.1934],\n",
      "        [ 1.1934],\n",
      "        [ 1.0551],\n",
      "        [-0.2918],\n",
      "        [-1.2165],\n",
      "        [ 1.0551],\n",
      "        [-1.3924],\n",
      "        [ 0.9553],\n",
      "        [-1.2941],\n",
      "        [ 0.5912],\n",
      "        [-0.1906],\n",
      "        [ 0.7664],\n",
      "        [ 0.5609],\n",
      "        [ 1.1934],\n",
      "        [ 0.7664],\n",
      "        [-0.7820],\n",
      "        [-0.0893],\n",
      "        [-1.1078],\n",
      "        [-1.1629],\n",
      "        [ 0.9553],\n",
      "        [-0.9643],\n",
      "        [ 0.6508],\n",
      "        [ 1.1934],\n",
      "        [-1.5495],\n",
      "        [ 0.1121],\n",
      "        [-0.4257],\n",
      "        [-1.2941],\n",
      "        [ 1.0059],\n",
      "        [-0.4257],\n",
      "        [-0.6229],\n",
      "        [-1.4845],\n",
      "        [ 0.6508],\n",
      "        [-1.3924],\n",
      "        [ 0.4684],\n",
      "        [-0.4590],\n",
      "        [ 0.4684],\n",
      "        [ 1.1934],\n",
      "        [ 1.1550]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 253/10000,\n",
      " train_loss: 0.0013,\n",
      " train_mae: 0.0294,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1975e+00],\n",
      "        [-1.1975e+00],\n",
      "        [ 6.6667e-01],\n",
      "        [ 6.0758e-01],\n",
      "        [-1.0944e-03],\n",
      "        [ 5.1674e-01],\n",
      "        [ 8.8998e-01],\n",
      "        [ 9.4231e-01],\n",
      "        [ 8.8998e-01],\n",
      "        [ 8.0880e-01],\n",
      "        [-1.5345e+00],\n",
      "        [-9.7309e-01],\n",
      "        [ 5.7758e-01],\n",
      "        [-2.0278e-01],\n",
      "        [ 9.1633e-01],\n",
      "        [-2.0278e-01],\n",
      "        [ 5.7758e-01],\n",
      "        [-6.9731e-01],\n",
      "        [-3.0356e-01],\n",
      "        [ 8.6328e-01],\n",
      "        [ 7.5294e-01],\n",
      "        [ 7.8104e-01],\n",
      "        [ 1.1815e+00],\n",
      "        [ 1.1815e+00],\n",
      "        [ 1.0425e+00],\n",
      "        [-3.0356e-01],\n",
      "        [-1.2240e+00],\n",
      "        [ 1.0425e+00],\n",
      "        [-1.3992e+00],\n",
      "        [ 9.4231e-01],\n",
      "        [-1.3013e+00],\n",
      "        [ 5.7758e-01],\n",
      "        [-2.0278e-01],\n",
      "        [ 7.5294e-01],\n",
      "        [ 5.4730e-01],\n",
      "        [ 1.1815e+00],\n",
      "        [ 7.5294e-01],\n",
      "        [-7.9161e-01],\n",
      "        [-1.0182e-01],\n",
      "        [-1.1159e+00],\n",
      "        [-1.1707e+00],\n",
      "        [ 9.4231e-01],\n",
      "        [-9.7309e-01],\n",
      "        [ 6.3727e-01],\n",
      "        [ 1.1815e+00],\n",
      "        [-1.5558e+00],\n",
      "        [ 9.9017e-02],\n",
      "        [-4.3700e-01],\n",
      "        [-1.3013e+00],\n",
      "        [ 9.9317e-01],\n",
      "        [-4.3700e-01],\n",
      "        [-6.3331e-01],\n",
      "        [-1.4909e+00],\n",
      "        [ 6.3727e-01],\n",
      "        [-1.3992e+00],\n",
      "        [ 4.5482e-01],\n",
      "        [-4.7009e-01],\n",
      "        [ 4.5482e-01],\n",
      "        [ 1.1815e+00],\n",
      "        [ 1.1429e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 254/10000,\n",
      " train_loss: 0.0013,\n",
      " train_mae: 0.0307,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1866],\n",
      "        [-1.1866],\n",
      "        [ 0.6846],\n",
      "        [ 0.6255],\n",
      "        [ 0.0162],\n",
      "        [ 0.5347],\n",
      "        [ 0.9075],\n",
      "        [ 0.9597],\n",
      "        [ 0.9075],\n",
      "        [ 0.8265],\n",
      "        [-1.5261],\n",
      "        [-0.9606],\n",
      "        [ 0.5956],\n",
      "        [-0.1861],\n",
      "        [ 0.9338],\n",
      "        [-0.1861],\n",
      "        [ 0.5956],\n",
      "        [-0.6831],\n",
      "        [-0.2873],\n",
      "        [ 0.8809],\n",
      "        [ 0.7707],\n",
      "        [ 0.7988],\n",
      "        [ 1.1979],\n",
      "        [ 1.1979],\n",
      "        [ 1.0595],\n",
      "        [-0.2873],\n",
      "        [-1.2133],\n",
      "        [ 1.0595],\n",
      "        [-1.3898],\n",
      "        [ 0.9597],\n",
      "        [-1.2912],\n",
      "        [ 0.5956],\n",
      "        [-0.1861],\n",
      "        [ 0.7707],\n",
      "        [ 0.5653],\n",
      "        [ 1.1979],\n",
      "        [ 0.7707],\n",
      "        [-0.7779],\n",
      "        [-0.0848],\n",
      "        [-1.1044],\n",
      "        [-1.1596],\n",
      "        [ 0.9597],\n",
      "        [-0.9606],\n",
      "        [ 0.6552],\n",
      "        [ 1.1979],\n",
      "        [-1.5475],\n",
      "        [ 0.1166],\n",
      "        [-0.4214],\n",
      "        [-1.2912],\n",
      "        [ 1.0104],\n",
      "        [-0.4214],\n",
      "        [-0.6187],\n",
      "        [-1.4822],\n",
      "        [ 0.6552],\n",
      "        [-1.3898],\n",
      "        [ 0.4728],\n",
      "        [-0.4546],\n",
      "        [ 0.4728],\n",
      "        [ 1.1979],\n",
      "        [ 1.1595]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 255/10000,\n",
      " train_loss: 0.0013,\n",
      " train_mae: 0.0301,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2018],\n",
      "        [-1.2018],\n",
      "        [ 0.6595],\n",
      "        [ 0.6003],\n",
      "        [-0.0080],\n",
      "        [ 0.5094],\n",
      "        [ 0.8832],\n",
      "        [ 0.9356],\n",
      "        [ 0.8832],\n",
      "        [ 0.8018],\n",
      "        [-1.5385],\n",
      "        [-0.9778],\n",
      "        [ 0.5703],\n",
      "        [-0.2093],\n",
      "        [ 0.9096],\n",
      "        [-0.2093],\n",
      "        [ 0.5703],\n",
      "        [-0.7027],\n",
      "        [-0.3098],\n",
      "        [ 0.8564],\n",
      "        [ 0.7458],\n",
      "        [ 0.7740],\n",
      "        [ 1.1757],\n",
      "        [ 1.1757],\n",
      "        [ 1.0362],\n",
      "        [-0.3098],\n",
      "        [-1.2283],\n",
      "        [ 1.0362],\n",
      "        [-1.4032],\n",
      "        [ 0.9356],\n",
      "        [-1.3055],\n",
      "        [ 0.5703],\n",
      "        [-0.2093],\n",
      "        [ 0.7458],\n",
      "        [ 0.5400],\n",
      "        [ 1.1757],\n",
      "        [ 0.7458],\n",
      "        [-0.7967],\n",
      "        [-0.1085],\n",
      "        [-1.1203],\n",
      "        [-1.1750],\n",
      "        [ 0.9356],\n",
      "        [-0.9778],\n",
      "        [ 0.6300],\n",
      "        [ 1.1757],\n",
      "        [-1.5597],\n",
      "        [ 0.0920],\n",
      "        [-0.4430],\n",
      "        [-1.3055],\n",
      "        [ 0.9866],\n",
      "        [-0.4430],\n",
      "        [-0.6388],\n",
      "        [-1.4949],\n",
      "        [ 0.6300],\n",
      "        [-1.4032],\n",
      "        [ 0.4475],\n",
      "        [-0.4760],\n",
      "        [ 0.4475],\n",
      "        [ 1.1757],\n",
      "        [ 1.1370]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 256/10000,\n",
      " train_loss: 0.0014,\n",
      " train_mae: 0.0334,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1803],\n",
      "        [-1.1803],\n",
      "        [ 0.6955],\n",
      "        [ 0.6364],\n",
      "        [ 0.0266],\n",
      "        [ 0.5456],\n",
      "        [ 0.9183],\n",
      "        [ 0.9704],\n",
      "        [ 0.9183],\n",
      "        [ 0.8373],\n",
      "        [-1.5218],\n",
      "        [-0.9532],\n",
      "        [ 0.6064],\n",
      "        [-0.1762],\n",
      "        [ 0.9445],\n",
      "        [-0.1762],\n",
      "        [ 0.6064],\n",
      "        [-0.6745],\n",
      "        [-0.2776],\n",
      "        [ 0.8917],\n",
      "        [ 0.7816],\n",
      "        [ 0.8096],\n",
      "        [ 1.2084],\n",
      "        [ 1.2084],\n",
      "        [ 1.0702],\n",
      "        [-0.2776],\n",
      "        [-1.2071],\n",
      "        [ 1.0702],\n",
      "        [-1.3846],\n",
      "        [ 0.9704],\n",
      "        [-1.2854],\n",
      "        [ 0.6064],\n",
      "        [-0.1762],\n",
      "        [ 0.7816],\n",
      "        [ 0.5762],\n",
      "        [ 1.2084],\n",
      "        [ 0.7816],\n",
      "        [-0.7697],\n",
      "        [-0.0746],\n",
      "        [-1.0977],\n",
      "        [-1.1531],\n",
      "        [ 0.9704],\n",
      "        [-0.9532],\n",
      "        [ 0.6661],\n",
      "        [ 1.2084],\n",
      "        [-1.5433],\n",
      "        [ 0.1271],\n",
      "        [-0.4120],\n",
      "        [-1.2854],\n",
      "        [ 1.0211],\n",
      "        [-0.4120],\n",
      "        [-0.6099],\n",
      "        [-1.4776],\n",
      "        [ 0.6661],\n",
      "        [-1.3846],\n",
      "        [ 0.4837],\n",
      "        [-0.4453],\n",
      "        [ 0.4837],\n",
      "        [ 1.2084],\n",
      "        [ 1.1701]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 257/10000,\n",
      " train_loss: 0.0016,\n",
      " train_mae: 0.0346,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2112],\n",
      "        [-1.2112],\n",
      "        [ 0.6421],\n",
      "        [ 0.5830],\n",
      "        [-0.0241],\n",
      "        [ 0.4921],\n",
      "        [ 0.8663],\n",
      "        [ 0.9190],\n",
      "        [ 0.8663],\n",
      "        [ 0.7847],\n",
      "        [-1.5462],\n",
      "        [-0.9885],\n",
      "        [ 0.5529],\n",
      "        [-0.2245],\n",
      "        [ 0.8928],\n",
      "        [-0.2245],\n",
      "        [ 0.5529],\n",
      "        [-0.7150],\n",
      "        [-0.3245],\n",
      "        [ 0.8395],\n",
      "        [ 0.7287],\n",
      "        [ 0.7569],\n",
      "        [ 1.1605],\n",
      "        [ 1.1605],\n",
      "        [ 1.0200],\n",
      "        [-0.3245],\n",
      "        [-1.2375],\n",
      "        [ 1.0200],\n",
      "        [-1.4115],\n",
      "        [ 0.9190],\n",
      "        [-1.3143],\n",
      "        [ 0.5529],\n",
      "        [-0.2245],\n",
      "        [ 0.7287],\n",
      "        [ 0.5226],\n",
      "        [ 1.1605],\n",
      "        [ 0.7287],\n",
      "        [-0.8085],\n",
      "        [-0.1242],\n",
      "        [-1.1302],\n",
      "        [-1.1845],\n",
      "        [ 0.9190],\n",
      "        [-0.9885],\n",
      "        [ 0.6127],\n",
      "        [ 1.1605],\n",
      "        [-1.5674],\n",
      "        [ 0.0755],\n",
      "        [-0.4569],\n",
      "        [-1.3143],\n",
      "        [ 0.9702],\n",
      "        [-0.4569],\n",
      "        [-0.6516],\n",
      "        [-1.5028],\n",
      "        [ 0.6127],\n",
      "        [-1.4115],\n",
      "        [ 0.4302],\n",
      "        [-0.4897],\n",
      "        [ 0.4302],\n",
      "        [ 1.1605],\n",
      "        [ 1.1215]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 258/10000,\n",
      " train_loss: 0.0020,\n",
      " train_mae: 0.0457,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1661],\n",
      "        [-1.1661],\n",
      "        [ 0.7233],\n",
      "        [ 0.6643],\n",
      "        [ 0.0524],\n",
      "        [ 0.5734],\n",
      "        [ 0.9458],\n",
      "        [ 0.9978],\n",
      "        [ 0.9458],\n",
      "        [ 0.8651],\n",
      "        [-1.5118],\n",
      "        [-0.9363],\n",
      "        [ 0.6343],\n",
      "        [-0.1517],\n",
      "        [ 0.9720],\n",
      "        [-0.1517],\n",
      "        [ 0.6343],\n",
      "        [-0.6546],\n",
      "        [-0.2539],\n",
      "        [ 0.9193],\n",
      "        [ 0.8094],\n",
      "        [ 0.8374],\n",
      "        [ 1.2347],\n",
      "        [ 1.2347],\n",
      "        [ 1.0972],\n",
      "        [-0.2539],\n",
      "        [-1.1932],\n",
      "        [ 1.0972],\n",
      "        [-1.3728],\n",
      "        [ 0.9978],\n",
      "        [-1.2725],\n",
      "        [ 0.6343],\n",
      "        [-0.1517],\n",
      "        [ 0.8094],\n",
      "        [ 0.6040],\n",
      "        [ 1.2347],\n",
      "        [ 0.8094],\n",
      "        [-0.7508],\n",
      "        [-0.0495],\n",
      "        [-1.0824],\n",
      "        [-1.1385],\n",
      "        [ 0.9978],\n",
      "        [-0.9363],\n",
      "        [ 0.6939],\n",
      "        [ 1.2347],\n",
      "        [-1.5336],\n",
      "        [ 0.1534],\n",
      "        [-0.3895],\n",
      "        [-1.2725],\n",
      "        [ 1.0483],\n",
      "        [-0.3895],\n",
      "        [-0.5893],\n",
      "        [-1.4670],\n",
      "        [ 0.6939],\n",
      "        [-1.3728],\n",
      "        [ 0.5113],\n",
      "        [-0.4231],\n",
      "        [ 0.5113],\n",
      "        [ 1.2347],\n",
      "        [ 1.1966]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 259/10000,\n",
      " train_loss: 0.0030,\n",
      " train_mae: 0.0667,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.2341],\n",
      "        [-1.2341],\n",
      "        [ 0.5991],\n",
      "        [ 0.5399],\n",
      "        [-0.0638],\n",
      "        [ 0.4491],\n",
      "        [ 0.8240],\n",
      "        [ 0.8770],\n",
      "        [ 0.8240],\n",
      "        [ 0.7420],\n",
      "        [-1.5643],\n",
      "        [-1.0149],\n",
      "        [ 0.5099],\n",
      "        [-0.2620],\n",
      "        [ 0.8507],\n",
      "        [-0.2620],\n",
      "        [ 0.5099],\n",
      "        [-0.7457],\n",
      "        [-0.3608],\n",
      "        [ 0.7970],\n",
      "        [ 0.6858],\n",
      "        [ 0.7140],\n",
      "        [ 1.1211],\n",
      "        [ 1.1211],\n",
      "        [ 0.9789],\n",
      "        [-0.3608],\n",
      "        [-1.2600],\n",
      "        [ 0.9789],\n",
      "        [-1.4315],\n",
      "        [ 0.8770],\n",
      "        [-1.3356],\n",
      "        [ 0.5099],\n",
      "        [-0.2620],\n",
      "        [ 0.6858],\n",
      "        [ 0.4796],\n",
      "        [ 1.1211],\n",
      "        [ 0.6858],\n",
      "        [-0.8378],\n",
      "        [-0.1629],\n",
      "        [-1.1543],\n",
      "        [-1.2078],\n",
      "        [ 0.8770],\n",
      "        [-1.0149],\n",
      "        [ 0.5696],\n",
      "        [ 1.1211],\n",
      "        [-1.5852],\n",
      "        [ 0.0348],\n",
      "        [-0.4914],\n",
      "        [-1.3356],\n",
      "        [ 0.9287],\n",
      "        [-0.4914],\n",
      "        [-0.6832],\n",
      "        [-1.5215],\n",
      "        [ 0.5696],\n",
      "        [-1.4315],\n",
      "        [ 0.3874],\n",
      "        [-0.5238],\n",
      "        [ 0.3874],\n",
      "        [ 1.1211],\n",
      "        [ 1.0816]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 260/10000,\n",
      " train_loss: 0.0053,\n",
      " train_mae: 0.0996,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "batch_graph: Data(x=[60, 1], edge_index=[2, 3540], edge_attr=[3540, 1], y=[60, 1], train_mask=[60], val_mask=[60])\n",
      "preds: tensor([[-1.1328],\n",
      "        [-1.1328],\n",
      "        [ 0.7958],\n",
      "        [ 0.7367],\n",
      "        [ 0.1184],\n",
      "        [ 0.6455],\n",
      "        [ 1.0174],\n",
      "        [ 1.0689],\n",
      "        [ 1.0174],\n",
      "        [ 0.9371],\n",
      "        [-1.4888],\n",
      "        [-0.8959],\n",
      "        [ 0.7066],\n",
      "        [-0.0899],\n",
      "        [ 1.0433],\n",
      "        [-0.0899],\n",
      "        [ 0.7066],\n",
      "        [-0.6058],\n",
      "        [-0.1944],\n",
      "        [ 0.9910],\n",
      "        [ 0.8817],\n",
      "        [ 0.9096],\n",
      "        [ 1.3020],\n",
      "        [ 1.3020],\n",
      "        [ 1.1670],\n",
      "        [-0.1944],\n",
      "        [-1.1608],\n",
      "        [ 1.1670],\n",
      "        [-1.3459],\n",
      "        [ 1.0689],\n",
      "        [-1.2425],\n",
      "        [ 0.7066],\n",
      "        [-0.0899],\n",
      "        [ 0.8817],\n",
      "        [ 0.6762],\n",
      "        [ 1.3020],\n",
      "        [ 0.8817],\n",
      "        [-0.7048],\n",
      "        [ 0.0145],\n",
      "        [-1.0466],\n",
      "        [-1.1045],\n",
      "        [ 1.0689],\n",
      "        [-0.8959],\n",
      "        [ 0.7664],\n",
      "        [ 1.3020],\n",
      "        [-1.5112],\n",
      "        [ 0.2212],\n",
      "        [-0.3334],\n",
      "        [-1.2425],\n",
      "        [ 1.1188],\n",
      "        [-0.3334],\n",
      "        [-0.5387],\n",
      "        [-1.4428],\n",
      "        [ 0.7664],\n",
      "        [-1.3459],\n",
      "        [ 0.5831],\n",
      "        [-0.3679],\n",
      "        [ 0.5831],\n",
      "        [ 1.3020],\n",
      "        [ 1.2647]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "epoch: 261/10000,\n",
      " train_loss: 0.0119,\n",
      " train_mae: 0.1455,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "early stopping activated\n",
      "== end training ==\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MLPModel(torch.nn.Module):\n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, dp_rate=0.1):\n",
    "        \"\"\"MLPModel.\n",
    "\n",
    "        Args:\n",
    "            c_in: Dimension of input features\n",
    "            c_hidden: Dimension of hidden features\n",
    "            c_out: Dimension of the output features. Usually number of classes in classification\n",
    "            num_layers: Number of hidden layers\n",
    "            dp_rate: Dropout rate to apply throughout the network\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l_idx in range(num_layers - 1):\n",
    "            layers += [torch.nn.Linear(in_channels, out_channels), torch.nn.Sigmoid(), torch.nn.Dropout(dp_rate)]\n",
    "            in_channels = c_hidden\n",
    "        layers += [torch.nn.Linear(in_channels, c_out)]\n",
    "        self.layers = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        \"\"\"Forward.\n",
    "\n",
    "        Args:\n",
    "            x: Input features per node\n",
    "\n",
    "        \"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "my_module = MLPModel(c_in=1, c_hidden=1, c_out=1,num_layers=2,dp_rate=0.0)\n",
    "\n",
    "print([param for param in my_module.parameters()])\n",
    "\n",
    "complete_model = GNN_naive_framework(my_module,device)\n",
    "opt = complete_model.configure_optimizer(lr=1)\n",
    "scheduler = complete_model.configure_scheduler(opt,1,1,10)\n",
    "\n",
    "history = complete_model.train([participant_graph],10000,1,opt,scheduler,\"train_loss\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 2.5801],\n",
       "         [-1.2462],\n",
       "         [-1.2404],\n",
       "         [-0.3156],\n",
       "         [-3.9072]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-8.2896,  0.5276, -9.6809, -8.8769, -4.8124], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 3.5628, -3.3370,  2.6040,  5.0165, -0.7148]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([2.0711], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param for param in my_module.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9899]]], device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_module.forward(torch.Tensor([[[1]]]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1722],\n",
      "        [-1.1722],\n",
      "        [ 0.6404],\n",
      "        [ 0.5790],\n",
      "        [-0.0048],\n",
      "        [ 0.4868],\n",
      "        [ 0.8862],\n",
      "        [ 0.9476],\n",
      "        [ 0.8862],\n",
      "        [ 0.7940],\n",
      "        [-1.6023],\n",
      "        [-0.9264],\n",
      "        [ 0.5483],\n",
      "        [-0.1891],\n",
      "        [ 0.9169],\n",
      "        [-0.1891],\n",
      "        [ 0.5483],\n",
      "        [-0.6499],\n",
      "        [-0.2813],\n",
      "        [ 0.8555],\n",
      "        [ 0.7326],\n",
      "        [ 0.7633],\n",
      "        [ 1.2549],\n",
      "        [ 1.2549],\n",
      "        [ 1.0705],\n",
      "        [-0.2813],\n",
      "        [-1.2029],\n",
      "        [ 1.0705],\n",
      "        [-1.4180],\n",
      "        [ 0.9476],\n",
      "        [-1.2951],\n",
      "        [ 0.5483],\n",
      "        [-0.1891],\n",
      "        [ 0.7326],\n",
      "        [ 0.5175],\n",
      "        [ 1.2549],\n",
      "        [ 0.7326],\n",
      "        [-0.7421],\n",
      "        [-0.0969],\n",
      "        [-1.0801],\n",
      "        [-1.1415],\n",
      "        [ 0.9476],\n",
      "        [-0.9264],\n",
      "        [ 0.6097],\n",
      "        [ 1.2549],\n",
      "        [-1.6331],\n",
      "        [ 0.0874],\n",
      "        [-0.4042],\n",
      "        [-1.2951],\n",
      "        [ 1.0091],\n",
      "        [-0.4042],\n",
      "        [-0.5885],\n",
      "        [-1.5409],\n",
      "        [ 0.6097],\n",
      "        [-1.4180],\n",
      "        [ 0.4254],\n",
      "        [-0.4349],\n",
      "        [ 0.4254],\n",
      "        [ 1.2549],\n",
      "        [ 1.2019]])\n",
      "tensor([[-1.1722],\n",
      "        [-1.1722],\n",
      "        [ 0.6404],\n",
      "        [ 0.5790],\n",
      "        [-0.0048],\n",
      "        [ 0.4868],\n",
      "        [ 0.8862],\n",
      "        [ 0.9476],\n",
      "        [ 0.8862],\n",
      "        [ 0.7940],\n",
      "        [-1.6023],\n",
      "        [-0.9264],\n",
      "        [ 0.5483],\n",
      "        [-0.1891],\n",
      "        [ 0.9169],\n",
      "        [-0.1891],\n",
      "        [ 0.5483],\n",
      "        [-0.6499],\n",
      "        [-0.2813],\n",
      "        [ 0.8555],\n",
      "        [ 0.7326],\n",
      "        [ 0.7633],\n",
      "        [ 1.2549],\n",
      "        [ 1.2549],\n",
      "        [ 1.0705],\n",
      "        [-0.2813],\n",
      "        [-1.2029],\n",
      "        [ 1.0705],\n",
      "        [-1.4180],\n",
      "        [ 0.9476],\n",
      "        [-1.2951],\n",
      "        [ 0.5483],\n",
      "        [-0.1891],\n",
      "        [ 0.7326],\n",
      "        [ 0.5175],\n",
      "        [ 1.2549],\n",
      "        [ 0.7326],\n",
      "        [-0.7421],\n",
      "        [-0.0969],\n",
      "        [-1.0801],\n",
      "        [-1.1415],\n",
      "        [ 0.9476],\n",
      "        [-0.9264],\n",
      "        [ 0.6097],\n",
      "        [ 1.2549],\n",
      "        [-1.6331],\n",
      "        [ 0.0874],\n",
      "        [-0.4042],\n",
      "        [-1.2951],\n",
      "        [ 1.0091],\n",
      "        [-0.4042],\n",
      "        [-0.5885],\n",
      "        [-1.5409],\n",
      "        [ 0.6097],\n",
      "        [-1.4180],\n",
      "        [ 0.4254],\n",
      "        [-0.4349],\n",
      "        [ 0.4254],\n",
      "        [ 1.2549],\n",
      "        [ 1.2019]])\n"
     ]
    }
   ],
   "source": [
    "print(participant_graph.y)\n",
    "\n",
    "print(participant_graph.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           -1.1729376316070557,
           -1.1729376316070557,
           0.6497519016265869,
           0.5856209993362427,
           -0.023499488830566406,
           0.4886244535446167,
           0.8987678289413452,
           0.958511233329773,
           0.8987678289413452,
           0.807091474533081,
           -1.5900251865386963,
           -0.9010846614837646,
           0.5533754825592041,
           -0.20427894592285156,
           0.9287847280502319,
           -0.20427894592285156,
           0.5533754825592041,
           -0.6297619342803955,
           -0.2915523052215576,
           0.8684713840484619,
           0.7447576522827148,
           0.776036262512207,
           1.2382137775421143,
           1.2382137775421143,
           1.0743253231048584,
           -0.2915523052215576,
           -1.2076339721679688,
           1.0743253231048584,
           -1.4355108737945557,
           0.958511233329773,
           -1.3096308708190918,
           0.5533754825592041,
           -0.20427894592285156,
           0.7447576522827148,
           0.5210363864898682,
           1.2382137775421143,
           0.7447576522827148,
           -0.7164273262023926,
           -0.11494565010070801,
           -1.0687370300292969,
           -1.138134241104126,
           0.958511233329773,
           -0.9010846614837646,
           0.6177530288696289,
           1.2382137775421143,
           -1.6112983226776123,
           0.06992363929748535,
           -0.40523242950439453,
           -1.3096308708190918,
           1.017052173614502,
           -0.40523242950439453,
           -0.5732204914093018,
           -1.5436301231384277,
           0.6177530288696289,
           -1.4355108737945557,
           0.42366552352905273,
           -0.43330812454223633,
           0.42366552352905273,
           1.2382137775421143,
           1.1923162937164307
          ]
         },
         "mode": "markers",
         "type": "scatter",
         "x": [
          -1.1722218990325928,
          -1.1722218990325928,
          0.6404194831848145,
          0.5789740085601807,
          -0.004757963586598635,
          0.48680579662323,
          0.8862013816833496,
          0.9476468563079834,
          0.8862013816833496,
          0.7940331697463989,
          -1.6023402214050293,
          -0.9264400005340576,
          0.5482512712478638,
          -0.18909437954425812,
          0.9169241189956665,
          -0.18909437954425812,
          0.5482512712478638,
          -0.6499354243278503,
          -0.2812625765800476,
          0.8554786443710327,
          0.7325876951217651,
          0.763310432434082,
          1.2548742294311523,
          1.2548742294311523,
          1.070537805557251,
          -0.2812625765800476,
          -1.2029446363449097,
          1.070537805557251,
          -1.418003797531128,
          0.9476468563079834,
          -1.2951128482818604,
          0.5482512712478638,
          -0.18909437954425812,
          0.7325876951217651,
          0.5175285339355469,
          1.2548742294311523,
          0.7325876951217651,
          -0.742103636264801,
          -0.09692616760730743,
          -1.080053687095642,
          -1.1414991617202759,
          0.9476468563079834,
          -0.9264400005340576,
          0.6096967458724976,
          1.2548742294311523,
          -1.6330629587173462,
          0.08741024136543274,
          -0.4041535258293152,
          -1.2951128482818604,
          1.0090923309326172,
          -0.4041535258293152,
          -0.5884899497032166,
          -1.5408947467803955,
          0.6096967458724976,
          -1.418003797531128,
          0.4253603219985962,
          -0.4348762631416321,
          0.4253603219985962,
          1.2548742294311523,
          1.2018709182739258
         ],
         "y": [
          0.0007157325744628906,
          0.0007157325744628906,
          -0.009332418441772461,
          -0.006646990776062012,
          0.01874152570962906,
          -0.0018186569213867188,
          -0.012566447257995605,
          -0.01086437702178955,
          -0.012566447257995605,
          -0.013058304786682129,
          -0.012315034866333008,
          -0.02535533905029297,
          -0.005124211311340332,
          0.015184566378593445,
          -0.01186060905456543,
          0.015184566378593445,
          -0.005124211311340332,
          -0.020173490047454834,
          0.01028972864151001,
          -0.0129927396774292,
          -0.012169957160949707,
          -0.012725830078125,
          0.016660451889038086,
          0.016660451889038086,
          -0.003787517547607422,
          0.01028972864151001,
          0.004689335823059082,
          -0.003787517547607422,
          0.017507076263427734,
          -0.01086437702178955,
          0.014518022537231445,
          -0.005124211311340332,
          0.015184566378593445,
          -0.012169957160949707,
          -0.003507852554321289,
          0.016660451889038086,
          -0.012169957160949707,
          -0.025676310062408447,
          0.018019482493400574,
          -0.011316657066345215,
          -0.0033649206161499023,
          -0.01086437702178955,
          -0.02535533905029297,
          -0.008056282997131348,
          0.016660451889038086,
          -0.021764636039733887,
          0.017486602067947388,
          0.0010789036750793457,
          0.014518022537231445,
          -0.007959842681884766,
          0.0010789036750793457,
          -0.015269458293914795,
          0.0027353763580322266,
          -0.008056282997131348,
          0.017507076263427734,
          0.001694798469543457,
          -0.001568138599395752,
          0.001694798469543457,
          0.016660451889038086,
          0.009554624557495117
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Residual depending on label value"
        },
        "xaxis": {
         "title": {
          "text": "Label"
         }
        },
        "yaxis": {
         "title": {
          "text": "Residual"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "type": "histogram",
         "x": [
          -1.1729376316070557,
          -1.1729376316070557,
          0.6497519016265869,
          0.5856209993362427,
          -0.023499488830566406,
          0.4886244535446167,
          0.8987678289413452,
          0.958511233329773,
          0.8987678289413452,
          0.807091474533081,
          -1.5900251865386963,
          -0.9010846614837646,
          0.5533754825592041,
          -0.20427894592285156,
          0.9287847280502319,
          -0.20427894592285156,
          0.5533754825592041,
          -0.6297619342803955,
          -0.2915523052215576,
          0.8684713840484619,
          0.7447576522827148,
          0.776036262512207,
          1.2382137775421143,
          1.2382137775421143,
          1.0743253231048584,
          -0.2915523052215576,
          -1.2076339721679688,
          1.0743253231048584,
          -1.4355108737945557,
          0.958511233329773,
          -1.3096308708190918,
          0.5533754825592041,
          -0.20427894592285156,
          0.7447576522827148,
          0.5210363864898682,
          1.2382137775421143,
          0.7447576522827148,
          -0.7164273262023926,
          -0.11494565010070801,
          -1.0687370300292969,
          -1.138134241104126,
          0.958511233329773,
          -0.9010846614837646,
          0.6177530288696289,
          1.2382137775421143,
          -1.6112983226776123,
          0.06992363929748535,
          -0.40523242950439453,
          -1.3096308708190918,
          1.017052173614502,
          -0.40523242950439453,
          -0.5732204914093018,
          -1.5436301231384277,
          0.6177530288696289,
          -1.4355108737945557,
          0.42366552352905273,
          -0.43330812454223633,
          0.42366552352905273,
          1.2382137775421143,
          1.1923162937164307
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Residual depending on label value"
        },
        "xaxis": {
         "title": {
          "text": "Label"
         }
        },
        "yaxis": {
         "title": {
          "text": "Residual"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_errors_labels_comparison(model:GNN_naive_framework,graph:torch_geometric.data.Data,plot_attention_weights=False):\n",
    "    if plot_attention_weights:\n",
    "        preds, (adj, alpha) = model.predict(graph.x,\n",
    "                                    graph.edge_index,\n",
    "                                    graph.edge_attr,\n",
    "                                    return_attention_weights=True)\n",
    "    else:\n",
    "        preds = model.predict(graph.x,\n",
    "                                    graph.edge_index,\n",
    "                                    graph.edge_attr,\n",
    "                                    return_attention_weights=False)\n",
    "        \n",
    "    preds = np.array(preds.detach().to(\"cpu\"))\n",
    "    preds = np.squeeze(preds)\n",
    "    labels = np.array(participant_graph.y)\n",
    "    labels = np.squeeze(labels)\n",
    "\n",
    "    errors = labels-preds\n",
    "\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x = labels,\n",
    "        y = errors,\n",
    "        mode = \"markers\",\n",
    "        marker=dict(color=preds)\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=\"Residual depending on label value\",\n",
    "        xaxis_title=\"Label\",\n",
    "        yaxis_title=\"Residual\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(\n",
    "        x = preds)\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=\"Residual depending on label value\",\n",
    "        xaxis_title=\"Label\",\n",
    "        yaxis_title=\"Residual\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    if plot_attention_weights:\n",
    "        from torch_geometric.utils import (\n",
    "            add_self_loops,\n",
    "            is_torch_sparse_tensor,\n",
    "            remove_self_loops,\n",
    "            softmax,\n",
    "            to_dense_adj\n",
    "        )\n",
    "\n",
    "        matrix_alpha = to_dense_adj(adj, edge_attr = alpha).cpu().detach()\n",
    "        matrix_alpha = matrix_alpha.squeeze()\n",
    "        fig = px.imshow(matrix_alpha)\n",
    "        fig.update_layout(\n",
    "            title=\"Alpha: the message passing strength between nodes\"\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "plot_errors_labels_comparison(complete_model,participant_graph,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_x_src, param_x_dst = 0.6226500868797302 3384.351806640625\n",
      "params_edge tensor(27.1259, device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "tensor(50.3570, device='cuda:0', grad_fn=<UnbindBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lin_src_params = [param for param in complete_model.update_node_module.lin_src.parameters()][0]\n",
    "lin_dst_params = [param for param in complete_model.update_node_module.lin_dst.parameters()][0]\n",
    "\n",
    "src_att_lin_params = lin_src_params * complete_model.update_node_module.att_src\n",
    "dst_att_lin_params = lin_dst_params * complete_model.update_node_module.att_dst\n",
    "#lin_params.squeeze()\n",
    "print(\"param_x_src, param_x_dst =\", float(src_att_lin_params), float(dst_att_lin_params))\n",
    "\n",
    "\n",
    "lin_edge_params = [param for param in complete_model.update_node_module.lin_edge.parameters()][0]\n",
    "att_lin_edge_params = lin_edge_params * complete_model.update_node_module.att_edge\n",
    "print(\"params_edge\", att_lin_edge_params.squeeze())\n",
    "\n",
    "bias = [param for param in complete_model.update_node_module.bias][0]\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That param_x_dst is that small is a good sign. It means only $\\alpha_{i,i}$ will keep an influence over the prediction of $x'_i$. The param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_id_main = 0\n",
    "edge_mask = participant_graph.edge_index[0,:] == node_id_main\n",
    "edge_mask = edge_mask + participant_graph.edge_index[1,:] == node_id_main\n",
    "edge_index_subgraph = participant_graph.edge_index[:,edge_mask]\n",
    "edge_attr_subgraph = participant_graph.edge_attr[edge_mask]\n",
    "\n",
    "participant_subgraph = Data(\n",
    "        x = participant_graph.x, \n",
    "        edge_index = edge_index_subgraph,\n",
    "        edge_attr = edge_attr_subgraph,\n",
    "        y = participant_graph.x, \n",
    "        train_mask = participant_graph.train_mask, \n",
    "        val_mask = participant_graph.val_mask\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 21.],\n",
       "        [ 21.],\n",
       "        [ 80.],\n",
       "        [ 78.],\n",
       "        [ 59.],\n",
       "        [ 75.],\n",
       "        [ 88.],\n",
       "        [ 90.],\n",
       "        [ 88.],\n",
       "        [ 85.],\n",
       "        [  7.],\n",
       "        [ 29.],\n",
       "        [ 77.],\n",
       "        [ 53.],\n",
       "        [ 89.],\n",
       "        [ 53.],\n",
       "        [ 77.],\n",
       "        [ 38.],\n",
       "        [ 50.],\n",
       "        [ 87.],\n",
       "        [ 83.],\n",
       "        [ 84.],\n",
       "        [100.],\n",
       "        [100.],\n",
       "        [ 94.],\n",
       "        [ 50.],\n",
       "        [ 20.],\n",
       "        [ 94.],\n",
       "        [ 13.],\n",
       "        [ 90.],\n",
       "        [ 17.],\n",
       "        [ 77.],\n",
       "        [ 53.],\n",
       "        [ 83.],\n",
       "        [ 76.],\n",
       "        [100.],\n",
       "        [ 83.],\n",
       "        [ 35.],\n",
       "        [ 56.],\n",
       "        [ 24.],\n",
       "        [ 22.],\n",
       "        [ 90.],\n",
       "        [ 29.],\n",
       "        [ 79.],\n",
       "        [100.],\n",
       "        [  6.],\n",
       "        [ 62.],\n",
       "        [ 46.],\n",
       "        [ 17.],\n",
       "        [ 92.],\n",
       "        [ 46.],\n",
       "        [ 40.],\n",
       "        [  9.],\n",
       "        [ 79.],\n",
       "        [ 13.],\n",
       "        [ 73.],\n",
       "        [ 45.],\n",
       "        [ 73.],\n",
       "        [100.],\n",
       "        [100.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participant_subgraph.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[60.7998],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570],\n",
       "        [50.3570]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_model.predict(node_attr=participant_subgraph.x,\n",
    "                       edge_index=participant_subgraph.edge_index,\n",
    "                       edge_attr=participant_subgraph.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[61.5520],\n",
      "        [16.1193],\n",
      "        [50.0500],\n",
      "        [48.8998],\n",
      "        [37.9730],\n",
      "        [47.1745],\n",
      "        [54.6508],\n",
      "        [55.8010],\n",
      "        [54.6508],\n",
      "        [52.9255],\n",
      "        [ 8.0679],\n",
      "        [20.7201],\n",
      "        [48.3247],\n",
      "        [34.5224],\n",
      "        [55.2259],\n",
      "        [34.5224],\n",
      "        [48.3247],\n",
      "        [25.8959],\n",
      "        [32.7971],\n",
      "        [54.0757],\n",
      "        [51.7753],\n",
      "        [52.3504],\n",
      "        [61.5520],\n",
      "        [61.5520],\n",
      "        [58.1014],\n",
      "        [32.7971],\n",
      "        [15.5442],\n",
      "        [58.1014],\n",
      "        [11.5185],\n",
      "        [55.8010],\n",
      "        [13.8189],\n",
      "        [48.3247],\n",
      "        [34.5224],\n",
      "        [51.7753],\n",
      "        [47.7496],\n",
      "        [61.5520],\n",
      "        [51.7753],\n",
      "        [24.1707],\n",
      "        [36.2477],\n",
      "        [17.8446],\n",
      "        [16.6944],\n",
      "        [55.8010],\n",
      "        [20.7201],\n",
      "        [49.4749],\n",
      "        [61.5520],\n",
      "        [ 7.4928],\n",
      "        [39.6983],\n",
      "        [30.4967],\n",
      "        [13.8189],\n",
      "        [56.9512],\n",
      "        [30.4967],\n",
      "        [27.0461],\n",
      "        [ 9.2181],\n",
      "        [49.4749],\n",
      "        [11.5185],\n",
      "        [46.0243],\n",
      "        [29.9216],\n",
      "        [46.0243],\n",
      "        [61.5520],\n",
      "        [61.5520]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "(tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
      "         37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
      "         55, 56, 57, 58, 59,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,\n",
      "         13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
      "         31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48,\n",
      "         49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,\n",
      "         13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
      "         31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48,\n",
      "         49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]], device='cuda:0'), tensor([[0.0000e+00],\n",
      "        [4.8577e-41],\n",
      "        [4.2039e-45],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [3.2460e-25],\n",
      "        [2.9721e-21],\n",
      "        [2.4658e-25],\n",
      "        [3.0930e-31],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [2.2045e-23],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [3.5304e-27],\n",
      "        [3.0677e-35],\n",
      "        [4.3514e-33],\n",
      "        [1.4563e-01],\n",
      "        [1.8648e-01],\n",
      "        [1.8861e-13],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [2.1064e-13],\n",
      "        [0.0000e+00],\n",
      "        [2.4069e-21],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [2.3206e-35],\n",
      "        [0.0000e+00],\n",
      "        [1.6531e-01],\n",
      "        [4.1804e-35],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [1.5302e-21],\n",
      "        [0.0000e+00],\n",
      "        [3.9657e-43],\n",
      "        [1.7270e-01],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [2.9667e-17],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [5.5491e-43],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [1.6707e-01],\n",
      "        [1.6281e-01],\n",
      "        [0.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00]], device='cuda:0', grad_fn=<DivBackward0>))\n"
     ]
    }
   ],
   "source": [
    "res = complete_model.update_node_module(participant_subgraph.x.to(device), participant_subgraph.edge_index.to(device), participant_subgraph.edge_attr.to(device),return_attention_weights=True)\n",
    "\n",
    "for r in res:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les derniers qui nous intÃ©ressent sont bien Ã  1 tandis que les autres sont Ã  0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== start training ==\n",
      "epoch: 1/10000,\n",
      " train_loss: 5378.2949,\n",
      " train_mae: 59.1030,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 2/10000,\n",
      " train_loss: 4373.0112,\n",
      " train_mae: 52.4354,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3/10000,\n",
      " train_loss: 3608.8101,\n",
      " train_mae: 46.4671,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 4/10000,\n",
      " train_loss: 2933.8242,\n",
      " train_mae: 41.5882,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5/10000,\n",
      " train_loss: 2374.7063,\n",
      " train_mae: 37.6855,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 6/10000,\n",
      " train_loss: 1913.9542,\n",
      " train_mae: 34.3410,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 7/10000,\n",
      " train_loss: 1548.0836,\n",
      " train_mae: 31.4919,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 8/10000,\n",
      " train_loss: 1272.1437,\n",
      " train_mae: 29.1906,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 9/10000,\n",
      " train_loss: 1079.2406,\n",
      " train_mae: 27.6272,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 10/10000,\n",
      " train_loss: 960.4120,\n",
      " train_mae: 26.8701,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 11/10000,\n",
      " train_loss: 904.7333,\n",
      " train_mae: 26.4361,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 12/10000,\n",
      " train_loss: 899.6686,\n",
      " train_mae: 26.1699,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 13/10000,\n",
      " train_loss: 931.6901,\n",
      " train_mae: 25.9395,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 14/10000,\n",
      " train_loss: 987.1213,\n",
      " train_mae: 25.8003,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 15/10000,\n",
      " train_loss: 1053.0928,\n",
      " train_mae: 25.8521,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 16/10000,\n",
      " train_loss: 1118.4487,\n",
      " train_mae: 26.0728,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 17/10000,\n",
      " train_loss: 1174.4438,\n",
      " train_mae: 26.3219,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 18/10000,\n",
      " train_loss: 1215.1248,\n",
      " train_mae: 26.4862,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 19/10000,\n",
      " train_loss: 1237.3660,\n",
      " train_mae: 26.5097,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 20/10000,\n",
      " train_loss: 1240.6007,\n",
      " train_mae: 26.4055,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 21/10000,\n",
      " train_loss: 1226.3479,\n",
      " train_mae: 26.2112,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 22/10000,\n",
      " train_loss: 1197.6333,\n",
      " train_mae: 25.9928,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 23/10000,\n",
      " train_loss: 1158.4042,\n",
      " train_mae: 25.8440,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 24/10000,\n",
      " train_loss: 1112.9894,\n",
      " train_mae: 25.8067,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 25/10000,\n",
      " train_loss: 1065.6417,\n",
      " train_mae: 25.8414,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 26/10000,\n",
      " train_loss: 1020.1714,\n",
      " train_mae: 25.9701,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 27/10000,\n",
      " train_loss: 979.6805,\n",
      " train_mae: 26.1025,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 28/10000,\n",
      " train_loss: 946.3939,\n",
      " train_mae: 26.2349,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 29/10000,\n",
      " train_loss: 921.5851,\n",
      " train_mae: 26.3641,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 30/10000,\n",
      " train_loss: 905.5898,\n",
      " train_mae: 26.4870,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 31/10000,\n",
      " train_loss: 897.8991,\n",
      " train_mae: 26.6469,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 32/10000,\n",
      " train_loss: 897.3178,\n",
      " train_mae: 26.8023,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 33/10000,\n",
      " train_loss: 902.1699,\n",
      " train_mae: 26.9827,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 34/10000,\n",
      " train_loss: 910.5287,\n",
      " train_mae: 27.1339,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 35/10000,\n",
      " train_loss: 920.4459,\n",
      " train_mae: 27.2673,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 36/10000,\n",
      " train_loss: 930.1548,\n",
      " train_mae: 27.3756,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 37/10000,\n",
      " train_loss: 938.2309,\n",
      " train_mae: 27.4430,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 38/10000,\n",
      " train_loss: 943.6913,\n",
      " train_mae: 27.4707,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 39/10000,\n",
      " train_loss: 946.0344,\n",
      " train_mae: 27.4611,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 40/10000,\n",
      " train_loss: 945.2186,\n",
      " train_mae: 27.4176,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 41/10000,\n",
      " train_loss: 941.5944,\n",
      " train_mae: 27.3443,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 42/10000,\n",
      " train_loss: 935.8018,\n",
      " train_mae: 27.2457,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 43/10000,\n",
      " train_loss: 928.6495,\n",
      " train_mae: 27.1413,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 44/10000,\n",
      " train_loss: 920.9962,\n",
      " train_mae: 27.0349,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 45/10000,\n",
      " train_loss: 913.6401,\n",
      " train_mae: 26.9209,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 46/10000,\n",
      " train_loss: 907.2347,\n",
      " train_mae: 26.8038,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 47/10000,\n",
      " train_loss: 902.2318,\n",
      " train_mae: 26.7168,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 48/10000,\n",
      " train_loss: 898.8561,\n",
      " train_mae: 26.6331,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 49/10000,\n",
      " train_loss: 897.1130,\n",
      " train_mae: 26.5551,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 50/10000,\n",
      " train_loss: 896.8178,\n",
      " train_mae: 26.4941,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 51/10000,\n",
      " train_loss: 897.6477,\n",
      " train_mae: 26.4541,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 52/10000,\n",
      " train_loss: 899.2006,\n",
      " train_mae: 26.4213,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 53/10000,\n",
      " train_loss: 901.0580,\n",
      " train_mae: 26.3960,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 54/10000,\n",
      " train_loss: 902.8398,\n",
      " train_mae: 26.3786,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 55/10000,\n",
      " train_loss: 904.2454,\n",
      " train_mae: 26.3690,\n",
      " epoch_time_duration: 0.0025\n",
      "\n",
      "epoch: 56/10000,\n",
      " train_loss: 905.0812,\n",
      " train_mae: 26.3669,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 57/10000,\n",
      " train_loss: 905.2675,\n",
      " train_mae: 26.3717,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 58/10000,\n",
      " train_loss: 904.8304,\n",
      " train_mae: 26.3827,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 59/10000,\n",
      " train_loss: 903.8821,\n",
      " train_mae: 26.3989,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 60/10000,\n",
      " train_loss: 902.5900,\n",
      " train_mae: 26.4194,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 61/10000,\n",
      " train_loss: 901.1459,\n",
      " train_mae: 26.4429,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 62/10000,\n",
      " train_loss: 899.7350,\n",
      " train_mae: 26.4684,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 63/10000,\n",
      " train_loss: 898.5111,\n",
      " train_mae: 26.4948,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 64/10000,\n",
      " train_loss: 897.5797,\n",
      " train_mae: 26.5260,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 65/10000,\n",
      " train_loss: 896.9905,\n",
      " train_mae: 26.5640,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 66/10000,\n",
      " train_loss: 896.7395,\n",
      " train_mae: 26.5989,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 67/10000,\n",
      " train_loss: 896.7774,\n",
      " train_mae: 26.6297,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 68/10000,\n",
      " train_loss: 897.0237,\n",
      " train_mae: 26.6555,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 69/10000,\n",
      " train_loss: 897.3837,\n",
      " train_mae: 26.6757,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 70/10000,\n",
      " train_loss: 897.7636,\n",
      " train_mae: 26.6900,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 71/10000,\n",
      " train_loss: 898.0848,\n",
      " train_mae: 26.6984,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 72/10000,\n",
      " train_loss: 898.2922,\n",
      " train_mae: 26.7011,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 73/10000,\n",
      " train_loss: 898.3590,\n",
      " train_mae: 26.6984,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 74/10000,\n",
      " train_loss: 898.2855,\n",
      " train_mae: 26.6909,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 75/10000,\n",
      " train_loss: 898.0945,\n",
      " train_mae: 26.6793,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 76/10000,\n",
      " train_loss: 897.8248,\n",
      " train_mae: 26.6645,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 77/10000,\n",
      " train_loss: 897.5209,\n",
      " train_mae: 26.6474,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 78/10000,\n",
      " train_loss: 897.2268,\n",
      " train_mae: 26.6288,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 79/10000,\n",
      " train_loss: 896.9779,\n",
      " train_mae: 26.6098,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 80/10000,\n",
      " train_loss: 896.7975,\n",
      " train_mae: 26.5910,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 81/10000,\n",
      " train_loss: 896.6945,\n",
      " train_mae: 26.5733,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 82/10000,\n",
      " train_loss: 896.6647,\n",
      " train_mae: 26.5574,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 83/10000,\n",
      " train_loss: 896.6934,\n",
      " train_mae: 26.5437,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 84/10000,\n",
      " train_loss: 896.7594,\n",
      " train_mae: 26.5327,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 85/10000,\n",
      " train_loss: 896.8398,\n",
      " train_mae: 26.5246,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 86/10000,\n",
      " train_loss: 896.9133,\n",
      " train_mae: 26.5195,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 87/10000,\n",
      " train_loss: 896.9642,\n",
      " train_mae: 26.5174,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 88/10000,\n",
      " train_loss: 896.9833,\n",
      " train_mae: 26.5180,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 89/10000,\n",
      " train_loss: 896.9687,\n",
      " train_mae: 26.5211,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 90/10000,\n",
      " train_loss: 896.9245,\n",
      " train_mae: 26.5263,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 91/10000,\n",
      " train_loss: 896.8597,\n",
      " train_mae: 26.5332,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 92/10000,\n",
      " train_loss: 896.7845,\n",
      " train_mae: 26.5414,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 93/10000,\n",
      " train_loss: 896.7097,\n",
      " train_mae: 26.5502,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 94/10000,\n",
      " train_loss: 896.6436,\n",
      " train_mae: 26.5593,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 95/10000,\n",
      " train_loss: 896.5905,\n",
      " train_mae: 26.5681,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 96/10000,\n",
      " train_loss: 896.5512,\n",
      " train_mae: 26.5762,\n",
      " epoch_time_duration: 0.0025\n",
      "\n",
      "epoch: 97/10000,\n",
      " train_loss: 896.5206,\n",
      " train_mae: 26.5831,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 98/10000,\n",
      " train_loss: 896.4871,\n",
      " train_mae: 26.5882,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 99/10000,\n",
      " train_loss: 896.4255,\n",
      " train_mae: 26.5903,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 100/10000,\n",
      " train_loss: 896.2666,\n",
      " train_mae: 26.5845,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 101/10000,\n",
      " train_loss: 895.7122,\n",
      " train_mae: 26.5432,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 102/10000,\n",
      " train_loss: 893.1200,\n",
      " train_mae: 26.4135,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 103/10000,\n",
      " train_loss: 885.1932,\n",
      " train_mae: 26.2009,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 104/10000,\n",
      " train_loss: 873.7490,\n",
      " train_mae: 25.9632,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 105/10000,\n",
      " train_loss: 870.0516,\n",
      " train_mae: 25.8510,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 106/10000,\n",
      " train_loss: 870.5014,\n",
      " train_mae: 25.7739,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 107/10000,\n",
      " train_loss: 867.5229,\n",
      " train_mae: 25.6307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 108/10000,\n",
      " train_loss: 861.7965,\n",
      " train_mae: 25.5927,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 109/10000,\n",
      " train_loss: 864.7092,\n",
      " train_mae: 25.6561,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 110/10000,\n",
      " train_loss: 864.0833,\n",
      " train_mae: 25.5727,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 111/10000,\n",
      " train_loss: 854.4782,\n",
      " train_mae: 25.8484,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 112/10000,\n",
      " train_loss: 856.5946,\n",
      " train_mae: 25.9758,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 113/10000,\n",
      " train_loss: 859.1998,\n",
      " train_mae: 26.0241,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 114/10000,\n",
      " train_loss: 859.7930,\n",
      " train_mae: 25.9942,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 115/10000,\n",
      " train_loss: 857.7511,\n",
      " train_mae: 25.8220,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 116/10000,\n",
      " train_loss: 853.3597,\n",
      " train_mae: 25.8322,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 117/10000,\n",
      " train_loss: 856.7487,\n",
      " train_mae: 25.8709,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 118/10000,\n",
      " train_loss: 858.5410,\n",
      " train_mae: 25.7909,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 119/10000,\n",
      " train_loss: 854.1005,\n",
      " train_mae: 25.9063,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 120/10000,\n",
      " train_loss: 854.8033,\n",
      " train_mae: 25.9781,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 121/10000,\n",
      " train_loss: 856.6354,\n",
      " train_mae: 25.9019,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 122/10000,\n",
      " train_loss: 854.5654,\n",
      " train_mae: 25.7416,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 123/10000,\n",
      " train_loss: 852.1403,\n",
      " train_mae: 25.7391,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 124/10000,\n",
      " train_loss: 853.4782,\n",
      " train_mae: 25.7251,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 125/10000,\n",
      " train_loss: 853.5333,\n",
      " train_mae: 25.6410,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 126/10000,\n",
      " train_loss: 851.4791,\n",
      " train_mae: 25.7086,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 127/10000,\n",
      " train_loss: 851.7664,\n",
      " train_mae: 25.7579,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 128/10000,\n",
      " train_loss: 852.8923,\n",
      " train_mae: 25.6961,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 129/10000,\n",
      " train_loss: 852.1617,\n",
      " train_mae: 25.5788,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 130/10000,\n",
      " train_loss: 851.4123,\n",
      " train_mae: 25.6017,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 131/10000,\n",
      " train_loss: 852.1057,\n",
      " train_mae: 25.6118,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 132/10000,\n",
      " train_loss: 852.4152,\n",
      " train_mae: 25.5874,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 133/10000,\n",
      " train_loss: 851.5861,\n",
      " train_mae: 25.5914,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 134/10000,\n",
      " train_loss: 851.1813,\n",
      " train_mae: 25.6651,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 135/10000,\n",
      " train_loss: 851.5658,\n",
      " train_mae: 25.6731,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 136/10000,\n",
      " train_loss: 851.4209,\n",
      " train_mae: 25.6224,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 137/10000,\n",
      " train_loss: 850.7806,\n",
      " train_mae: 25.6233,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 138/10000,\n",
      " train_loss: 850.7025,\n",
      " train_mae: 25.6531,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 139/10000,\n",
      " train_loss: 850.9678,\n",
      " train_mae: 25.6601,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 140/10000,\n",
      " train_loss: 850.8686,\n",
      " train_mae: 25.6491,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 141/10000,\n",
      " train_loss: 850.5676,\n",
      " train_mae: 25.6560,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 142/10000,\n",
      " train_loss: 850.5908,\n",
      " train_mae: 25.6860,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 143/10000,\n",
      " train_loss: 850.7903,\n",
      " train_mae: 25.6815,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 144/10000,\n",
      " train_loss: 850.7383,\n",
      " train_mae: 25.6509,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 145/10000,\n",
      " train_loss: 850.5205,\n",
      " train_mae: 25.6593,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 146/10000,\n",
      " train_loss: 850.4570,\n",
      " train_mae: 25.6671,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 147/10000,\n",
      " train_loss: 850.5292,\n",
      " train_mae: 25.6630,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 148/10000,\n",
      " train_loss: 850.4819,\n",
      " train_mae: 25.6491,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 149/10000,\n",
      " train_loss: 850.3145,\n",
      " train_mae: 25.6305,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 150/10000,\n",
      " train_loss: 850.2391,\n",
      " train_mae: 25.6143,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 151/10000,\n",
      " train_loss: 850.2735,\n",
      " train_mae: 25.6134,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 152/10000,\n",
      " train_loss: 850.2885,\n",
      " train_mae: 25.6055,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 153/10000,\n",
      " train_loss: 850.2267,\n",
      " train_mae: 25.6128,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 154/10000,\n",
      " train_loss: 850.1678,\n",
      " train_mae: 25.6198,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 155/10000,\n",
      " train_loss: 850.1709,\n",
      " train_mae: 25.6237,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 156/10000,\n",
      " train_loss: 850.1974,\n",
      " train_mae: 25.6238,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 157/10000,\n",
      " train_loss: 850.1793,\n",
      " train_mae: 25.6209,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 158/10000,\n",
      " train_loss: 850.1208,\n",
      " train_mae: 25.6169,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 159/10000,\n",
      " train_loss: 850.0800,\n",
      " train_mae: 25.6141,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 160/10000,\n",
      " train_loss: 850.0727,\n",
      " train_mae: 25.6150,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 161/10000,\n",
      " train_loss: 850.0664,\n",
      " train_mae: 25.6203,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 162/10000,\n",
      " train_loss: 850.0370,\n",
      " train_mae: 25.6283,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 163/10000,\n",
      " train_loss: 849.9974,\n",
      " train_mae: 25.6365,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 164/10000,\n",
      " train_loss: 849.9750,\n",
      " train_mae: 25.6428,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 165/10000,\n",
      " train_loss: 849.9756,\n",
      " train_mae: 25.6463,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 166/10000,\n",
      " train_loss: 849.9736,\n",
      " train_mae: 25.6468,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 167/10000,\n",
      " train_loss: 849.9525,\n",
      " train_mae: 25.6451,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 168/10000,\n",
      " train_loss: 849.9280,\n",
      " train_mae: 25.6426,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 169/10000,\n",
      " train_loss: 849.9171,\n",
      " train_mae: 25.6407,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 170/10000,\n",
      " train_loss: 849.9117,\n",
      " train_mae: 25.6400,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 171/10000,\n",
      " train_loss: 849.8975,\n",
      " train_mae: 25.6405,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 172/10000,\n",
      " train_loss: 849.8740,\n",
      " train_mae: 25.6414,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 173/10000,\n",
      " train_loss: 849.8530,\n",
      " train_mae: 25.6419,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 174/10000,\n",
      " train_loss: 849.8417,\n",
      " train_mae: 25.6414,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 175/10000,\n",
      " train_loss: 849.8331,\n",
      " train_mae: 25.6398,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 176/10000,\n",
      " train_loss: 849.8188,\n",
      " train_mae: 25.6375,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 177/10000,\n",
      " train_loss: 849.8013,\n",
      " train_mae: 25.6352,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 178/10000,\n",
      " train_loss: 849.7888,\n",
      " train_mae: 25.6336,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 179/10000,\n",
      " train_loss: 849.7803,\n",
      " train_mae: 25.6332,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 180/10000,\n",
      " train_loss: 849.7695,\n",
      " train_mae: 25.6339,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 181/10000,\n",
      " train_loss: 849.7550,\n",
      " train_mae: 25.6355,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 182/10000,\n",
      " train_loss: 849.7389,\n",
      " train_mae: 25.6373,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 183/10000,\n",
      " train_loss: 849.7258,\n",
      " train_mae: 25.6389,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 184/10000,\n",
      " train_loss: 849.7150,\n",
      " train_mae: 25.6399,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 185/10000,\n",
      " train_loss: 849.7018,\n",
      " train_mae: 25.6404,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 186/10000,\n",
      " train_loss: 849.6865,\n",
      " train_mae: 25.6407,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 187/10000,\n",
      " train_loss: 849.6728,\n",
      " train_mae: 25.6410,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 188/10000,\n",
      " train_loss: 849.6615,\n",
      " train_mae: 25.6415,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 189/10000,\n",
      " train_loss: 849.6494,\n",
      " train_mae: 25.6423,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 190/10000,\n",
      " train_loss: 849.6364,\n",
      " train_mae: 25.6432,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 191/10000,\n",
      " train_loss: 849.6221,\n",
      " train_mae: 25.6439,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 192/10000,\n",
      " train_loss: 849.6096,\n",
      " train_mae: 25.6441,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 193/10000,\n",
      " train_loss: 849.5977,\n",
      " train_mae: 25.6438,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 194/10000,\n",
      " train_loss: 849.5837,\n",
      " train_mae: 25.6431,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 195/10000,\n",
      " train_loss: 849.5697,\n",
      " train_mae: 25.6423,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 196/10000,\n",
      " train_loss: 849.5560,\n",
      " train_mae: 25.6415,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 197/10000,\n",
      " train_loss: 849.5429,\n",
      " train_mae: 25.6410,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 198/10000,\n",
      " train_loss: 849.5292,\n",
      " train_mae: 25.6407,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 199/10000,\n",
      " train_loss: 849.5147,\n",
      " train_mae: 25.6406,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 200/10000,\n",
      " train_loss: 849.5012,\n",
      " train_mae: 25.6405,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 201/10000,\n",
      " train_loss: 849.4873,\n",
      " train_mae: 25.6403,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 202/10000,\n",
      " train_loss: 849.4738,\n",
      " train_mae: 25.6401,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 203/10000,\n",
      " train_loss: 849.4594,\n",
      " train_mae: 25.6398,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 204/10000,\n",
      " train_loss: 849.4448,\n",
      " train_mae: 25.6397,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 205/10000,\n",
      " train_loss: 849.4305,\n",
      " train_mae: 25.6397,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 206/10000,\n",
      " train_loss: 849.4160,\n",
      " train_mae: 25.6400,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 207/10000,\n",
      " train_loss: 849.4007,\n",
      " train_mae: 25.6403,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 208/10000,\n",
      " train_loss: 849.3856,\n",
      " train_mae: 25.6406,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 209/10000,\n",
      " train_loss: 849.3711,\n",
      " train_mae: 25.6408,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 210/10000,\n",
      " train_loss: 849.3560,\n",
      " train_mae: 25.6408,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 211/10000,\n",
      " train_loss: 849.3405,\n",
      " train_mae: 25.6407,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 212/10000,\n",
      " train_loss: 849.3253,\n",
      " train_mae: 25.6405,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 213/10000,\n",
      " train_loss: 849.3098,\n",
      " train_mae: 25.6404,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 214/10000,\n",
      " train_loss: 849.2939,\n",
      " train_mae: 25.6402,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 215/10000,\n",
      " train_loss: 849.2778,\n",
      " train_mae: 25.6401,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 216/10000,\n",
      " train_loss: 849.2621,\n",
      " train_mae: 25.6398,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 217/10000,\n",
      " train_loss: 849.2457,\n",
      " train_mae: 25.6394,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 218/10000,\n",
      " train_loss: 849.2291,\n",
      " train_mae: 25.6390,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 219/10000,\n",
      " train_loss: 849.2126,\n",
      " train_mae: 25.6385,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 220/10000,\n",
      " train_loss: 849.1959,\n",
      " train_mae: 25.6381,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 221/10000,\n",
      " train_loss: 849.1791,\n",
      " train_mae: 25.6378,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 222/10000,\n",
      " train_loss: 849.1620,\n",
      " train_mae: 25.6375,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 223/10000,\n",
      " train_loss: 849.1448,\n",
      " train_mae: 25.6372,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 224/10000,\n",
      " train_loss: 849.1271,\n",
      " train_mae: 25.6370,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 225/10000,\n",
      " train_loss: 849.1099,\n",
      " train_mae: 25.6367,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 226/10000,\n",
      " train_loss: 849.0915,\n",
      " train_mae: 25.6364,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 227/10000,\n",
      " train_loss: 849.0737,\n",
      " train_mae: 25.6361,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 228/10000,\n",
      " train_loss: 849.0557,\n",
      " train_mae: 25.6359,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 229/10000,\n",
      " train_loss: 849.0374,\n",
      " train_mae: 25.6357,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 230/10000,\n",
      " train_loss: 849.0189,\n",
      " train_mae: 25.6355,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 231/10000,\n",
      " train_loss: 849.0002,\n",
      " train_mae: 25.6353,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 232/10000,\n",
      " train_loss: 848.9812,\n",
      " train_mae: 25.6350,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 233/10000,\n",
      " train_loss: 848.9623,\n",
      " train_mae: 25.6347,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 234/10000,\n",
      " train_loss: 848.9433,\n",
      " train_mae: 25.6344,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 235/10000,\n",
      " train_loss: 848.9235,\n",
      " train_mae: 25.6340,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 236/10000,\n",
      " train_loss: 848.9047,\n",
      " train_mae: 25.6337,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 237/10000,\n",
      " train_loss: 848.8844,\n",
      " train_mae: 25.6333,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 238/10000,\n",
      " train_loss: 848.8645,\n",
      " train_mae: 25.6329,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 239/10000,\n",
      " train_loss: 848.8450,\n",
      " train_mae: 25.6324,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 240/10000,\n",
      " train_loss: 848.8243,\n",
      " train_mae: 25.6320,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 241/10000,\n",
      " train_loss: 848.8040,\n",
      " train_mae: 25.6315,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 242/10000,\n",
      " train_loss: 848.7834,\n",
      " train_mae: 25.6311,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 243/10000,\n",
      " train_loss: 848.7628,\n",
      " train_mae: 25.6307,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 244/10000,\n",
      " train_loss: 848.7416,\n",
      " train_mae: 25.6303,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 245/10000,\n",
      " train_loss: 848.7206,\n",
      " train_mae: 25.6299,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 246/10000,\n",
      " train_loss: 848.6993,\n",
      " train_mae: 25.6295,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 247/10000,\n",
      " train_loss: 848.6780,\n",
      " train_mae: 25.6290,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 248/10000,\n",
      " train_loss: 848.6562,\n",
      " train_mae: 25.6286,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 249/10000,\n",
      " train_loss: 848.6345,\n",
      " train_mae: 25.6283,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 250/10000,\n",
      " train_loss: 848.6125,\n",
      " train_mae: 25.6279,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 251/10000,\n",
      " train_loss: 848.5901,\n",
      " train_mae: 25.6275,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 252/10000,\n",
      " train_loss: 848.5680,\n",
      " train_mae: 25.6270,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 253/10000,\n",
      " train_loss: 848.5456,\n",
      " train_mae: 25.6266,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 254/10000,\n",
      " train_loss: 848.5232,\n",
      " train_mae: 25.6262,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 255/10000,\n",
      " train_loss: 848.5009,\n",
      " train_mae: 25.6257,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 256/10000,\n",
      " train_loss: 848.4779,\n",
      " train_mae: 25.6253,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 257/10000,\n",
      " train_loss: 848.4548,\n",
      " train_mae: 25.6248,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 258/10000,\n",
      " train_loss: 848.4319,\n",
      " train_mae: 25.6244,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 259/10000,\n",
      " train_loss: 848.4088,\n",
      " train_mae: 25.6239,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 260/10000,\n",
      " train_loss: 848.3853,\n",
      " train_mae: 25.6234,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 261/10000,\n",
      " train_loss: 848.3620,\n",
      " train_mae: 25.6229,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 262/10000,\n",
      " train_loss: 848.3382,\n",
      " train_mae: 25.6224,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 263/10000,\n",
      " train_loss: 848.3146,\n",
      " train_mae: 25.6219,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 264/10000,\n",
      " train_loss: 848.2905,\n",
      " train_mae: 25.6214,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 265/10000,\n",
      " train_loss: 848.2667,\n",
      " train_mae: 25.6210,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 266/10000,\n",
      " train_loss: 848.2430,\n",
      " train_mae: 25.6205,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 267/10000,\n",
      " train_loss: 848.2187,\n",
      " train_mae: 25.6200,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 268/10000,\n",
      " train_loss: 848.1947,\n",
      " train_mae: 25.6195,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 269/10000,\n",
      " train_loss: 848.1706,\n",
      " train_mae: 25.6191,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 270/10000,\n",
      " train_loss: 848.1458,\n",
      " train_mae: 25.6186,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 271/10000,\n",
      " train_loss: 848.1213,\n",
      " train_mae: 25.6181,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 272/10000,\n",
      " train_loss: 848.0968,\n",
      " train_mae: 25.6176,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 273/10000,\n",
      " train_loss: 848.0722,\n",
      " train_mae: 25.6171,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 274/10000,\n",
      " train_loss: 848.0474,\n",
      " train_mae: 25.6167,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 275/10000,\n",
      " train_loss: 848.0225,\n",
      " train_mae: 25.6162,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 276/10000,\n",
      " train_loss: 847.9977,\n",
      " train_mae: 25.6157,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 277/10000,\n",
      " train_loss: 847.9730,\n",
      " train_mae: 25.6152,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 278/10000,\n",
      " train_loss: 847.9478,\n",
      " train_mae: 25.6147,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 279/10000,\n",
      " train_loss: 847.9231,\n",
      " train_mae: 25.6142,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 280/10000,\n",
      " train_loss: 847.8984,\n",
      " train_mae: 25.6137,\n",
      " epoch_time_duration: 0.0093\n",
      "\n",
      "epoch: 281/10000,\n",
      " train_loss: 847.8730,\n",
      " train_mae: 25.6132,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 282/10000,\n",
      " train_loss: 847.8479,\n",
      " train_mae: 25.6127,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 283/10000,\n",
      " train_loss: 847.8229,\n",
      " train_mae: 25.6122,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 284/10000,\n",
      " train_loss: 847.7980,\n",
      " train_mae: 25.6117,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 285/10000,\n",
      " train_loss: 847.7729,\n",
      " train_mae: 25.6112,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 286/10000,\n",
      " train_loss: 847.7477,\n",
      " train_mae: 25.6107,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 287/10000,\n",
      " train_loss: 847.7221,\n",
      " train_mae: 25.6103,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 288/10000,\n",
      " train_loss: 847.6971,\n",
      " train_mae: 25.6098,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 289/10000,\n",
      " train_loss: 847.6721,\n",
      " train_mae: 25.6093,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 290/10000,\n",
      " train_loss: 847.6474,\n",
      " train_mae: 25.6088,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 291/10000,\n",
      " train_loss: 847.6217,\n",
      " train_mae: 25.6083,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 292/10000,\n",
      " train_loss: 847.5967,\n",
      " train_mae: 25.6079,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 293/10000,\n",
      " train_loss: 847.5717,\n",
      " train_mae: 25.6074,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 294/10000,\n",
      " train_loss: 847.5466,\n",
      " train_mae: 25.6069,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 295/10000,\n",
      " train_loss: 847.5217,\n",
      " train_mae: 25.6064,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 296/10000,\n",
      " train_loss: 847.4965,\n",
      " train_mae: 25.6059,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 297/10000,\n",
      " train_loss: 847.4718,\n",
      " train_mae: 25.6055,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 298/10000,\n",
      " train_loss: 847.4467,\n",
      " train_mae: 25.6050,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 299/10000,\n",
      " train_loss: 847.4221,\n",
      " train_mae: 25.6045,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 300/10000,\n",
      " train_loss: 847.3972,\n",
      " train_mae: 25.6040,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 301/10000,\n",
      " train_loss: 847.3725,\n",
      " train_mae: 25.6036,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 302/10000,\n",
      " train_loss: 847.3481,\n",
      " train_mae: 25.6031,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 303/10000,\n",
      " train_loss: 847.3234,\n",
      " train_mae: 25.6026,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 304/10000,\n",
      " train_loss: 847.2991,\n",
      " train_mae: 25.6022,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 305/10000,\n",
      " train_loss: 847.2745,\n",
      " train_mae: 25.6017,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 306/10000,\n",
      " train_loss: 847.2503,\n",
      " train_mae: 25.6013,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 307/10000,\n",
      " train_loss: 847.2260,\n",
      " train_mae: 25.6008,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 308/10000,\n",
      " train_loss: 847.2019,\n",
      " train_mae: 25.6004,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 309/10000,\n",
      " train_loss: 847.1777,\n",
      " train_mae: 25.5999,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 310/10000,\n",
      " train_loss: 847.1537,\n",
      " train_mae: 25.5995,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 311/10000,\n",
      " train_loss: 847.1301,\n",
      " train_mae: 25.5990,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 312/10000,\n",
      " train_loss: 847.1063,\n",
      " train_mae: 25.5986,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 313/10000,\n",
      " train_loss: 847.0827,\n",
      " train_mae: 25.5982,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 314/10000,\n",
      " train_loss: 847.0591,\n",
      " train_mae: 25.5977,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 315/10000,\n",
      " train_loss: 847.0358,\n",
      " train_mae: 25.5973,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 316/10000,\n",
      " train_loss: 847.0121,\n",
      " train_mae: 25.5969,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 317/10000,\n",
      " train_loss: 846.9893,\n",
      " train_mae: 25.5964,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 318/10000,\n",
      " train_loss: 846.9661,\n",
      " train_mae: 25.5960,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 319/10000,\n",
      " train_loss: 846.9434,\n",
      " train_mae: 25.5956,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 320/10000,\n",
      " train_loss: 846.9203,\n",
      " train_mae: 25.5952,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 321/10000,\n",
      " train_loss: 846.8975,\n",
      " train_mae: 25.5948,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 322/10000,\n",
      " train_loss: 846.8755,\n",
      " train_mae: 25.5944,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 323/10000,\n",
      " train_loss: 846.8527,\n",
      " train_mae: 25.5940,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 324/10000,\n",
      " train_loss: 846.8305,\n",
      " train_mae: 25.5936,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 325/10000,\n",
      " train_loss: 846.8087,\n",
      " train_mae: 25.5932,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 326/10000,\n",
      " train_loss: 846.7864,\n",
      " train_mae: 25.5928,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 327/10000,\n",
      " train_loss: 846.7649,\n",
      " train_mae: 25.5924,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 328/10000,\n",
      " train_loss: 846.7433,\n",
      " train_mae: 25.5920,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 329/10000,\n",
      " train_loss: 846.7213,\n",
      " train_mae: 25.5916,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 330/10000,\n",
      " train_loss: 846.6998,\n",
      " train_mae: 25.5912,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 331/10000,\n",
      " train_loss: 846.6787,\n",
      " train_mae: 25.5908,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 332/10000,\n",
      " train_loss: 846.6575,\n",
      " train_mae: 25.5904,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 333/10000,\n",
      " train_loss: 846.6367,\n",
      " train_mae: 25.5901,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 334/10000,\n",
      " train_loss: 846.6158,\n",
      " train_mae: 25.5897,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 335/10000,\n",
      " train_loss: 846.5952,\n",
      " train_mae: 25.5893,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 336/10000,\n",
      " train_loss: 846.5748,\n",
      " train_mae: 25.5890,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 337/10000,\n",
      " train_loss: 846.5546,\n",
      " train_mae: 25.5886,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 338/10000,\n",
      " train_loss: 846.5342,\n",
      " train_mae: 25.5882,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 339/10000,\n",
      " train_loss: 846.5142,\n",
      " train_mae: 25.5879,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 340/10000,\n",
      " train_loss: 846.4944,\n",
      " train_mae: 25.5875,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 341/10000,\n",
      " train_loss: 846.4744,\n",
      " train_mae: 25.5872,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 342/10000,\n",
      " train_loss: 846.4550,\n",
      " train_mae: 25.5868,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 343/10000,\n",
      " train_loss: 846.4356,\n",
      " train_mae: 25.5865,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 344/10000,\n",
      " train_loss: 846.4166,\n",
      " train_mae: 25.5861,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 345/10000,\n",
      " train_loss: 846.3973,\n",
      " train_mae: 25.5858,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 346/10000,\n",
      " train_loss: 846.3783,\n",
      " train_mae: 25.5855,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 347/10000,\n",
      " train_loss: 846.3595,\n",
      " train_mae: 25.5851,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 348/10000,\n",
      " train_loss: 846.3411,\n",
      " train_mae: 25.5848,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 349/10000,\n",
      " train_loss: 846.3226,\n",
      " train_mae: 25.5845,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 350/10000,\n",
      " train_loss: 846.3040,\n",
      " train_mae: 25.5842,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 351/10000,\n",
      " train_loss: 846.2860,\n",
      " train_mae: 25.5838,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 352/10000,\n",
      " train_loss: 846.2680,\n",
      " train_mae: 25.5835,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 353/10000,\n",
      " train_loss: 846.2498,\n",
      " train_mae: 25.5832,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 354/10000,\n",
      " train_loss: 846.2322,\n",
      " train_mae: 25.5829,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 355/10000,\n",
      " train_loss: 846.2148,\n",
      " train_mae: 25.5826,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 356/10000,\n",
      " train_loss: 846.1975,\n",
      " train_mae: 25.5823,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 357/10000,\n",
      " train_loss: 846.1801,\n",
      " train_mae: 25.5820,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 358/10000,\n",
      " train_loss: 846.1632,\n",
      " train_mae: 25.5817,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 359/10000,\n",
      " train_loss: 846.1462,\n",
      " train_mae: 25.5814,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 360/10000,\n",
      " train_loss: 846.1292,\n",
      " train_mae: 25.5811,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 361/10000,\n",
      " train_loss: 846.1125,\n",
      " train_mae: 25.5808,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 362/10000,\n",
      " train_loss: 846.0961,\n",
      " train_mae: 25.5805,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 363/10000,\n",
      " train_loss: 846.0801,\n",
      " train_mae: 25.5802,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 364/10000,\n",
      " train_loss: 846.0638,\n",
      " train_mae: 25.5799,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 365/10000,\n",
      " train_loss: 846.0476,\n",
      " train_mae: 25.5796,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 366/10000,\n",
      " train_loss: 846.0317,\n",
      " train_mae: 25.5794,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 367/10000,\n",
      " train_loss: 846.0159,\n",
      " train_mae: 25.5791,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 368/10000,\n",
      " train_loss: 846.0002,\n",
      " train_mae: 25.5788,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 369/10000,\n",
      " train_loss: 845.9847,\n",
      " train_mae: 25.5785,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 370/10000,\n",
      " train_loss: 845.9694,\n",
      " train_mae: 25.5783,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 371/10000,\n",
      " train_loss: 845.9543,\n",
      " train_mae: 25.5780,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 372/10000,\n",
      " train_loss: 845.9391,\n",
      " train_mae: 25.5777,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 373/10000,\n",
      " train_loss: 845.9244,\n",
      " train_mae: 25.5775,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 374/10000,\n",
      " train_loss: 845.9094,\n",
      " train_mae: 25.5772,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 375/10000,\n",
      " train_loss: 845.8948,\n",
      " train_mae: 25.5769,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 376/10000,\n",
      " train_loss: 845.8801,\n",
      " train_mae: 25.5767,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 377/10000,\n",
      " train_loss: 845.8657,\n",
      " train_mae: 25.5764,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 378/10000,\n",
      " train_loss: 845.8515,\n",
      " train_mae: 25.5762,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 379/10000,\n",
      " train_loss: 845.8375,\n",
      " train_mae: 25.5759,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 380/10000,\n",
      " train_loss: 845.8232,\n",
      " train_mae: 25.5757,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 381/10000,\n",
      " train_loss: 845.8095,\n",
      " train_mae: 25.5754,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 382/10000,\n",
      " train_loss: 845.7958,\n",
      " train_mae: 25.5752,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 383/10000,\n",
      " train_loss: 845.7821,\n",
      " train_mae: 25.5749,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 384/10000,\n",
      " train_loss: 845.7686,\n",
      " train_mae: 25.5747,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 385/10000,\n",
      " train_loss: 845.7548,\n",
      " train_mae: 25.5745,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 386/10000,\n",
      " train_loss: 845.7418,\n",
      " train_mae: 25.5742,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 387/10000,\n",
      " train_loss: 845.7286,\n",
      " train_mae: 25.5740,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 388/10000,\n",
      " train_loss: 845.7157,\n",
      " train_mae: 25.5738,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 389/10000,\n",
      " train_loss: 845.7026,\n",
      " train_mae: 25.5735,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 390/10000,\n",
      " train_loss: 845.6900,\n",
      " train_mae: 25.5733,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 391/10000,\n",
      " train_loss: 845.6771,\n",
      " train_mae: 25.5731,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 392/10000,\n",
      " train_loss: 845.6645,\n",
      " train_mae: 25.5729,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 393/10000,\n",
      " train_loss: 845.6523,\n",
      " train_mae: 25.5726,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 394/10000,\n",
      " train_loss: 845.6398,\n",
      " train_mae: 25.5724,\n",
      " epoch_time_duration: 0.0079\n",
      "\n",
      "epoch: 395/10000,\n",
      " train_loss: 845.6275,\n",
      " train_mae: 25.5722,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 396/10000,\n",
      " train_loss: 845.6154,\n",
      " train_mae: 25.5720,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 397/10000,\n",
      " train_loss: 845.6035,\n",
      " train_mae: 25.5718,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 398/10000,\n",
      " train_loss: 845.5912,\n",
      " train_mae: 25.5715,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 399/10000,\n",
      " train_loss: 845.5795,\n",
      " train_mae: 25.5713,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 400/10000,\n",
      " train_loss: 845.5676,\n",
      " train_mae: 25.5711,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 401/10000,\n",
      " train_loss: 845.5561,\n",
      " train_mae: 25.5709,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 402/10000,\n",
      " train_loss: 845.5446,\n",
      " train_mae: 25.5707,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 403/10000,\n",
      " train_loss: 845.5331,\n",
      " train_mae: 25.5705,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 404/10000,\n",
      " train_loss: 845.5217,\n",
      " train_mae: 25.5703,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 405/10000,\n",
      " train_loss: 845.5108,\n",
      " train_mae: 25.5701,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 406/10000,\n",
      " train_loss: 845.4991,\n",
      " train_mae: 25.5698,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 407/10000,\n",
      " train_loss: 845.4882,\n",
      " train_mae: 25.5696,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 408/10000,\n",
      " train_loss: 845.4772,\n",
      " train_mae: 25.5694,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 409/10000,\n",
      " train_loss: 845.4664,\n",
      " train_mae: 25.5692,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 410/10000,\n",
      " train_loss: 845.4556,\n",
      " train_mae: 25.5690,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 411/10000,\n",
      " train_loss: 845.4450,\n",
      " train_mae: 25.5689,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 412/10000,\n",
      " train_loss: 845.4344,\n",
      " train_mae: 25.5687,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 413/10000,\n",
      " train_loss: 845.4238,\n",
      " train_mae: 25.5685,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 414/10000,\n",
      " train_loss: 845.4133,\n",
      " train_mae: 25.5683,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 415/10000,\n",
      " train_loss: 845.4030,\n",
      " train_mae: 25.5681,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 416/10000,\n",
      " train_loss: 845.3928,\n",
      " train_mae: 25.5679,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 417/10000,\n",
      " train_loss: 845.3825,\n",
      " train_mae: 25.5677,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 418/10000,\n",
      " train_loss: 845.3725,\n",
      " train_mae: 25.5675,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 419/10000,\n",
      " train_loss: 845.3625,\n",
      " train_mae: 25.5673,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 420/10000,\n",
      " train_loss: 845.3524,\n",
      " train_mae: 25.5671,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 421/10000,\n",
      " train_loss: 845.3423,\n",
      " train_mae: 25.5669,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 422/10000,\n",
      " train_loss: 845.3329,\n",
      " train_mae: 25.5667,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 423/10000,\n",
      " train_loss: 845.3228,\n",
      " train_mae: 25.5665,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 424/10000,\n",
      " train_loss: 845.3135,\n",
      " train_mae: 25.5664,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 425/10000,\n",
      " train_loss: 845.3037,\n",
      " train_mae: 25.5662,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 426/10000,\n",
      " train_loss: 845.2943,\n",
      " train_mae: 25.5660,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 427/10000,\n",
      " train_loss: 845.2850,\n",
      " train_mae: 25.5658,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 428/10000,\n",
      " train_loss: 845.2755,\n",
      " train_mae: 25.5656,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 429/10000,\n",
      " train_loss: 845.2661,\n",
      " train_mae: 25.5655,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 430/10000,\n",
      " train_loss: 845.2569,\n",
      " train_mae: 25.5653,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 431/10000,\n",
      " train_loss: 845.2477,\n",
      " train_mae: 25.5651,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 432/10000,\n",
      " train_loss: 845.2386,\n",
      " train_mae: 25.5649,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 433/10000,\n",
      " train_loss: 845.2296,\n",
      " train_mae: 25.5647,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 434/10000,\n",
      " train_loss: 845.2205,\n",
      " train_mae: 25.5646,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 435/10000,\n",
      " train_loss: 845.2115,\n",
      " train_mae: 25.5644,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 436/10000,\n",
      " train_loss: 845.2030,\n",
      " train_mae: 25.5642,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 437/10000,\n",
      " train_loss: 845.1940,\n",
      " train_mae: 25.5640,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 438/10000,\n",
      " train_loss: 845.1850,\n",
      " train_mae: 25.5639,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 439/10000,\n",
      " train_loss: 845.1765,\n",
      " train_mae: 25.5637,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 440/10000,\n",
      " train_loss: 845.1682,\n",
      " train_mae: 25.5635,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 441/10000,\n",
      " train_loss: 845.1594,\n",
      " train_mae: 25.5633,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 442/10000,\n",
      " train_loss: 845.1511,\n",
      " train_mae: 25.5632,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 443/10000,\n",
      " train_loss: 845.1426,\n",
      " train_mae: 25.5630,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 444/10000,\n",
      " train_loss: 845.1342,\n",
      " train_mae: 25.5628,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 445/10000,\n",
      " train_loss: 845.1260,\n",
      " train_mae: 25.5627,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 446/10000,\n",
      " train_loss: 845.1175,\n",
      " train_mae: 25.5625,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 447/10000,\n",
      " train_loss: 845.1094,\n",
      " train_mae: 25.5623,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 448/10000,\n",
      " train_loss: 845.1010,\n",
      " train_mae: 25.5621,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 449/10000,\n",
      " train_loss: 845.0928,\n",
      " train_mae: 25.5620,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 450/10000,\n",
      " train_loss: 845.0848,\n",
      " train_mae: 25.5618,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 451/10000,\n",
      " train_loss: 845.0769,\n",
      " train_mae: 25.5616,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 452/10000,\n",
      " train_loss: 845.0688,\n",
      " train_mae: 25.5615,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 453/10000,\n",
      " train_loss: 845.0611,\n",
      " train_mae: 25.5613,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 454/10000,\n",
      " train_loss: 845.0529,\n",
      " train_mae: 25.5611,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 455/10000,\n",
      " train_loss: 845.0451,\n",
      " train_mae: 25.5610,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 456/10000,\n",
      " train_loss: 845.0374,\n",
      " train_mae: 25.5608,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 457/10000,\n",
      " train_loss: 845.0295,\n",
      " train_mae: 25.5606,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 458/10000,\n",
      " train_loss: 845.0218,\n",
      " train_mae: 25.5605,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 459/10000,\n",
      " train_loss: 845.0142,\n",
      " train_mae: 25.5603,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 460/10000,\n",
      " train_loss: 845.0065,\n",
      " train_mae: 25.5601,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 461/10000,\n",
      " train_loss: 844.9990,\n",
      " train_mae: 25.5600,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 462/10000,\n",
      " train_loss: 844.9916,\n",
      " train_mae: 25.5598,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 463/10000,\n",
      " train_loss: 844.9840,\n",
      " train_mae: 25.5597,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 464/10000,\n",
      " train_loss: 844.9766,\n",
      " train_mae: 25.5595,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 465/10000,\n",
      " train_loss: 844.9692,\n",
      " train_mae: 25.5593,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 466/10000,\n",
      " train_loss: 844.9620,\n",
      " train_mae: 25.5592,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 467/10000,\n",
      " train_loss: 844.9543,\n",
      " train_mae: 25.5590,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 468/10000,\n",
      " train_loss: 844.9473,\n",
      " train_mae: 25.5588,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 469/10000,\n",
      " train_loss: 844.9399,\n",
      " train_mae: 25.5587,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 470/10000,\n",
      " train_loss: 844.9326,\n",
      " train_mae: 25.5585,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 471/10000,\n",
      " train_loss: 844.9254,\n",
      " train_mae: 25.5584,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 472/10000,\n",
      " train_loss: 844.9183,\n",
      " train_mae: 25.5582,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 473/10000,\n",
      " train_loss: 844.9111,\n",
      " train_mae: 25.5580,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 474/10000,\n",
      " train_loss: 844.9042,\n",
      " train_mae: 25.5579,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 475/10000,\n",
      " train_loss: 844.8971,\n",
      " train_mae: 25.5577,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 476/10000,\n",
      " train_loss: 844.8901,\n",
      " train_mae: 25.5576,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 477/10000,\n",
      " train_loss: 844.8831,\n",
      " train_mae: 25.5574,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 478/10000,\n",
      " train_loss: 844.8762,\n",
      " train_mae: 25.5572,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 479/10000,\n",
      " train_loss: 844.8692,\n",
      " train_mae: 25.5571,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 480/10000,\n",
      " train_loss: 844.8623,\n",
      " train_mae: 25.5569,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 481/10000,\n",
      " train_loss: 844.8552,\n",
      " train_mae: 25.5567,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 482/10000,\n",
      " train_loss: 844.8486,\n",
      " train_mae: 25.5566,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 483/10000,\n",
      " train_loss: 844.8419,\n",
      " train_mae: 25.5564,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 484/10000,\n",
      " train_loss: 844.8350,\n",
      " train_mae: 25.5563,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 485/10000,\n",
      " train_loss: 844.8283,\n",
      " train_mae: 25.5561,\n",
      " epoch_time_duration: 0.0199\n",
      "\n",
      "epoch: 486/10000,\n",
      " train_loss: 844.8217,\n",
      " train_mae: 25.5559,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 487/10000,\n",
      " train_loss: 844.8149,\n",
      " train_mae: 25.5558,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 488/10000,\n",
      " train_loss: 844.8082,\n",
      " train_mae: 25.5556,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 489/10000,\n",
      " train_loss: 844.8015,\n",
      " train_mae: 25.5555,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 490/10000,\n",
      " train_loss: 844.7952,\n",
      " train_mae: 25.5553,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 491/10000,\n",
      " train_loss: 844.7885,\n",
      " train_mae: 25.5551,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 492/10000,\n",
      " train_loss: 844.7819,\n",
      " train_mae: 25.5550,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 493/10000,\n",
      " train_loss: 844.7753,\n",
      " train_mae: 25.5548,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 494/10000,\n",
      " train_loss: 844.7689,\n",
      " train_mae: 25.5547,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 495/10000,\n",
      " train_loss: 844.7623,\n",
      " train_mae: 25.5545,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 496/10000,\n",
      " train_loss: 844.7559,\n",
      " train_mae: 25.5543,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 497/10000,\n",
      " train_loss: 844.7494,\n",
      " train_mae: 25.5542,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 498/10000,\n",
      " train_loss: 844.7432,\n",
      " train_mae: 25.5540,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 499/10000,\n",
      " train_loss: 844.7366,\n",
      " train_mae: 25.5539,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 500/10000,\n",
      " train_loss: 844.7305,\n",
      " train_mae: 25.5537,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 501/10000,\n",
      " train_loss: 844.7238,\n",
      " train_mae: 25.5536,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 502/10000,\n",
      " train_loss: 844.7175,\n",
      " train_mae: 25.5534,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 503/10000,\n",
      " train_loss: 844.7111,\n",
      " train_mae: 25.5532,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 504/10000,\n",
      " train_loss: 844.7051,\n",
      " train_mae: 25.5531,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 505/10000,\n",
      " train_loss: 844.6986,\n",
      " train_mae: 25.5529,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 506/10000,\n",
      " train_loss: 844.6924,\n",
      " train_mae: 25.5527,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 507/10000,\n",
      " train_loss: 844.6861,\n",
      " train_mae: 25.5526,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 508/10000,\n",
      " train_loss: 844.6799,\n",
      " train_mae: 25.5524,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 509/10000,\n",
      " train_loss: 844.6738,\n",
      " train_mae: 25.5523,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 510/10000,\n",
      " train_loss: 844.6675,\n",
      " train_mae: 25.5521,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 511/10000,\n",
      " train_loss: 844.6615,\n",
      " train_mae: 25.5520,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 512/10000,\n",
      " train_loss: 844.6553,\n",
      " train_mae: 25.5518,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 513/10000,\n",
      " train_loss: 844.6494,\n",
      " train_mae: 25.5516,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 514/10000,\n",
      " train_loss: 844.6431,\n",
      " train_mae: 25.5515,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 515/10000,\n",
      " train_loss: 844.6369,\n",
      " train_mae: 25.5513,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 516/10000,\n",
      " train_loss: 844.6310,\n",
      " train_mae: 25.5511,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 517/10000,\n",
      " train_loss: 844.6248,\n",
      " train_mae: 25.5510,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 518/10000,\n",
      " train_loss: 844.6190,\n",
      " train_mae: 25.5508,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 519/10000,\n",
      " train_loss: 844.6127,\n",
      " train_mae: 25.5507,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 520/10000,\n",
      " train_loss: 844.6067,\n",
      " train_mae: 25.5505,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 521/10000,\n",
      " train_loss: 844.6008,\n",
      " train_mae: 25.5503,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 522/10000,\n",
      " train_loss: 844.5947,\n",
      " train_mae: 25.5502,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 523/10000,\n",
      " train_loss: 844.5888,\n",
      " train_mae: 25.5500,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 524/10000,\n",
      " train_loss: 844.5828,\n",
      " train_mae: 25.5498,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 525/10000,\n",
      " train_loss: 844.5770,\n",
      " train_mae: 25.5497,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 526/10000,\n",
      " train_loss: 844.5709,\n",
      " train_mae: 25.5495,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 527/10000,\n",
      " train_loss: 844.5650,\n",
      " train_mae: 25.5493,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 528/10000,\n",
      " train_loss: 844.5590,\n",
      " train_mae: 25.5492,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 529/10000,\n",
      " train_loss: 844.5530,\n",
      " train_mae: 25.5490,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 530/10000,\n",
      " train_loss: 844.5472,\n",
      " train_mae: 25.5489,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 531/10000,\n",
      " train_loss: 844.5413,\n",
      " train_mae: 25.5487,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 532/10000,\n",
      " train_loss: 844.5355,\n",
      " train_mae: 25.5485,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 533/10000,\n",
      " train_loss: 844.5295,\n",
      " train_mae: 25.5484,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 534/10000,\n",
      " train_loss: 844.5238,\n",
      " train_mae: 25.5482,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 535/10000,\n",
      " train_loss: 844.5180,\n",
      " train_mae: 25.5480,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 536/10000,\n",
      " train_loss: 844.5121,\n",
      " train_mae: 25.5479,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 537/10000,\n",
      " train_loss: 844.5061,\n",
      " train_mae: 25.5477,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 538/10000,\n",
      " train_loss: 844.5003,\n",
      " train_mae: 25.5475,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 539/10000,\n",
      " train_loss: 844.4946,\n",
      " train_mae: 25.5474,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 540/10000,\n",
      " train_loss: 844.4888,\n",
      " train_mae: 25.5472,\n",
      " epoch_time_duration: 0.0155\n",
      "\n",
      "epoch: 541/10000,\n",
      " train_loss: 844.4830,\n",
      " train_mae: 25.5470,\n",
      " epoch_time_duration: 0.0075\n",
      "\n",
      "epoch: 542/10000,\n",
      " train_loss: 844.4771,\n",
      " train_mae: 25.5469,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 543/10000,\n",
      " train_loss: 844.4713,\n",
      " train_mae: 25.5467,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 544/10000,\n",
      " train_loss: 844.4653,\n",
      " train_mae: 25.5465,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 545/10000,\n",
      " train_loss: 844.4598,\n",
      " train_mae: 25.5464,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 546/10000,\n",
      " train_loss: 844.4541,\n",
      " train_mae: 25.5462,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 547/10000,\n",
      " train_loss: 844.4484,\n",
      " train_mae: 25.5460,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 548/10000,\n",
      " train_loss: 844.4427,\n",
      " train_mae: 25.5459,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 549/10000,\n",
      " train_loss: 844.4370,\n",
      " train_mae: 25.5457,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 550/10000,\n",
      " train_loss: 844.4310,\n",
      " train_mae: 25.5455,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 551/10000,\n",
      " train_loss: 844.4254,\n",
      " train_mae: 25.5454,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 552/10000,\n",
      " train_loss: 844.4197,\n",
      " train_mae: 25.5452,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 553/10000,\n",
      " train_loss: 844.4138,\n",
      " train_mae: 25.5450,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 554/10000,\n",
      " train_loss: 844.4083,\n",
      " train_mae: 25.5449,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 555/10000,\n",
      " train_loss: 844.4025,\n",
      " train_mae: 25.5447,\n",
      " epoch_time_duration: 0.0084\n",
      "\n",
      "epoch: 556/10000,\n",
      " train_loss: 844.3969,\n",
      " train_mae: 25.5445,\n",
      " epoch_time_duration: 0.0091\n",
      "\n",
      "epoch: 557/10000,\n",
      " train_loss: 844.3912,\n",
      " train_mae: 25.5443,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 558/10000,\n",
      " train_loss: 844.3852,\n",
      " train_mae: 25.5442,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 559/10000,\n",
      " train_loss: 844.3798,\n",
      " train_mae: 25.5440,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 560/10000,\n",
      " train_loss: 844.3741,\n",
      " train_mae: 25.5438,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 561/10000,\n",
      " train_loss: 844.3684,\n",
      " train_mae: 25.5437,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 562/10000,\n",
      " train_loss: 844.3625,\n",
      " train_mae: 25.5435,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 563/10000,\n",
      " train_loss: 844.3569,\n",
      " train_mae: 25.5433,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 564/10000,\n",
      " train_loss: 844.3513,\n",
      " train_mae: 25.5432,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 565/10000,\n",
      " train_loss: 844.3453,\n",
      " train_mae: 25.5430,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 566/10000,\n",
      " train_loss: 844.3400,\n",
      " train_mae: 25.5428,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 567/10000,\n",
      " train_loss: 844.3343,\n",
      " train_mae: 25.5426,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 568/10000,\n",
      " train_loss: 844.3286,\n",
      " train_mae: 25.5425,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 569/10000,\n",
      " train_loss: 844.3231,\n",
      " train_mae: 25.5423,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 570/10000,\n",
      " train_loss: 844.3174,\n",
      " train_mae: 25.5421,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 571/10000,\n",
      " train_loss: 844.3119,\n",
      " train_mae: 25.5420,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 572/10000,\n",
      " train_loss: 844.3062,\n",
      " train_mae: 25.5418,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 573/10000,\n",
      " train_loss: 844.3007,\n",
      " train_mae: 25.5416,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 574/10000,\n",
      " train_loss: 844.2948,\n",
      " train_mae: 25.5414,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 575/10000,\n",
      " train_loss: 844.2892,\n",
      " train_mae: 25.5413,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 576/10000,\n",
      " train_loss: 844.2838,\n",
      " train_mae: 25.5411,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 577/10000,\n",
      " train_loss: 844.2779,\n",
      " train_mae: 25.5409,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 578/10000,\n",
      " train_loss: 844.2725,\n",
      " train_mae: 25.5407,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 579/10000,\n",
      " train_loss: 844.2665,\n",
      " train_mae: 25.5406,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 580/10000,\n",
      " train_loss: 844.2609,\n",
      " train_mae: 25.5404,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 581/10000,\n",
      " train_loss: 844.2554,\n",
      " train_mae: 25.5402,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 582/10000,\n",
      " train_loss: 844.2499,\n",
      " train_mae: 25.5400,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 583/10000,\n",
      " train_loss: 844.2443,\n",
      " train_mae: 25.5399,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 584/10000,\n",
      " train_loss: 844.2387,\n",
      " train_mae: 25.5397,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 585/10000,\n",
      " train_loss: 844.2327,\n",
      " train_mae: 25.5395,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 586/10000,\n",
      " train_loss: 844.2271,\n",
      " train_mae: 25.5393,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 587/10000,\n",
      " train_loss: 844.2217,\n",
      " train_mae: 25.5391,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 588/10000,\n",
      " train_loss: 844.2160,\n",
      " train_mae: 25.5390,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 589/10000,\n",
      " train_loss: 844.2103,\n",
      " train_mae: 25.5388,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 590/10000,\n",
      " train_loss: 844.2050,\n",
      " train_mae: 25.5386,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 591/10000,\n",
      " train_loss: 844.1992,\n",
      " train_mae: 25.5384,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 592/10000,\n",
      " train_loss: 844.1933,\n",
      " train_mae: 25.5383,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 593/10000,\n",
      " train_loss: 844.1880,\n",
      " train_mae: 25.5381,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 594/10000,\n",
      " train_loss: 844.1822,\n",
      " train_mae: 25.5379,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 595/10000,\n",
      " train_loss: 844.1766,\n",
      " train_mae: 25.5377,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 596/10000,\n",
      " train_loss: 844.1708,\n",
      " train_mae: 25.5375,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 597/10000,\n",
      " train_loss: 844.1653,\n",
      " train_mae: 25.5374,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 598/10000,\n",
      " train_loss: 844.1596,\n",
      " train_mae: 25.5372,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 599/10000,\n",
      " train_loss: 844.1542,\n",
      " train_mae: 25.5370,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 600/10000,\n",
      " train_loss: 844.1483,\n",
      " train_mae: 25.5369,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 601/10000,\n",
      " train_loss: 844.1430,\n",
      " train_mae: 25.5367,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 602/10000,\n",
      " train_loss: 844.1371,\n",
      " train_mae: 25.5365,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 603/10000,\n",
      " train_loss: 844.1317,\n",
      " train_mae: 25.5363,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 604/10000,\n",
      " train_loss: 844.1260,\n",
      " train_mae: 25.5361,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 605/10000,\n",
      " train_loss: 844.1205,\n",
      " train_mae: 25.5359,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 606/10000,\n",
      " train_loss: 844.1148,\n",
      " train_mae: 25.5358,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 607/10000,\n",
      " train_loss: 844.1090,\n",
      " train_mae: 25.5356,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 608/10000,\n",
      " train_loss: 844.1033,\n",
      " train_mae: 25.5354,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 609/10000,\n",
      " train_loss: 844.0978,\n",
      " train_mae: 25.5352,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 610/10000,\n",
      " train_loss: 844.0922,\n",
      " train_mae: 25.5350,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 611/10000,\n",
      " train_loss: 844.0866,\n",
      " train_mae: 25.5348,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 612/10000,\n",
      " train_loss: 844.0806,\n",
      " train_mae: 25.5347,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 613/10000,\n",
      " train_loss: 844.0753,\n",
      " train_mae: 25.5345,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 614/10000,\n",
      " train_loss: 844.0697,\n",
      " train_mae: 25.5343,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 615/10000,\n",
      " train_loss: 844.0638,\n",
      " train_mae: 25.5341,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 616/10000,\n",
      " train_loss: 844.0583,\n",
      " train_mae: 25.5340,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 617/10000,\n",
      " train_loss: 844.0527,\n",
      " train_mae: 25.5338,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 618/10000,\n",
      " train_loss: 844.0471,\n",
      " train_mae: 25.5336,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 619/10000,\n",
      " train_loss: 844.0414,\n",
      " train_mae: 25.5334,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 620/10000,\n",
      " train_loss: 844.0360,\n",
      " train_mae: 25.5332,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 621/10000,\n",
      " train_loss: 844.0300,\n",
      " train_mae: 25.5331,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 622/10000,\n",
      " train_loss: 844.0245,\n",
      " train_mae: 25.5329,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 623/10000,\n",
      " train_loss: 844.0189,\n",
      " train_mae: 25.5327,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 624/10000,\n",
      " train_loss: 844.0131,\n",
      " train_mae: 25.5325,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 625/10000,\n",
      " train_loss: 844.0073,\n",
      " train_mae: 25.5323,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 626/10000,\n",
      " train_loss: 844.0014,\n",
      " train_mae: 25.5321,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 627/10000,\n",
      " train_loss: 843.9962,\n",
      " train_mae: 25.5319,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 628/10000,\n",
      " train_loss: 843.9904,\n",
      " train_mae: 25.5318,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 629/10000,\n",
      " train_loss: 843.9848,\n",
      " train_mae: 25.5316,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 630/10000,\n",
      " train_loss: 843.9791,\n",
      " train_mae: 25.5314,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 631/10000,\n",
      " train_loss: 843.9735,\n",
      " train_mae: 25.5312,\n",
      " epoch_time_duration: 0.0090\n",
      "\n",
      "epoch: 632/10000,\n",
      " train_loss: 843.9677,\n",
      " train_mae: 25.5310,\n",
      " epoch_time_duration: 0.0081\n",
      "\n",
      "epoch: 633/10000,\n",
      " train_loss: 843.9621,\n",
      " train_mae: 25.5308,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 634/10000,\n",
      " train_loss: 843.9563,\n",
      " train_mae: 25.5306,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 635/10000,\n",
      " train_loss: 843.9508,\n",
      " train_mae: 25.5305,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 636/10000,\n",
      " train_loss: 843.9449,\n",
      " train_mae: 25.5303,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 637/10000,\n",
      " train_loss: 843.9396,\n",
      " train_mae: 25.5301,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 638/10000,\n",
      " train_loss: 843.9335,\n",
      " train_mae: 25.5299,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 639/10000,\n",
      " train_loss: 843.9279,\n",
      " train_mae: 25.5297,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 640/10000,\n",
      " train_loss: 843.9227,\n",
      " train_mae: 25.5295,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 641/10000,\n",
      " train_loss: 843.9163,\n",
      " train_mae: 25.5293,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 642/10000,\n",
      " train_loss: 843.9108,\n",
      " train_mae: 25.5292,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 643/10000,\n",
      " train_loss: 843.9052,\n",
      " train_mae: 25.5290,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 644/10000,\n",
      " train_loss: 843.8996,\n",
      " train_mae: 25.5288,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 645/10000,\n",
      " train_loss: 843.8941,\n",
      " train_mae: 25.5286,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 646/10000,\n",
      " train_loss: 843.8882,\n",
      " train_mae: 25.5284,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 647/10000,\n",
      " train_loss: 843.8823,\n",
      " train_mae: 25.5282,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 648/10000,\n",
      " train_loss: 843.8768,\n",
      " train_mae: 25.5280,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 649/10000,\n",
      " train_loss: 843.8707,\n",
      " train_mae: 25.5279,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 650/10000,\n",
      " train_loss: 843.8654,\n",
      " train_mae: 25.5277,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 651/10000,\n",
      " train_loss: 843.8595,\n",
      " train_mae: 25.5275,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 652/10000,\n",
      " train_loss: 843.8539,\n",
      " train_mae: 25.5273,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 653/10000,\n",
      " train_loss: 843.8481,\n",
      " train_mae: 25.5271,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 654/10000,\n",
      " train_loss: 843.8424,\n",
      " train_mae: 25.5269,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 655/10000,\n",
      " train_loss: 843.8368,\n",
      " train_mae: 25.5267,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 656/10000,\n",
      " train_loss: 843.8307,\n",
      " train_mae: 25.5265,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 657/10000,\n",
      " train_loss: 843.8254,\n",
      " train_mae: 25.5264,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 658/10000,\n",
      " train_loss: 843.8195,\n",
      " train_mae: 25.5262,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 659/10000,\n",
      " train_loss: 843.8137,\n",
      " train_mae: 25.5260,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 660/10000,\n",
      " train_loss: 843.8077,\n",
      " train_mae: 25.5258,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 661/10000,\n",
      " train_loss: 843.8020,\n",
      " train_mae: 25.5256,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 662/10000,\n",
      " train_loss: 843.7963,\n",
      " train_mae: 25.5254,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 663/10000,\n",
      " train_loss: 843.7905,\n",
      " train_mae: 25.5252,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 664/10000,\n",
      " train_loss: 843.7851,\n",
      " train_mae: 25.5250,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 665/10000,\n",
      " train_loss: 843.7794,\n",
      " train_mae: 25.5248,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 666/10000,\n",
      " train_loss: 843.7734,\n",
      " train_mae: 25.5247,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 667/10000,\n",
      " train_loss: 843.7677,\n",
      " train_mae: 25.5244,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 668/10000,\n",
      " train_loss: 843.7618,\n",
      " train_mae: 25.5242,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 669/10000,\n",
      " train_loss: 843.7562,\n",
      " train_mae: 25.5241,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 670/10000,\n",
      " train_loss: 843.7505,\n",
      " train_mae: 25.5239,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 671/10000,\n",
      " train_loss: 843.7443,\n",
      " train_mae: 25.5237,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 672/10000,\n",
      " train_loss: 843.7389,\n",
      " train_mae: 25.5235,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 673/10000,\n",
      " train_loss: 843.7330,\n",
      " train_mae: 25.5233,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 674/10000,\n",
      " train_loss: 843.7271,\n",
      " train_mae: 25.5231,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 675/10000,\n",
      " train_loss: 843.7211,\n",
      " train_mae: 25.5229,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 676/10000,\n",
      " train_loss: 843.7158,\n",
      " train_mae: 25.5227,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 677/10000,\n",
      " train_loss: 843.7098,\n",
      " train_mae: 25.5225,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 678/10000,\n",
      " train_loss: 843.7038,\n",
      " train_mae: 25.5224,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 679/10000,\n",
      " train_loss: 843.6984,\n",
      " train_mae: 25.5222,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 680/10000,\n",
      " train_loss: 843.6921,\n",
      " train_mae: 25.5219,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 681/10000,\n",
      " train_loss: 843.6862,\n",
      " train_mae: 25.5217,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 682/10000,\n",
      " train_loss: 843.6807,\n",
      " train_mae: 25.5216,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 683/10000,\n",
      " train_loss: 843.6747,\n",
      " train_mae: 25.5214,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 684/10000,\n",
      " train_loss: 843.6691,\n",
      " train_mae: 25.5212,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 685/10000,\n",
      " train_loss: 843.6633,\n",
      " train_mae: 25.5210,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 686/10000,\n",
      " train_loss: 843.6572,\n",
      " train_mae: 25.5208,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 687/10000,\n",
      " train_loss: 843.6516,\n",
      " train_mae: 25.5206,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 688/10000,\n",
      " train_loss: 843.6456,\n",
      " train_mae: 25.5204,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 689/10000,\n",
      " train_loss: 843.6400,\n",
      " train_mae: 25.5202,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 690/10000,\n",
      " train_loss: 843.6340,\n",
      " train_mae: 25.5200,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 691/10000,\n",
      " train_loss: 843.6282,\n",
      " train_mae: 25.5198,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 692/10000,\n",
      " train_loss: 843.6225,\n",
      " train_mae: 25.5196,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 693/10000,\n",
      " train_loss: 843.6164,\n",
      " train_mae: 25.5194,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 694/10000,\n",
      " train_loss: 843.6104,\n",
      " train_mae: 25.5192,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 695/10000,\n",
      " train_loss: 843.6045,\n",
      " train_mae: 25.5190,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 696/10000,\n",
      " train_loss: 843.5991,\n",
      " train_mae: 25.5189,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 697/10000,\n",
      " train_loss: 843.5929,\n",
      " train_mae: 25.5187,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 698/10000,\n",
      " train_loss: 843.5873,\n",
      " train_mae: 25.5185,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 699/10000,\n",
      " train_loss: 843.5815,\n",
      " train_mae: 25.5183,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 700/10000,\n",
      " train_loss: 843.5756,\n",
      " train_mae: 25.5181,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 701/10000,\n",
      " train_loss: 843.5697,\n",
      " train_mae: 25.5179,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 702/10000,\n",
      " train_loss: 843.5638,\n",
      " train_mae: 25.5177,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 703/10000,\n",
      " train_loss: 843.5579,\n",
      " train_mae: 25.5175,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 704/10000,\n",
      " train_loss: 843.5519,\n",
      " train_mae: 25.5173,\n",
      " epoch_time_duration: 0.0074\n",
      "\n",
      "epoch: 705/10000,\n",
      " train_loss: 843.5459,\n",
      " train_mae: 25.5171,\n",
      " epoch_time_duration: 0.0093\n",
      "\n",
      "epoch: 706/10000,\n",
      " train_loss: 843.5403,\n",
      " train_mae: 25.5169,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 707/10000,\n",
      " train_loss: 843.5342,\n",
      " train_mae: 25.5167,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 708/10000,\n",
      " train_loss: 843.5287,\n",
      " train_mae: 25.5165,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 709/10000,\n",
      " train_loss: 843.5229,\n",
      " train_mae: 25.5163,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 710/10000,\n",
      " train_loss: 843.5166,\n",
      " train_mae: 25.5161,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 711/10000,\n",
      " train_loss: 843.5104,\n",
      " train_mae: 25.5159,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 712/10000,\n",
      " train_loss: 843.5049,\n",
      " train_mae: 25.5157,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 713/10000,\n",
      " train_loss: 843.4990,\n",
      " train_mae: 25.5155,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 714/10000,\n",
      " train_loss: 843.4929,\n",
      " train_mae: 25.5153,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 715/10000,\n",
      " train_loss: 843.4872,\n",
      " train_mae: 25.5151,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 716/10000,\n",
      " train_loss: 843.4811,\n",
      " train_mae: 25.5149,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 717/10000,\n",
      " train_loss: 843.4752,\n",
      " train_mae: 25.5147,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 718/10000,\n",
      " train_loss: 843.4694,\n",
      " train_mae: 25.5146,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 719/10000,\n",
      " train_loss: 843.4640,\n",
      " train_mae: 25.5144,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 720/10000,\n",
      " train_loss: 843.4576,\n",
      " train_mae: 25.5142,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 721/10000,\n",
      " train_loss: 843.4520,\n",
      " train_mae: 25.5140,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 722/10000,\n",
      " train_loss: 843.4457,\n",
      " train_mae: 25.5138,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 723/10000,\n",
      " train_loss: 843.4403,\n",
      " train_mae: 25.5136,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 724/10000,\n",
      " train_loss: 843.4339,\n",
      " train_mae: 25.5134,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 725/10000,\n",
      " train_loss: 843.4280,\n",
      " train_mae: 25.5132,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 726/10000,\n",
      " train_loss: 843.4223,\n",
      " train_mae: 25.5130,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 727/10000,\n",
      " train_loss: 843.4161,\n",
      " train_mae: 25.5128,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 728/10000,\n",
      " train_loss: 843.4103,\n",
      " train_mae: 25.5126,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 729/10000,\n",
      " train_loss: 843.4043,\n",
      " train_mae: 25.5124,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 730/10000,\n",
      " train_loss: 843.3984,\n",
      " train_mae: 25.5122,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 731/10000,\n",
      " train_loss: 843.3925,\n",
      " train_mae: 25.5120,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 732/10000,\n",
      " train_loss: 843.3864,\n",
      " train_mae: 25.5118,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 733/10000,\n",
      " train_loss: 843.3804,\n",
      " train_mae: 25.5116,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 734/10000,\n",
      " train_loss: 843.3746,\n",
      " train_mae: 25.5114,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 735/10000,\n",
      " train_loss: 843.3684,\n",
      " train_mae: 25.5112,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 736/10000,\n",
      " train_loss: 843.3625,\n",
      " train_mae: 25.5110,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 737/10000,\n",
      " train_loss: 843.3564,\n",
      " train_mae: 25.5108,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 738/10000,\n",
      " train_loss: 843.3506,\n",
      " train_mae: 25.5106,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 739/10000,\n",
      " train_loss: 843.3448,\n",
      " train_mae: 25.5104,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 740/10000,\n",
      " train_loss: 843.3386,\n",
      " train_mae: 25.5102,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 741/10000,\n",
      " train_loss: 843.3327,\n",
      " train_mae: 25.5100,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 742/10000,\n",
      " train_loss: 843.3270,\n",
      " train_mae: 25.5098,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 743/10000,\n",
      " train_loss: 843.3206,\n",
      " train_mae: 25.5096,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 744/10000,\n",
      " train_loss: 843.3150,\n",
      " train_mae: 25.5094,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 745/10000,\n",
      " train_loss: 843.3089,\n",
      " train_mae: 25.5092,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 746/10000,\n",
      " train_loss: 843.3030,\n",
      " train_mae: 25.5090,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 747/10000,\n",
      " train_loss: 843.2968,\n",
      " train_mae: 25.5088,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 748/10000,\n",
      " train_loss: 843.2911,\n",
      " train_mae: 25.5086,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 749/10000,\n",
      " train_loss: 843.2848,\n",
      " train_mae: 25.5084,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 750/10000,\n",
      " train_loss: 843.2787,\n",
      " train_mae: 25.5082,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 751/10000,\n",
      " train_loss: 843.2731,\n",
      " train_mae: 25.5080,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 752/10000,\n",
      " train_loss: 843.2668,\n",
      " train_mae: 25.5078,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 753/10000,\n",
      " train_loss: 843.2607,\n",
      " train_mae: 25.5076,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 754/10000,\n",
      " train_loss: 843.2549,\n",
      " train_mae: 25.5074,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 755/10000,\n",
      " train_loss: 843.2491,\n",
      " train_mae: 25.5072,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 756/10000,\n",
      " train_loss: 843.2431,\n",
      " train_mae: 25.5070,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 757/10000,\n",
      " train_loss: 843.2369,\n",
      " train_mae: 25.5068,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 758/10000,\n",
      " train_loss: 843.2310,\n",
      " train_mae: 25.5066,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 759/10000,\n",
      " train_loss: 843.2250,\n",
      " train_mae: 25.5064,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 760/10000,\n",
      " train_loss: 843.2188,\n",
      " train_mae: 25.5062,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 761/10000,\n",
      " train_loss: 843.2128,\n",
      " train_mae: 25.5060,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 762/10000,\n",
      " train_loss: 843.2070,\n",
      " train_mae: 25.5058,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 763/10000,\n",
      " train_loss: 843.2009,\n",
      " train_mae: 25.5056,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 764/10000,\n",
      " train_loss: 843.1947,\n",
      " train_mae: 25.5054,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 765/10000,\n",
      " train_loss: 843.1888,\n",
      " train_mae: 25.5052,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 766/10000,\n",
      " train_loss: 843.1826,\n",
      " train_mae: 25.5050,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 767/10000,\n",
      " train_loss: 843.1768,\n",
      " train_mae: 25.5048,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 768/10000,\n",
      " train_loss: 843.1710,\n",
      " train_mae: 25.5046,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 769/10000,\n",
      " train_loss: 843.1649,\n",
      " train_mae: 25.5044,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 770/10000,\n",
      " train_loss: 843.1586,\n",
      " train_mae: 25.5042,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 771/10000,\n",
      " train_loss: 843.1529,\n",
      " train_mae: 25.5040,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 772/10000,\n",
      " train_loss: 843.1469,\n",
      " train_mae: 25.5038,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 773/10000,\n",
      " train_loss: 843.1409,\n",
      " train_mae: 25.5036,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 774/10000,\n",
      " train_loss: 843.1348,\n",
      " train_mae: 25.5034,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 775/10000,\n",
      " train_loss: 843.1289,\n",
      " train_mae: 25.5032,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 776/10000,\n",
      " train_loss: 843.1227,\n",
      " train_mae: 25.5030,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 777/10000,\n",
      " train_loss: 843.1166,\n",
      " train_mae: 25.5027,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 778/10000,\n",
      " train_loss: 843.1105,\n",
      " train_mae: 25.5025,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 779/10000,\n",
      " train_loss: 843.1044,\n",
      " train_mae: 25.5024,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 780/10000,\n",
      " train_loss: 843.0990,\n",
      " train_mae: 25.5022,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 781/10000,\n",
      " train_loss: 843.0923,\n",
      " train_mae: 25.5019,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 782/10000,\n",
      " train_loss: 843.0865,\n",
      " train_mae: 25.5017,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 783/10000,\n",
      " train_loss: 843.0805,\n",
      " train_mae: 25.5015,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 784/10000,\n",
      " train_loss: 843.0744,\n",
      " train_mae: 25.5013,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 785/10000,\n",
      " train_loss: 843.0684,\n",
      " train_mae: 25.5011,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 786/10000,\n",
      " train_loss: 843.0626,\n",
      " train_mae: 25.5009,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 787/10000,\n",
      " train_loss: 843.0561,\n",
      " train_mae: 25.5007,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 788/10000,\n",
      " train_loss: 843.0505,\n",
      " train_mae: 25.5005,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 789/10000,\n",
      " train_loss: 843.0442,\n",
      " train_mae: 25.5003,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 790/10000,\n",
      " train_loss: 843.0381,\n",
      " train_mae: 25.5001,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 791/10000,\n",
      " train_loss: 843.0319,\n",
      " train_mae: 25.4999,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 792/10000,\n",
      " train_loss: 843.0262,\n",
      " train_mae: 25.4997,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 793/10000,\n",
      " train_loss: 843.0200,\n",
      " train_mae: 25.4995,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 794/10000,\n",
      " train_loss: 843.0141,\n",
      " train_mae: 25.4993,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 795/10000,\n",
      " train_loss: 843.0079,\n",
      " train_mae: 25.4991,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 796/10000,\n",
      " train_loss: 843.0018,\n",
      " train_mae: 25.4989,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 797/10000,\n",
      " train_loss: 842.9957,\n",
      " train_mae: 25.4987,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 798/10000,\n",
      " train_loss: 842.9897,\n",
      " train_mae: 25.4985,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 799/10000,\n",
      " train_loss: 842.9836,\n",
      " train_mae: 25.4983,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 800/10000,\n",
      " train_loss: 842.9779,\n",
      " train_mae: 25.4981,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 801/10000,\n",
      " train_loss: 842.9713,\n",
      " train_mae: 25.4979,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 802/10000,\n",
      " train_loss: 842.9654,\n",
      " train_mae: 25.4977,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 803/10000,\n",
      " train_loss: 842.9591,\n",
      " train_mae: 25.4975,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 804/10000,\n",
      " train_loss: 842.9534,\n",
      " train_mae: 25.4973,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 805/10000,\n",
      " train_loss: 842.9476,\n",
      " train_mae: 25.4971,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 806/10000,\n",
      " train_loss: 842.9413,\n",
      " train_mae: 25.4969,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 807/10000,\n",
      " train_loss: 842.9352,\n",
      " train_mae: 25.4967,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 808/10000,\n",
      " train_loss: 842.9291,\n",
      " train_mae: 25.4964,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 809/10000,\n",
      " train_loss: 842.9230,\n",
      " train_mae: 25.4962,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 810/10000,\n",
      " train_loss: 842.9172,\n",
      " train_mae: 25.4960,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 811/10000,\n",
      " train_loss: 842.9111,\n",
      " train_mae: 25.4958,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 812/10000,\n",
      " train_loss: 842.9052,\n",
      " train_mae: 25.4956,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 813/10000,\n",
      " train_loss: 842.8990,\n",
      " train_mae: 25.4954,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 814/10000,\n",
      " train_loss: 842.8928,\n",
      " train_mae: 25.4952,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 815/10000,\n",
      " train_loss: 842.8867,\n",
      " train_mae: 25.4950,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 816/10000,\n",
      " train_loss: 842.8805,\n",
      " train_mae: 25.4948,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 817/10000,\n",
      " train_loss: 842.8748,\n",
      " train_mae: 25.4946,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 818/10000,\n",
      " train_loss: 842.8685,\n",
      " train_mae: 25.4944,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 819/10000,\n",
      " train_loss: 842.8624,\n",
      " train_mae: 25.4942,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 820/10000,\n",
      " train_loss: 842.8564,\n",
      " train_mae: 25.4940,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 821/10000,\n",
      " train_loss: 842.8506,\n",
      " train_mae: 25.4938,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 822/10000,\n",
      " train_loss: 842.8443,\n",
      " train_mae: 25.4936,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 823/10000,\n",
      " train_loss: 842.8385,\n",
      " train_mae: 25.4934,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 824/10000,\n",
      " train_loss: 842.8323,\n",
      " train_mae: 25.4932,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 825/10000,\n",
      " train_loss: 842.8262,\n",
      " train_mae: 25.4930,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 826/10000,\n",
      " train_loss: 842.8201,\n",
      " train_mae: 25.4928,\n",
      " epoch_time_duration: 0.0136\n",
      "\n",
      "epoch: 827/10000,\n",
      " train_loss: 842.8139,\n",
      " train_mae: 25.4926,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 828/10000,\n",
      " train_loss: 842.8079,\n",
      " train_mae: 25.4923,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 829/10000,\n",
      " train_loss: 842.8018,\n",
      " train_mae: 25.4921,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 830/10000,\n",
      " train_loss: 842.7959,\n",
      " train_mae: 25.4919,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 831/10000,\n",
      " train_loss: 842.7899,\n",
      " train_mae: 25.4918,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 832/10000,\n",
      " train_loss: 842.7836,\n",
      " train_mae: 25.4915,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 833/10000,\n",
      " train_loss: 842.7775,\n",
      " train_mae: 25.4913,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 834/10000,\n",
      " train_loss: 842.7714,\n",
      " train_mae: 25.4911,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 835/10000,\n",
      " train_loss: 842.7653,\n",
      " train_mae: 25.4909,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 836/10000,\n",
      " train_loss: 842.7594,\n",
      " train_mae: 25.4907,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 837/10000,\n",
      " train_loss: 842.7535,\n",
      " train_mae: 25.4905,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 838/10000,\n",
      " train_loss: 842.7469,\n",
      " train_mae: 25.4903,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 839/10000,\n",
      " train_loss: 842.7412,\n",
      " train_mae: 25.4901,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 840/10000,\n",
      " train_loss: 842.7348,\n",
      " train_mae: 25.4899,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 841/10000,\n",
      " train_loss: 842.7289,\n",
      " train_mae: 25.4897,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 842/10000,\n",
      " train_loss: 842.7230,\n",
      " train_mae: 25.4895,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 843/10000,\n",
      " train_loss: 842.7167,\n",
      " train_mae: 25.4893,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 844/10000,\n",
      " train_loss: 842.7110,\n",
      " train_mae: 25.4891,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 845/10000,\n",
      " train_loss: 842.7048,\n",
      " train_mae: 25.4889,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 846/10000,\n",
      " train_loss: 842.6989,\n",
      " train_mae: 25.4887,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 847/10000,\n",
      " train_loss: 842.6926,\n",
      " train_mae: 25.4884,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 848/10000,\n",
      " train_loss: 842.6864,\n",
      " train_mae: 25.4882,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 849/10000,\n",
      " train_loss: 842.6802,\n",
      " train_mae: 25.4880,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 850/10000,\n",
      " train_loss: 842.6739,\n",
      " train_mae: 25.4878,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 851/10000,\n",
      " train_loss: 842.6684,\n",
      " train_mae: 25.4876,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 852/10000,\n",
      " train_loss: 842.6622,\n",
      " train_mae: 25.4874,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 853/10000,\n",
      " train_loss: 842.6564,\n",
      " train_mae: 25.4872,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 854/10000,\n",
      " train_loss: 842.6504,\n",
      " train_mae: 25.4870,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 855/10000,\n",
      " train_loss: 842.6442,\n",
      " train_mae: 25.4868,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 856/10000,\n",
      " train_loss: 842.6379,\n",
      " train_mae: 25.4866,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 857/10000,\n",
      " train_loss: 842.6319,\n",
      " train_mae: 25.4864,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 858/10000,\n",
      " train_loss: 842.6261,\n",
      " train_mae: 25.4862,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 859/10000,\n",
      " train_loss: 842.6200,\n",
      " train_mae: 25.4860,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 860/10000,\n",
      " train_loss: 842.6139,\n",
      " train_mae: 25.4858,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 861/10000,\n",
      " train_loss: 842.6080,\n",
      " train_mae: 25.4856,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 862/10000,\n",
      " train_loss: 842.6017,\n",
      " train_mae: 25.4854,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 863/10000,\n",
      " train_loss: 842.5956,\n",
      " train_mae: 25.4851,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 864/10000,\n",
      " train_loss: 842.5894,\n",
      " train_mae: 25.4849,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 865/10000,\n",
      " train_loss: 842.5834,\n",
      " train_mae: 25.4847,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 866/10000,\n",
      " train_loss: 842.5771,\n",
      " train_mae: 25.4845,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 867/10000,\n",
      " train_loss: 842.5713,\n",
      " train_mae: 25.4843,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 868/10000,\n",
      " train_loss: 842.5654,\n",
      " train_mae: 25.4841,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 869/10000,\n",
      " train_loss: 842.5591,\n",
      " train_mae: 25.4839,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 870/10000,\n",
      " train_loss: 842.5532,\n",
      " train_mae: 25.4837,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 871/10000,\n",
      " train_loss: 842.5471,\n",
      " train_mae: 25.4835,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 872/10000,\n",
      " train_loss: 842.5411,\n",
      " train_mae: 25.4833,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 873/10000,\n",
      " train_loss: 842.5349,\n",
      " train_mae: 25.4831,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 874/10000,\n",
      " train_loss: 842.5289,\n",
      " train_mae: 25.4829,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 875/10000,\n",
      " train_loss: 842.5231,\n",
      " train_mae: 25.4827,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 876/10000,\n",
      " train_loss: 842.5170,\n",
      " train_mae: 25.4825,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 877/10000,\n",
      " train_loss: 842.5107,\n",
      " train_mae: 25.4823,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 878/10000,\n",
      " train_loss: 842.5051,\n",
      " train_mae: 25.4821,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 879/10000,\n",
      " train_loss: 842.4991,\n",
      " train_mae: 25.4818,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 880/10000,\n",
      " train_loss: 842.4927,\n",
      " train_mae: 25.4816,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 881/10000,\n",
      " train_loss: 842.4869,\n",
      " train_mae: 25.4814,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 882/10000,\n",
      " train_loss: 842.4809,\n",
      " train_mae: 25.4812,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 883/10000,\n",
      " train_loss: 842.4745,\n",
      " train_mae: 25.4810,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 884/10000,\n",
      " train_loss: 842.4688,\n",
      " train_mae: 25.4808,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 885/10000,\n",
      " train_loss: 842.4625,\n",
      " train_mae: 25.4806,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 886/10000,\n",
      " train_loss: 842.4564,\n",
      " train_mae: 25.4804,\n",
      " epoch_time_duration: 0.0076\n",
      "\n",
      "epoch: 887/10000,\n",
      " train_loss: 842.4503,\n",
      " train_mae: 25.4802,\n",
      " epoch_time_duration: 0.0089\n",
      "\n",
      "epoch: 888/10000,\n",
      " train_loss: 842.4446,\n",
      " train_mae: 25.4800,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 889/10000,\n",
      " train_loss: 842.4384,\n",
      " train_mae: 25.4798,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 890/10000,\n",
      " train_loss: 842.4322,\n",
      " train_mae: 25.4796,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 891/10000,\n",
      " train_loss: 842.4265,\n",
      " train_mae: 25.4794,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 892/10000,\n",
      " train_loss: 842.4201,\n",
      " train_mae: 25.4791,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 893/10000,\n",
      " train_loss: 842.4140,\n",
      " train_mae: 25.4789,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 894/10000,\n",
      " train_loss: 842.4081,\n",
      " train_mae: 25.4787,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 895/10000,\n",
      " train_loss: 842.4023,\n",
      " train_mae: 25.4785,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 896/10000,\n",
      " train_loss: 842.3964,\n",
      " train_mae: 25.4783,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 897/10000,\n",
      " train_loss: 842.3898,\n",
      " train_mae: 25.4781,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 898/10000,\n",
      " train_loss: 842.3839,\n",
      " train_mae: 25.4779,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 899/10000,\n",
      " train_loss: 842.3779,\n",
      " train_mae: 25.4777,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 900/10000,\n",
      " train_loss: 842.3717,\n",
      " train_mae: 25.4775,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 901/10000,\n",
      " train_loss: 842.3658,\n",
      " train_mae: 25.4773,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 902/10000,\n",
      " train_loss: 842.3599,\n",
      " train_mae: 25.4771,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 903/10000,\n",
      " train_loss: 842.3542,\n",
      " train_mae: 25.4769,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 904/10000,\n",
      " train_loss: 842.3482,\n",
      " train_mae: 25.4767,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 905/10000,\n",
      " train_loss: 842.3420,\n",
      " train_mae: 25.4765,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 906/10000,\n",
      " train_loss: 842.3359,\n",
      " train_mae: 25.4762,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 907/10000,\n",
      " train_loss: 842.3302,\n",
      " train_mae: 25.4760,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 908/10000,\n",
      " train_loss: 842.3236,\n",
      " train_mae: 25.4758,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 909/10000,\n",
      " train_loss: 842.3181,\n",
      " train_mae: 25.4756,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 910/10000,\n",
      " train_loss: 842.3120,\n",
      " train_mae: 25.4754,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 911/10000,\n",
      " train_loss: 842.3058,\n",
      " train_mae: 25.4752,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 912/10000,\n",
      " train_loss: 842.2998,\n",
      " train_mae: 25.4750,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 913/10000,\n",
      " train_loss: 842.2935,\n",
      " train_mae: 25.4748,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 914/10000,\n",
      " train_loss: 842.2878,\n",
      " train_mae: 25.4746,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 915/10000,\n",
      " train_loss: 842.2819,\n",
      " train_mae: 25.4744,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 916/10000,\n",
      " train_loss: 842.2759,\n",
      " train_mae: 25.4742,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 917/10000,\n",
      " train_loss: 842.2701,\n",
      " train_mae: 25.4740,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 918/10000,\n",
      " train_loss: 842.2641,\n",
      " train_mae: 25.4738,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 919/10000,\n",
      " train_loss: 842.2576,\n",
      " train_mae: 25.4736,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 920/10000,\n",
      " train_loss: 842.2521,\n",
      " train_mae: 25.4734,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 921/10000,\n",
      " train_loss: 842.2457,\n",
      " train_mae: 25.4731,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 922/10000,\n",
      " train_loss: 842.2399,\n",
      " train_mae: 25.4729,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 923/10000,\n",
      " train_loss: 842.2338,\n",
      " train_mae: 25.4727,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 924/10000,\n",
      " train_loss: 842.2278,\n",
      " train_mae: 25.4725,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 925/10000,\n",
      " train_loss: 842.2219,\n",
      " train_mae: 25.4723,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 926/10000,\n",
      " train_loss: 842.2162,\n",
      " train_mae: 25.4721,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 927/10000,\n",
      " train_loss: 842.2101,\n",
      " train_mae: 25.4719,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 928/10000,\n",
      " train_loss: 842.2042,\n",
      " train_mae: 25.4717,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 929/10000,\n",
      " train_loss: 842.1984,\n",
      " train_mae: 25.4715,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 930/10000,\n",
      " train_loss: 842.1924,\n",
      " train_mae: 25.4713,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 931/10000,\n",
      " train_loss: 842.1862,\n",
      " train_mae: 25.4711,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 932/10000,\n",
      " train_loss: 842.1802,\n",
      " train_mae: 25.4709,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 933/10000,\n",
      " train_loss: 842.1742,\n",
      " train_mae: 25.4707,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 934/10000,\n",
      " train_loss: 842.1680,\n",
      " train_mae: 25.4704,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 935/10000,\n",
      " train_loss: 842.1627,\n",
      " train_mae: 25.4702,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 936/10000,\n",
      " train_loss: 842.1563,\n",
      " train_mae: 25.4700,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 937/10000,\n",
      " train_loss: 842.1500,\n",
      " train_mae: 25.4698,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 938/10000,\n",
      " train_loss: 842.1443,\n",
      " train_mae: 25.4696,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 939/10000,\n",
      " train_loss: 842.1384,\n",
      " train_mae: 25.4694,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 940/10000,\n",
      " train_loss: 842.1328,\n",
      " train_mae: 25.4692,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 941/10000,\n",
      " train_loss: 842.1264,\n",
      " train_mae: 25.4690,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 942/10000,\n",
      " train_loss: 842.1205,\n",
      " train_mae: 25.4688,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 943/10000,\n",
      " train_loss: 842.1146,\n",
      " train_mae: 25.4686,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 944/10000,\n",
      " train_loss: 842.1086,\n",
      " train_mae: 25.4684,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 945/10000,\n",
      " train_loss: 842.1027,\n",
      " train_mae: 25.4682,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 946/10000,\n",
      " train_loss: 842.0971,\n",
      " train_mae: 25.4680,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 947/10000,\n",
      " train_loss: 842.0911,\n",
      " train_mae: 25.4678,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 948/10000,\n",
      " train_loss: 842.0850,\n",
      " train_mae: 25.4675,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 949/10000,\n",
      " train_loss: 842.0788,\n",
      " train_mae: 25.4674,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 950/10000,\n",
      " train_loss: 842.0734,\n",
      " train_mae: 25.4672,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 951/10000,\n",
      " train_loss: 842.0675,\n",
      " train_mae: 25.4669,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 952/10000,\n",
      " train_loss: 842.0615,\n",
      " train_mae: 25.4667,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 953/10000,\n",
      " train_loss: 842.0552,\n",
      " train_mae: 25.4665,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 954/10000,\n",
      " train_loss: 842.0494,\n",
      " train_mae: 25.4663,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 955/10000,\n",
      " train_loss: 842.0438,\n",
      " train_mae: 25.4661,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 956/10000,\n",
      " train_loss: 842.0375,\n",
      " train_mae: 25.4659,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 957/10000,\n",
      " train_loss: 842.0320,\n",
      " train_mae: 25.4657,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 958/10000,\n",
      " train_loss: 842.0261,\n",
      " train_mae: 25.4655,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 959/10000,\n",
      " train_loss: 842.0201,\n",
      " train_mae: 25.4653,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 960/10000,\n",
      " train_loss: 842.0141,\n",
      " train_mae: 25.4651,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 961/10000,\n",
      " train_loss: 842.0085,\n",
      " train_mae: 25.4649,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 962/10000,\n",
      " train_loss: 842.0026,\n",
      " train_mae: 25.4647,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 963/10000,\n",
      " train_loss: 841.9966,\n",
      " train_mae: 25.4645,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 964/10000,\n",
      " train_loss: 841.9905,\n",
      " train_mae: 25.4643,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 965/10000,\n",
      " train_loss: 841.9852,\n",
      " train_mae: 25.4641,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 966/10000,\n",
      " train_loss: 841.9786,\n",
      " train_mae: 25.4639,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 967/10000,\n",
      " train_loss: 841.9732,\n",
      " train_mae: 25.4636,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 968/10000,\n",
      " train_loss: 841.9673,\n",
      " train_mae: 25.4634,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 969/10000,\n",
      " train_loss: 841.9614,\n",
      " train_mae: 25.4632,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 970/10000,\n",
      " train_loss: 841.9554,\n",
      " train_mae: 25.4630,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 971/10000,\n",
      " train_loss: 841.9498,\n",
      " train_mae: 25.4628,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 972/10000,\n",
      " train_loss: 841.9437,\n",
      " train_mae: 25.4626,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 973/10000,\n",
      " train_loss: 841.9378,\n",
      " train_mae: 25.4624,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 974/10000,\n",
      " train_loss: 841.9321,\n",
      " train_mae: 25.4622,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 975/10000,\n",
      " train_loss: 841.9262,\n",
      " train_mae: 25.4620,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 976/10000,\n",
      " train_loss: 841.9207,\n",
      " train_mae: 25.4618,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 977/10000,\n",
      " train_loss: 841.9152,\n",
      " train_mae: 25.4616,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 978/10000,\n",
      " train_loss: 841.9088,\n",
      " train_mae: 25.4614,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 979/10000,\n",
      " train_loss: 841.9030,\n",
      " train_mae: 25.4612,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 980/10000,\n",
      " train_loss: 841.8976,\n",
      " train_mae: 25.4610,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 981/10000,\n",
      " train_loss: 841.8915,\n",
      " train_mae: 25.4608,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 982/10000,\n",
      " train_loss: 841.8856,\n",
      " train_mae: 25.4606,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 983/10000,\n",
      " train_loss: 841.8798,\n",
      " train_mae: 25.4604,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 984/10000,\n",
      " train_loss: 841.8741,\n",
      " train_mae: 25.4602,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 985/10000,\n",
      " train_loss: 841.8682,\n",
      " train_mae: 25.4599,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 986/10000,\n",
      " train_loss: 841.8625,\n",
      " train_mae: 25.4598,\n",
      " epoch_time_duration: 0.0089\n",
      "\n",
      "epoch: 987/10000,\n",
      " train_loss: 841.8566,\n",
      " train_mae: 25.4596,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 988/10000,\n",
      " train_loss: 841.8510,\n",
      " train_mae: 25.4593,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 989/10000,\n",
      " train_loss: 841.8450,\n",
      " train_mae: 25.4591,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 990/10000,\n",
      " train_loss: 841.8392,\n",
      " train_mae: 25.4589,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 991/10000,\n",
      " train_loss: 841.8339,\n",
      " train_mae: 25.4587,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 992/10000,\n",
      " train_loss: 841.8278,\n",
      " train_mae: 25.4585,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 993/10000,\n",
      " train_loss: 841.8220,\n",
      " train_mae: 25.4583,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 994/10000,\n",
      " train_loss: 841.8163,\n",
      " train_mae: 25.4581,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 995/10000,\n",
      " train_loss: 841.8104,\n",
      " train_mae: 25.4579,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 996/10000,\n",
      " train_loss: 841.8050,\n",
      " train_mae: 25.4577,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 997/10000,\n",
      " train_loss: 841.7991,\n",
      " train_mae: 25.4575,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 998/10000,\n",
      " train_loss: 841.7930,\n",
      " train_mae: 25.4573,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 999/10000,\n",
      " train_loss: 841.7874,\n",
      " train_mae: 25.4571,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1000/10000,\n",
      " train_loss: 841.7816,\n",
      " train_mae: 25.4569,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1001/10000,\n",
      " train_loss: 841.7758,\n",
      " train_mae: 25.4567,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1002/10000,\n",
      " train_loss: 841.7701,\n",
      " train_mae: 25.4565,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1003/10000,\n",
      " train_loss: 841.7646,\n",
      " train_mae: 25.4563,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1004/10000,\n",
      " train_loss: 841.7590,\n",
      " train_mae: 25.4561,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1005/10000,\n",
      " train_loss: 841.7532,\n",
      " train_mae: 25.4559,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1006/10000,\n",
      " train_loss: 841.7477,\n",
      " train_mae: 25.4557,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1007/10000,\n",
      " train_loss: 841.7416,\n",
      " train_mae: 25.4555,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1008/10000,\n",
      " train_loss: 841.7361,\n",
      " train_mae: 25.4553,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1009/10000,\n",
      " train_loss: 841.7301,\n",
      " train_mae: 25.4551,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1010/10000,\n",
      " train_loss: 841.7248,\n",
      " train_mae: 25.4549,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1011/10000,\n",
      " train_loss: 841.7186,\n",
      " train_mae: 25.4546,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 1012/10000,\n",
      " train_loss: 841.7130,\n",
      " train_mae: 25.4544,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1013/10000,\n",
      " train_loss: 841.7073,\n",
      " train_mae: 25.4543,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1014/10000,\n",
      " train_loss: 841.7019,\n",
      " train_mae: 25.4541,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1015/10000,\n",
      " train_loss: 841.6960,\n",
      " train_mae: 25.4538,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1016/10000,\n",
      " train_loss: 841.6904,\n",
      " train_mae: 25.4536,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1017/10000,\n",
      " train_loss: 841.6849,\n",
      " train_mae: 25.4535,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1018/10000,\n",
      " train_loss: 841.6790,\n",
      " train_mae: 25.4532,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1019/10000,\n",
      " train_loss: 841.6733,\n",
      " train_mae: 25.4530,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1020/10000,\n",
      " train_loss: 841.6680,\n",
      " train_mae: 25.4529,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1021/10000,\n",
      " train_loss: 841.6622,\n",
      " train_mae: 25.4527,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1022/10000,\n",
      " train_loss: 841.6567,\n",
      " train_mae: 25.4524,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1023/10000,\n",
      " train_loss: 841.6509,\n",
      " train_mae: 25.4522,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1024/10000,\n",
      " train_loss: 841.6456,\n",
      " train_mae: 25.4521,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1025/10000,\n",
      " train_loss: 841.6400,\n",
      " train_mae: 25.4518,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1026/10000,\n",
      " train_loss: 841.6345,\n",
      " train_mae: 25.4516,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 1027/10000,\n",
      " train_loss: 841.6285,\n",
      " train_mae: 25.4515,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1028/10000,\n",
      " train_loss: 841.6233,\n",
      " train_mae: 25.4512,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1029/10000,\n",
      " train_loss: 841.6172,\n",
      " train_mae: 25.4510,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1030/10000,\n",
      " train_loss: 841.6116,\n",
      " train_mae: 25.4508,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1031/10000,\n",
      " train_loss: 841.6061,\n",
      " train_mae: 25.4506,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1032/10000,\n",
      " train_loss: 841.6007,\n",
      " train_mae: 25.4504,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1033/10000,\n",
      " train_loss: 841.5948,\n",
      " train_mae: 25.4502,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1034/10000,\n",
      " train_loss: 841.5895,\n",
      " train_mae: 25.4500,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1035/10000,\n",
      " train_loss: 841.5834,\n",
      " train_mae: 25.4498,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1036/10000,\n",
      " train_loss: 841.5782,\n",
      " train_mae: 25.4496,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1037/10000,\n",
      " train_loss: 841.5726,\n",
      " train_mae: 25.4494,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 1038/10000,\n",
      " train_loss: 841.5673,\n",
      " train_mae: 25.4492,\n",
      " epoch_time_duration: 0.0109\n",
      "\n",
      "epoch: 1039/10000,\n",
      " train_loss: 841.5616,\n",
      " train_mae: 25.4490,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1040/10000,\n",
      " train_loss: 841.5560,\n",
      " train_mae: 25.4488,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 1041/10000,\n",
      " train_loss: 841.5504,\n",
      " train_mae: 25.4486,\n",
      " epoch_time_duration: 0.0097\n",
      "\n",
      "epoch: 1042/10000,\n",
      " train_loss: 841.5449,\n",
      " train_mae: 25.4484,\n",
      " epoch_time_duration: 0.0084\n",
      "\n",
      "epoch: 1043/10000,\n",
      " train_loss: 841.5395,\n",
      " train_mae: 25.4482,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 1044/10000,\n",
      " train_loss: 841.5335,\n",
      " train_mae: 25.4480,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 1045/10000,\n",
      " train_loss: 841.5283,\n",
      " train_mae: 25.4478,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 1046/10000,\n",
      " train_loss: 841.5228,\n",
      " train_mae: 25.4476,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 1047/10000,\n",
      " train_loss: 841.5175,\n",
      " train_mae: 25.4474,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1048/10000,\n",
      " train_loss: 841.5117,\n",
      " train_mae: 25.4472,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 1049/10000,\n",
      " train_loss: 841.5062,\n",
      " train_mae: 25.4470,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1050/10000,\n",
      " train_loss: 841.5010,\n",
      " train_mae: 25.4468,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1051/10000,\n",
      " train_loss: 841.4952,\n",
      " train_mae: 25.4466,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1052/10000,\n",
      " train_loss: 841.4900,\n",
      " train_mae: 25.4464,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1053/10000,\n",
      " train_loss: 841.4843,\n",
      " train_mae: 25.4462,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1054/10000,\n",
      " train_loss: 841.4788,\n",
      " train_mae: 25.4460,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1055/10000,\n",
      " train_loss: 841.4732,\n",
      " train_mae: 25.4458,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1056/10000,\n",
      " train_loss: 841.4680,\n",
      " train_mae: 25.4456,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1057/10000,\n",
      " train_loss: 841.4623,\n",
      " train_mae: 25.4454,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1058/10000,\n",
      " train_loss: 841.4571,\n",
      " train_mae: 25.4452,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1059/10000,\n",
      " train_loss: 841.4516,\n",
      " train_mae: 25.4450,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1060/10000,\n",
      " train_loss: 841.4460,\n",
      " train_mae: 25.4448,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1061/10000,\n",
      " train_loss: 841.4407,\n",
      " train_mae: 25.4446,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1062/10000,\n",
      " train_loss: 841.4349,\n",
      " train_mae: 25.4444,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1063/10000,\n",
      " train_loss: 841.4296,\n",
      " train_mae: 25.4442,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1064/10000,\n",
      " train_loss: 841.4245,\n",
      " train_mae: 25.4440,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1065/10000,\n",
      " train_loss: 841.4186,\n",
      " train_mae: 25.4438,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1066/10000,\n",
      " train_loss: 841.4135,\n",
      " train_mae: 25.4436,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1067/10000,\n",
      " train_loss: 841.4085,\n",
      " train_mae: 25.4434,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1068/10000,\n",
      " train_loss: 841.4026,\n",
      " train_mae: 25.4432,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 1069/10000,\n",
      " train_loss: 841.3972,\n",
      " train_mae: 25.4430,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1070/10000,\n",
      " train_loss: 841.3918,\n",
      " train_mae: 25.4429,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1071/10000,\n",
      " train_loss: 841.3869,\n",
      " train_mae: 25.4427,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1072/10000,\n",
      " train_loss: 841.3810,\n",
      " train_mae: 25.4424,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1073/10000,\n",
      " train_loss: 841.3755,\n",
      " train_mae: 25.4423,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1074/10000,\n",
      " train_loss: 841.3702,\n",
      " train_mae: 25.4421,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1075/10000,\n",
      " train_loss: 841.3648,\n",
      " train_mae: 25.4418,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1076/10000,\n",
      " train_loss: 841.3596,\n",
      " train_mae: 25.4417,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1077/10000,\n",
      " train_loss: 841.3539,\n",
      " train_mae: 25.4415,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1078/10000,\n",
      " train_loss: 841.3491,\n",
      " train_mae: 25.4413,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1079/10000,\n",
      " train_loss: 841.3435,\n",
      " train_mae: 25.4411,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1080/10000,\n",
      " train_loss: 841.3383,\n",
      " train_mae: 25.4409,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1081/10000,\n",
      " train_loss: 841.3331,\n",
      " train_mae: 25.4407,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1082/10000,\n",
      " train_loss: 841.3279,\n",
      " train_mae: 25.4405,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1083/10000,\n",
      " train_loss: 841.3223,\n",
      " train_mae: 25.4403,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1084/10000,\n",
      " train_loss: 841.3168,\n",
      " train_mae: 25.4401,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1085/10000,\n",
      " train_loss: 841.3114,\n",
      " train_mae: 25.4399,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1086/10000,\n",
      " train_loss: 841.3063,\n",
      " train_mae: 25.4397,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1087/10000,\n",
      " train_loss: 841.3011,\n",
      " train_mae: 25.4395,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1088/10000,\n",
      " train_loss: 841.2956,\n",
      " train_mae: 25.4393,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 1089/10000,\n",
      " train_loss: 841.2904,\n",
      " train_mae: 25.4391,\n",
      " epoch_time_duration: 0.0094\n",
      "\n",
      "epoch: 1090/10000,\n",
      " train_loss: 841.2850,\n",
      " train_mae: 25.4390,\n",
      " epoch_time_duration: 0.0072\n",
      "\n",
      "epoch: 1091/10000,\n",
      " train_loss: 841.2803,\n",
      " train_mae: 25.4387,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 1092/10000,\n",
      " train_loss: 841.2748,\n",
      " train_mae: 25.4385,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 1093/10000,\n",
      " train_loss: 841.2692,\n",
      " train_mae: 25.4384,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1094/10000,\n",
      " train_loss: 841.2641,\n",
      " train_mae: 25.4382,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1095/10000,\n",
      " train_loss: 841.2592,\n",
      " train_mae: 25.4380,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 1096/10000,\n",
      " train_loss: 841.2538,\n",
      " train_mae: 25.4378,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 1097/10000,\n",
      " train_loss: 841.2487,\n",
      " train_mae: 25.4376,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1098/10000,\n",
      " train_loss: 841.2434,\n",
      " train_mae: 25.4374,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1099/10000,\n",
      " train_loss: 841.2379,\n",
      " train_mae: 25.4372,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1100/10000,\n",
      " train_loss: 841.2331,\n",
      " train_mae: 25.4370,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1101/10000,\n",
      " train_loss: 841.2278,\n",
      " train_mae: 25.4368,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1102/10000,\n",
      " train_loss: 841.2226,\n",
      " train_mae: 25.4366,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1103/10000,\n",
      " train_loss: 841.2173,\n",
      " train_mae: 25.4364,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1104/10000,\n",
      " train_loss: 841.2123,\n",
      " train_mae: 25.4362,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1105/10000,\n",
      " train_loss: 841.2066,\n",
      " train_mae: 25.4360,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1106/10000,\n",
      " train_loss: 841.2016,\n",
      " train_mae: 25.4359,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1107/10000,\n",
      " train_loss: 841.1967,\n",
      " train_mae: 25.4356,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1108/10000,\n",
      " train_loss: 841.1913,\n",
      " train_mae: 25.4355,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1109/10000,\n",
      " train_loss: 841.1868,\n",
      " train_mae: 25.4353,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1110/10000,\n",
      " train_loss: 841.1816,\n",
      " train_mae: 25.4351,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1111/10000,\n",
      " train_loss: 841.1762,\n",
      " train_mae: 25.4349,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1112/10000,\n",
      " train_loss: 841.1711,\n",
      " train_mae: 25.4347,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1113/10000,\n",
      " train_loss: 841.1660,\n",
      " train_mae: 25.4345,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1114/10000,\n",
      " train_loss: 841.1609,\n",
      " train_mae: 25.4343,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1115/10000,\n",
      " train_loss: 841.1557,\n",
      " train_mae: 25.4342,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1116/10000,\n",
      " train_loss: 841.1508,\n",
      " train_mae: 25.4339,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1117/10000,\n",
      " train_loss: 841.1458,\n",
      " train_mae: 25.4337,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1118/10000,\n",
      " train_loss: 841.1407,\n",
      " train_mae: 25.4336,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1119/10000,\n",
      " train_loss: 841.1357,\n",
      " train_mae: 25.4334,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1120/10000,\n",
      " train_loss: 841.1305,\n",
      " train_mae: 25.4332,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1121/10000,\n",
      " train_loss: 841.1253,\n",
      " train_mae: 25.4330,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1122/10000,\n",
      " train_loss: 841.1207,\n",
      " train_mae: 25.4328,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1123/10000,\n",
      " train_loss: 841.1155,\n",
      " train_mae: 25.4326,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1124/10000,\n",
      " train_loss: 841.1103,\n",
      " train_mae: 25.4324,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1125/10000,\n",
      " train_loss: 841.1052,\n",
      " train_mae: 25.4322,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1126/10000,\n",
      " train_loss: 841.1002,\n",
      " train_mae: 25.4320,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1127/10000,\n",
      " train_loss: 841.0953,\n",
      " train_mae: 25.4319,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1128/10000,\n",
      " train_loss: 841.0904,\n",
      " train_mae: 25.4317,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1129/10000,\n",
      " train_loss: 841.0852,\n",
      " train_mae: 25.4315,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1130/10000,\n",
      " train_loss: 841.0803,\n",
      " train_mae: 25.4313,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1131/10000,\n",
      " train_loss: 841.0754,\n",
      " train_mae: 25.4311,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1132/10000,\n",
      " train_loss: 841.0701,\n",
      " train_mae: 25.4309,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1133/10000,\n",
      " train_loss: 841.0651,\n",
      " train_mae: 25.4307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1134/10000,\n",
      " train_loss: 841.0600,\n",
      " train_mae: 25.4305,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1135/10000,\n",
      " train_loss: 841.0552,\n",
      " train_mae: 25.4303,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1136/10000,\n",
      " train_loss: 841.0504,\n",
      " train_mae: 25.4302,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1137/10000,\n",
      " train_loss: 841.0455,\n",
      " train_mae: 25.4300,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 1138/10000,\n",
      " train_loss: 841.0403,\n",
      " train_mae: 25.4298,\n",
      " epoch_time_duration: 0.0088\n",
      "\n",
      "epoch: 1139/10000,\n",
      " train_loss: 841.0355,\n",
      " train_mae: 25.4296,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1140/10000,\n",
      " train_loss: 841.0306,\n",
      " train_mae: 25.4294,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 1141/10000,\n",
      " train_loss: 841.0256,\n",
      " train_mae: 25.4292,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1142/10000,\n",
      " train_loss: 841.0207,\n",
      " train_mae: 25.4291,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1143/10000,\n",
      " train_loss: 841.0161,\n",
      " train_mae: 25.4288,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 1144/10000,\n",
      " train_loss: 841.0108,\n",
      " train_mae: 25.4287,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 1145/10000,\n",
      " train_loss: 841.0060,\n",
      " train_mae: 25.4285,\n",
      " epoch_time_duration: 0.0072\n",
      "\n",
      "epoch: 1146/10000,\n",
      " train_loss: 841.0010,\n",
      " train_mae: 25.4283,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 1147/10000,\n",
      " train_loss: 840.9963,\n",
      " train_mae: 25.4281,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 1148/10000,\n",
      " train_loss: 840.9913,\n",
      " train_mae: 25.4279,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1149/10000,\n",
      " train_loss: 840.9863,\n",
      " train_mae: 25.4277,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 1150/10000,\n",
      " train_loss: 840.9818,\n",
      " train_mae: 25.4276,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 1151/10000,\n",
      " train_loss: 840.9771,\n",
      " train_mae: 25.4274,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 1152/10000,\n",
      " train_loss: 840.9720,\n",
      " train_mae: 25.4272,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 1153/10000,\n",
      " train_loss: 840.9671,\n",
      " train_mae: 25.4270,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 1154/10000,\n",
      " train_loss: 840.9623,\n",
      " train_mae: 25.4268,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 1155/10000,\n",
      " train_loss: 840.9575,\n",
      " train_mae: 25.4266,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 1156/10000,\n",
      " train_loss: 840.9528,\n",
      " train_mae: 25.4265,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1157/10000,\n",
      " train_loss: 840.9478,\n",
      " train_mae: 25.4263,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 1158/10000,\n",
      " train_loss: 840.9430,\n",
      " train_mae: 25.4261,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1159/10000,\n",
      " train_loss: 840.9385,\n",
      " train_mae: 25.4259,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1160/10000,\n",
      " train_loss: 840.9338,\n",
      " train_mae: 25.4257,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1161/10000,\n",
      " train_loss: 840.9288,\n",
      " train_mae: 25.4255,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 1162/10000,\n",
      " train_loss: 840.9244,\n",
      " train_mae: 25.4254,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1163/10000,\n",
      " train_loss: 840.9192,\n",
      " train_mae: 25.4252,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1164/10000,\n",
      " train_loss: 840.9146,\n",
      " train_mae: 25.4250,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1165/10000,\n",
      " train_loss: 840.9099,\n",
      " train_mae: 25.4248,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1166/10000,\n",
      " train_loss: 840.9050,\n",
      " train_mae: 25.4246,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1167/10000,\n",
      " train_loss: 840.9003,\n",
      " train_mae: 25.4244,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1168/10000,\n",
      " train_loss: 840.8958,\n",
      " train_mae: 25.4243,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1169/10000,\n",
      " train_loss: 840.8909,\n",
      " train_mae: 25.4241,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1170/10000,\n",
      " train_loss: 840.8864,\n",
      " train_mae: 25.4239,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1171/10000,\n",
      " train_loss: 840.8815,\n",
      " train_mae: 25.4237,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1172/10000,\n",
      " train_loss: 840.8773,\n",
      " train_mae: 25.4235,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1173/10000,\n",
      " train_loss: 840.8724,\n",
      " train_mae: 25.4234,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1174/10000,\n",
      " train_loss: 840.8676,\n",
      " train_mae: 25.4232,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1175/10000,\n",
      " train_loss: 840.8627,\n",
      " train_mae: 25.4230,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1176/10000,\n",
      " train_loss: 840.8583,\n",
      " train_mae: 25.4228,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1177/10000,\n",
      " train_loss: 840.8537,\n",
      " train_mae: 25.4226,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1178/10000,\n",
      " train_loss: 840.8488,\n",
      " train_mae: 25.4224,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1179/10000,\n",
      " train_loss: 840.8444,\n",
      " train_mae: 25.4223,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1180/10000,\n",
      " train_loss: 840.8398,\n",
      " train_mae: 25.4221,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1181/10000,\n",
      " train_loss: 840.8351,\n",
      " train_mae: 25.4219,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1182/10000,\n",
      " train_loss: 840.8304,\n",
      " train_mae: 25.4218,\n",
      " epoch_time_duration: 0.0084\n",
      "\n",
      "epoch: 1183/10000,\n",
      " train_loss: 840.8258,\n",
      " train_mae: 25.4215,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 1184/10000,\n",
      " train_loss: 840.8215,\n",
      " train_mae: 25.4214,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 1185/10000,\n",
      " train_loss: 840.8167,\n",
      " train_mae: 25.4212,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1186/10000,\n",
      " train_loss: 840.8120,\n",
      " train_mae: 25.4210,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1187/10000,\n",
      " train_loss: 840.8078,\n",
      " train_mae: 25.4209,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1188/10000,\n",
      " train_loss: 840.8030,\n",
      " train_mae: 25.4207,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1189/10000,\n",
      " train_loss: 840.7984,\n",
      " train_mae: 25.4205,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1190/10000,\n",
      " train_loss: 840.7941,\n",
      " train_mae: 25.4203,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1191/10000,\n",
      " train_loss: 840.7894,\n",
      " train_mae: 25.4201,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1192/10000,\n",
      " train_loss: 840.7848,\n",
      " train_mae: 25.4200,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1193/10000,\n",
      " train_loss: 840.7804,\n",
      " train_mae: 25.4198,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1194/10000,\n",
      " train_loss: 840.7758,\n",
      " train_mae: 25.4196,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1195/10000,\n",
      " train_loss: 840.7711,\n",
      " train_mae: 25.4195,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1196/10000,\n",
      " train_loss: 840.7666,\n",
      " train_mae: 25.4192,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1197/10000,\n",
      " train_loss: 840.7623,\n",
      " train_mae: 25.4191,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1198/10000,\n",
      " train_loss: 840.7579,\n",
      " train_mae: 25.4190,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1199/10000,\n",
      " train_loss: 840.7533,\n",
      " train_mae: 25.4187,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1200/10000,\n",
      " train_loss: 840.7488,\n",
      " train_mae: 25.4186,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1201/10000,\n",
      " train_loss: 840.7443,\n",
      " train_mae: 25.4184,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1202/10000,\n",
      " train_loss: 840.7399,\n",
      " train_mae: 25.4182,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1203/10000,\n",
      " train_loss: 840.7356,\n",
      " train_mae: 25.4181,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1204/10000,\n",
      " train_loss: 840.7310,\n",
      " train_mae: 25.4179,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1205/10000,\n",
      " train_loss: 840.7267,\n",
      " train_mae: 25.4177,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1206/10000,\n",
      " train_loss: 840.7226,\n",
      " train_mae: 25.4176,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1207/10000,\n",
      " train_loss: 840.7179,\n",
      " train_mae: 25.4173,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1208/10000,\n",
      " train_loss: 840.7134,\n",
      " train_mae: 25.4172,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1209/10000,\n",
      " train_loss: 840.7089,\n",
      " train_mae: 25.4170,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1210/10000,\n",
      " train_loss: 840.7047,\n",
      " train_mae: 25.4168,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1211/10000,\n",
      " train_loss: 840.7003,\n",
      " train_mae: 25.4167,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1212/10000,\n",
      " train_loss: 840.6962,\n",
      " train_mae: 25.4165,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1213/10000,\n",
      " train_loss: 840.6918,\n",
      " train_mae: 25.4163,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1214/10000,\n",
      " train_loss: 840.6873,\n",
      " train_mae: 25.4161,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1215/10000,\n",
      " train_loss: 840.6830,\n",
      " train_mae: 25.4160,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1216/10000,\n",
      " train_loss: 840.6786,\n",
      " train_mae: 25.4158,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1217/10000,\n",
      " train_loss: 840.6743,\n",
      " train_mae: 25.4156,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1218/10000,\n",
      " train_loss: 840.6700,\n",
      " train_mae: 25.4155,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1219/10000,\n",
      " train_loss: 840.6660,\n",
      " train_mae: 25.4153,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1220/10000,\n",
      " train_loss: 840.6615,\n",
      " train_mae: 25.4151,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 1221/10000,\n",
      " train_loss: 840.6572,\n",
      " train_mae: 25.4150,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1222/10000,\n",
      " train_loss: 840.6530,\n",
      " train_mae: 25.4148,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1223/10000,\n",
      " train_loss: 840.6492,\n",
      " train_mae: 25.4146,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1224/10000,\n",
      " train_loss: 840.6445,\n",
      " train_mae: 25.4145,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1225/10000,\n",
      " train_loss: 840.6401,\n",
      " train_mae: 25.4143,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1226/10000,\n",
      " train_loss: 840.6360,\n",
      " train_mae: 25.4141,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1227/10000,\n",
      " train_loss: 840.6316,\n",
      " train_mae: 25.4139,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 1228/10000,\n",
      " train_loss: 840.6276,\n",
      " train_mae: 25.4138,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 1229/10000,\n",
      " train_loss: 840.6232,\n",
      " train_mae: 25.4136,\n",
      " epoch_time_duration: 0.0093\n",
      "\n",
      "epoch: 1230/10000,\n",
      " train_loss: 840.6191,\n",
      " train_mae: 25.4134,\n",
      " epoch_time_duration: 0.0091\n",
      "\n",
      "epoch: 1231/10000,\n",
      " train_loss: 840.6147,\n",
      " train_mae: 25.4133,\n",
      " epoch_time_duration: 0.0081\n",
      "\n",
      "epoch: 1232/10000,\n",
      " train_loss: 840.6106,\n",
      " train_mae: 25.4131,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 1233/10000,\n",
      " train_loss: 840.6066,\n",
      " train_mae: 25.4129,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 1234/10000,\n",
      " train_loss: 840.6022,\n",
      " train_mae: 25.4128,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1235/10000,\n",
      " train_loss: 840.5983,\n",
      " train_mae: 25.4126,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 1236/10000,\n",
      " train_loss: 840.5939,\n",
      " train_mae: 25.4124,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 1237/10000,\n",
      " train_loss: 840.5898,\n",
      " train_mae: 25.4123,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1238/10000,\n",
      " train_loss: 840.5857,\n",
      " train_mae: 25.4121,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1239/10000,\n",
      " train_loss: 840.5814,\n",
      " train_mae: 25.4119,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1240/10000,\n",
      " train_loss: 840.5776,\n",
      " train_mae: 25.4118,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1241/10000,\n",
      " train_loss: 840.5729,\n",
      " train_mae: 25.4116,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1242/10000,\n",
      " train_loss: 840.5693,\n",
      " train_mae: 25.4115,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1243/10000,\n",
      " train_loss: 840.5650,\n",
      " train_mae: 25.4113,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1244/10000,\n",
      " train_loss: 840.5612,\n",
      " train_mae: 25.4111,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1245/10000,\n",
      " train_loss: 840.5570,\n",
      " train_mae: 25.4110,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1246/10000,\n",
      " train_loss: 840.5529,\n",
      " train_mae: 25.4107,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 1247/10000,\n",
      " train_loss: 840.5486,\n",
      " train_mae: 25.4106,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1248/10000,\n",
      " train_loss: 840.5449,\n",
      " train_mae: 25.4105,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1249/10000,\n",
      " train_loss: 840.5405,\n",
      " train_mae: 25.4102,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1250/10000,\n",
      " train_loss: 840.5367,\n",
      " train_mae: 25.4102,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1251/10000,\n",
      " train_loss: 840.5327,\n",
      " train_mae: 25.4099,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1252/10000,\n",
      " train_loss: 840.5287,\n",
      " train_mae: 25.4098,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1253/10000,\n",
      " train_loss: 840.5247,\n",
      " train_mae: 25.4097,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1254/10000,\n",
      " train_loss: 840.5206,\n",
      " train_mae: 25.4094,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1255/10000,\n",
      " train_loss: 840.5163,\n",
      " train_mae: 25.4093,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 1256/10000,\n",
      " train_loss: 840.5127,\n",
      " train_mae: 25.4091,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1257/10000,\n",
      " train_loss: 840.5084,\n",
      " train_mae: 25.4090,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1258/10000,\n",
      " train_loss: 840.5048,\n",
      " train_mae: 25.4089,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1259/10000,\n",
      " train_loss: 840.5005,\n",
      " train_mae: 25.4086,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1260/10000,\n",
      " train_loss: 840.4966,\n",
      " train_mae: 25.4085,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1261/10000,\n",
      " train_loss: 840.4927,\n",
      " train_mae: 25.4083,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1262/10000,\n",
      " train_loss: 840.4889,\n",
      " train_mae: 25.4081,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1263/10000,\n",
      " train_loss: 840.4848,\n",
      " train_mae: 25.4081,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1264/10000,\n",
      " train_loss: 840.4808,\n",
      " train_mae: 25.4078,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1265/10000,\n",
      " train_loss: 840.4772,\n",
      " train_mae: 25.4077,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 1266/10000,\n",
      " train_loss: 840.4733,\n",
      " train_mae: 25.4075,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1267/10000,\n",
      " train_loss: 840.4693,\n",
      " train_mae: 25.4074,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1268/10000,\n",
      " train_loss: 840.4651,\n",
      " train_mae: 25.4072,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 1269/10000,\n",
      " train_loss: 840.4615,\n",
      " train_mae: 25.4071,\n",
      " epoch_time_duration: 0.0074\n",
      "\n",
      "epoch: 1270/10000,\n",
      " train_loss: 840.4576,\n",
      " train_mae: 25.4069,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1271/10000,\n",
      " train_loss: 840.4534,\n",
      " train_mae: 25.4068,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 1272/10000,\n",
      " train_loss: 840.4501,\n",
      " train_mae: 25.4066,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1273/10000,\n",
      " train_loss: 840.4459,\n",
      " train_mae: 25.4064,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1274/10000,\n",
      " train_loss: 840.4420,\n",
      " train_mae: 25.4063,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1275/10000,\n",
      " train_loss: 840.4382,\n",
      " train_mae: 25.4061,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 1276/10000,\n",
      " train_loss: 840.4347,\n",
      " train_mae: 25.4060,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1277/10000,\n",
      " train_loss: 840.4305,\n",
      " train_mae: 25.4058,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1278/10000,\n",
      " train_loss: 840.4268,\n",
      " train_mae: 25.4056,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1279/10000,\n",
      " train_loss: 840.4228,\n",
      " train_mae: 25.4055,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1280/10000,\n",
      " train_loss: 840.4191,\n",
      " train_mae: 25.4053,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1281/10000,\n",
      " train_loss: 840.4152,\n",
      " train_mae: 25.4052,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 1282/10000,\n",
      " train_loss: 840.4117,\n",
      " train_mae: 25.4050,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1283/10000,\n",
      " train_loss: 840.4078,\n",
      " train_mae: 25.4048,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1284/10000,\n",
      " train_loss: 840.4041,\n",
      " train_mae: 25.4047,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1285/10000,\n",
      " train_loss: 840.4003,\n",
      " train_mae: 25.4045,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1286/10000,\n",
      " train_loss: 840.3967,\n",
      " train_mae: 25.4044,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1287/10000,\n",
      " train_loss: 840.3930,\n",
      " train_mae: 25.4042,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1288/10000,\n",
      " train_loss: 840.3892,\n",
      " train_mae: 25.4041,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1289/10000,\n",
      " train_loss: 840.3854,\n",
      " train_mae: 25.4039,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1290/10000,\n",
      " train_loss: 840.3818,\n",
      " train_mae: 25.4038,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1291/10000,\n",
      " train_loss: 840.3782,\n",
      " train_mae: 25.4036,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 1292/10000,\n",
      " train_loss: 840.3746,\n",
      " train_mae: 25.4035,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1293/10000,\n",
      " train_loss: 840.3705,\n",
      " train_mae: 25.4033,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1294/10000,\n",
      " train_loss: 840.3671,\n",
      " train_mae: 25.4032,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1295/10000,\n",
      " train_loss: 840.3633,\n",
      " train_mae: 25.4030,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1296/10000,\n",
      " train_loss: 840.3597,\n",
      " train_mae: 25.4029,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1297/10000,\n",
      " train_loss: 840.3563,\n",
      " train_mae: 25.4027,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 1298/10000,\n",
      " train_loss: 840.3525,\n",
      " train_mae: 25.4026,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1299/10000,\n",
      " train_loss: 840.3488,\n",
      " train_mae: 25.4024,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1300/10000,\n",
      " train_loss: 840.3451,\n",
      " train_mae: 25.4023,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1301/10000,\n",
      " train_loss: 840.3415,\n",
      " train_mae: 25.4021,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1302/10000,\n",
      " train_loss: 840.3380,\n",
      " train_mae: 25.4020,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1303/10000,\n",
      " train_loss: 840.3344,\n",
      " train_mae: 25.4018,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1304/10000,\n",
      " train_loss: 840.3306,\n",
      " train_mae: 25.4016,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1305/10000,\n",
      " train_loss: 840.3270,\n",
      " train_mae: 25.4015,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1306/10000,\n",
      " train_loss: 840.3235,\n",
      " train_mae: 25.4013,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1307/10000,\n",
      " train_loss: 840.3202,\n",
      " train_mae: 25.4012,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1308/10000,\n",
      " train_loss: 840.3167,\n",
      " train_mae: 25.4011,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1309/10000,\n",
      " train_loss: 840.3129,\n",
      " train_mae: 25.4009,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1310/10000,\n",
      " train_loss: 840.3094,\n",
      " train_mae: 25.4008,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 1311/10000,\n",
      " train_loss: 840.3058,\n",
      " train_mae: 25.4006,\n",
      " epoch_time_duration: 0.0072\n",
      "\n",
      "epoch: 1312/10000,\n",
      " train_loss: 840.3022,\n",
      " train_mae: 25.4005,\n",
      " epoch_time_duration: 0.0082\n",
      "\n",
      "epoch: 1313/10000,\n",
      " train_loss: 840.2988,\n",
      " train_mae: 25.4003,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 1314/10000,\n",
      " train_loss: 840.2953,\n",
      " train_mae: 25.4001,\n",
      " epoch_time_duration: 0.0072\n",
      "\n",
      "epoch: 1315/10000,\n",
      " train_loss: 840.2916,\n",
      " train_mae: 25.4000,\n",
      " epoch_time_duration: 0.0073\n",
      "\n",
      "epoch: 1316/10000,\n",
      " train_loss: 840.2883,\n",
      " train_mae: 25.3998,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 1317/10000,\n",
      " train_loss: 840.2846,\n",
      " train_mae: 25.3997,\n",
      " epoch_time_duration: 0.0073\n",
      "\n",
      "epoch: 1318/10000,\n",
      " train_loss: 840.2812,\n",
      " train_mae: 25.3995,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 1319/10000,\n",
      " train_loss: 840.2779,\n",
      " train_mae: 25.3994,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 1320/10000,\n",
      " train_loss: 840.2745,\n",
      " train_mae: 25.3993,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 1321/10000,\n",
      " train_loss: 840.2709,\n",
      " train_mae: 25.3991,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1322/10000,\n",
      " train_loss: 840.2673,\n",
      " train_mae: 25.3990,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1323/10000,\n",
      " train_loss: 840.2639,\n",
      " train_mae: 25.3988,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1324/10000,\n",
      " train_loss: 840.2604,\n",
      " train_mae: 25.3987,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1325/10000,\n",
      " train_loss: 840.2571,\n",
      " train_mae: 25.3985,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1326/10000,\n",
      " train_loss: 840.2538,\n",
      " train_mae: 25.3984,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1327/10000,\n",
      " train_loss: 840.2503,\n",
      " train_mae: 25.3983,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1328/10000,\n",
      " train_loss: 840.2468,\n",
      " train_mae: 25.3981,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 1329/10000,\n",
      " train_loss: 840.2436,\n",
      " train_mae: 25.3980,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1330/10000,\n",
      " train_loss: 840.2403,\n",
      " train_mae: 25.3978,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 1331/10000,\n",
      " train_loss: 840.2369,\n",
      " train_mae: 25.3977,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1332/10000,\n",
      " train_loss: 840.2333,\n",
      " train_mae: 25.3975,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 1333/10000,\n",
      " train_loss: 840.2301,\n",
      " train_mae: 25.3974,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 1334/10000,\n",
      " train_loss: 840.2269,\n",
      " train_mae: 25.3973,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 1335/10000,\n",
      " train_loss: 840.2233,\n",
      " train_mae: 25.3971,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1336/10000,\n",
      " train_loss: 840.2200,\n",
      " train_mae: 25.3970,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1337/10000,\n",
      " train_loss: 840.2170,\n",
      " train_mae: 25.3968,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 1338/10000,\n",
      " train_loss: 840.2136,\n",
      " train_mae: 25.3967,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 1339/10000,\n",
      " train_loss: 840.2102,\n",
      " train_mae: 25.3965,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 1340/10000,\n",
      " train_loss: 840.2067,\n",
      " train_mae: 25.3964,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 1341/10000,\n",
      " train_loss: 840.2037,\n",
      " train_mae: 25.3963,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 1342/10000,\n",
      " train_loss: 840.2004,\n",
      " train_mae: 25.3961,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 1343/10000,\n",
      " train_loss: 840.1970,\n",
      " train_mae: 25.3960,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1344/10000,\n",
      " train_loss: 840.1936,\n",
      " train_mae: 25.3958,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1345/10000,\n",
      " train_loss: 840.1904,\n",
      " train_mae: 25.3957,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1346/10000,\n",
      " train_loss: 840.1870,\n",
      " train_mae: 25.3955,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1347/10000,\n",
      " train_loss: 840.1843,\n",
      " train_mae: 25.3954,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1348/10000,\n",
      " train_loss: 840.1807,\n",
      " train_mae: 25.3953,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 1349/10000,\n",
      " train_loss: 840.1778,\n",
      " train_mae: 25.3951,\n",
      " epoch_time_duration: 0.0118\n",
      "\n",
      "epoch: 1350/10000,\n",
      " train_loss: 840.1741,\n",
      " train_mae: 25.3950,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1351/10000,\n",
      " train_loss: 840.1708,\n",
      " train_mae: 25.3948,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1352/10000,\n",
      " train_loss: 840.1677,\n",
      " train_mae: 25.3947,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 1353/10000,\n",
      " train_loss: 840.1644,\n",
      " train_mae: 25.3946,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 1354/10000,\n",
      " train_loss: 840.1614,\n",
      " train_mae: 25.3945,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1355/10000,\n",
      " train_loss: 840.1585,\n",
      " train_mae: 25.3943,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1356/10000,\n",
      " train_loss: 840.1549,\n",
      " train_mae: 25.3942,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1357/10000,\n",
      " train_loss: 840.1520,\n",
      " train_mae: 25.3940,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1358/10000,\n",
      " train_loss: 840.1489,\n",
      " train_mae: 25.3938,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1359/10000,\n",
      " train_loss: 840.1455,\n",
      " train_mae: 25.3938,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1360/10000,\n",
      " train_loss: 840.1425,\n",
      " train_mae: 25.3936,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1361/10000,\n",
      " train_loss: 840.1392,\n",
      " train_mae: 25.3935,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1362/10000,\n",
      " train_loss: 840.1362,\n",
      " train_mae: 25.3933,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1363/10000,\n",
      " train_loss: 840.1331,\n",
      " train_mae: 25.3932,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1364/10000,\n",
      " train_loss: 840.1301,\n",
      " train_mae: 25.3931,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1365/10000,\n",
      " train_loss: 840.1269,\n",
      " train_mae: 25.3929,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1366/10000,\n",
      " train_loss: 840.1237,\n",
      " train_mae: 25.3928,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1367/10000,\n",
      " train_loss: 840.1208,\n",
      " train_mae: 25.3927,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1368/10000,\n",
      " train_loss: 840.1176,\n",
      " train_mae: 25.3925,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1369/10000,\n",
      " train_loss: 840.1145,\n",
      " train_mae: 25.3924,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1370/10000,\n",
      " train_loss: 840.1112,\n",
      " train_mae: 25.3922,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1371/10000,\n",
      " train_loss: 840.1084,\n",
      " train_mae: 25.3921,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1372/10000,\n",
      " train_loss: 840.1056,\n",
      " train_mae: 25.3920,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1373/10000,\n",
      " train_loss: 840.1025,\n",
      " train_mae: 25.3918,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1374/10000,\n",
      " train_loss: 840.0992,\n",
      " train_mae: 25.3917,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1375/10000,\n",
      " train_loss: 840.0960,\n",
      " train_mae: 25.3916,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1376/10000,\n",
      " train_loss: 840.0932,\n",
      " train_mae: 25.3915,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1377/10000,\n",
      " train_loss: 840.0902,\n",
      " train_mae: 25.3913,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1378/10000,\n",
      " train_loss: 840.0872,\n",
      " train_mae: 25.3912,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 1379/10000,\n",
      " train_loss: 840.0845,\n",
      " train_mae: 25.3911,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1380/10000,\n",
      " train_loss: 840.0812,\n",
      " train_mae: 25.3909,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1381/10000,\n",
      " train_loss: 840.0786,\n",
      " train_mae: 25.3908,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1382/10000,\n",
      " train_loss: 840.0754,\n",
      " train_mae: 25.3906,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1383/10000,\n",
      " train_loss: 840.0724,\n",
      " train_mae: 25.3905,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1384/10000,\n",
      " train_loss: 840.0692,\n",
      " train_mae: 25.3904,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1385/10000,\n",
      " train_loss: 840.0663,\n",
      " train_mae: 25.3903,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1386/10000,\n",
      " train_loss: 840.0635,\n",
      " train_mae: 25.3901,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1387/10000,\n",
      " train_loss: 840.0605,\n",
      " train_mae: 25.3900,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1388/10000,\n",
      " train_loss: 840.0577,\n",
      " train_mae: 25.3898,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 1389/10000,\n",
      " train_loss: 840.0547,\n",
      " train_mae: 25.3898,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1390/10000,\n",
      " train_loss: 840.0519,\n",
      " train_mae: 25.3896,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 1391/10000,\n",
      " train_loss: 840.0486,\n",
      " train_mae: 25.3895,\n",
      " epoch_time_duration: 0.0080\n",
      "\n",
      "epoch: 1392/10000,\n",
      " train_loss: 840.0460,\n",
      " train_mae: 25.3894,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1393/10000,\n",
      " train_loss: 840.0430,\n",
      " train_mae: 25.3892,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1394/10000,\n",
      " train_loss: 840.0400,\n",
      " train_mae: 25.3891,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1395/10000,\n",
      " train_loss: 840.0374,\n",
      " train_mae: 25.3890,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1396/10000,\n",
      " train_loss: 840.0344,\n",
      " train_mae: 25.3888,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 1397/10000,\n",
      " train_loss: 840.0319,\n",
      " train_mae: 25.3887,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 1398/10000,\n",
      " train_loss: 840.0287,\n",
      " train_mae: 25.3886,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 1399/10000,\n",
      " train_loss: 840.0258,\n",
      " train_mae: 25.3885,\n",
      " epoch_time_duration: 0.0077\n",
      "\n",
      "epoch: 1400/10000,\n",
      " train_loss: 840.0229,\n",
      " train_mae: 25.3883,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1401/10000,\n",
      " train_loss: 840.0204,\n",
      " train_mae: 25.3882,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 1402/10000,\n",
      " train_loss: 840.0173,\n",
      " train_mae: 25.3881,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 1403/10000,\n",
      " train_loss: 840.0143,\n",
      " train_mae: 25.3880,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1404/10000,\n",
      " train_loss: 840.0118,\n",
      " train_mae: 25.3878,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1405/10000,\n",
      " train_loss: 840.0089,\n",
      " train_mae: 25.3877,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1406/10000,\n",
      " train_loss: 840.0062,\n",
      " train_mae: 25.3876,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1407/10000,\n",
      " train_loss: 840.0034,\n",
      " train_mae: 25.3874,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 1408/10000,\n",
      " train_loss: 840.0005,\n",
      " train_mae: 25.3873,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1409/10000,\n",
      " train_loss: 839.9978,\n",
      " train_mae: 25.3872,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1410/10000,\n",
      " train_loss: 839.9951,\n",
      " train_mae: 25.3871,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1411/10000,\n",
      " train_loss: 839.9923,\n",
      " train_mae: 25.3870,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1412/10000,\n",
      " train_loss: 839.9896,\n",
      " train_mae: 25.3868,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1413/10000,\n",
      " train_loss: 839.9869,\n",
      " train_mae: 25.3867,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1414/10000,\n",
      " train_loss: 839.9838,\n",
      " train_mae: 25.3866,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1415/10000,\n",
      " train_loss: 839.9815,\n",
      " train_mae: 25.3865,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1416/10000,\n",
      " train_loss: 839.9786,\n",
      " train_mae: 25.3863,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1417/10000,\n",
      " train_loss: 839.9759,\n",
      " train_mae: 25.3862,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1418/10000,\n",
      " train_loss: 839.9732,\n",
      " train_mae: 25.3861,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1419/10000,\n",
      " train_loss: 839.9706,\n",
      " train_mae: 25.3859,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1420/10000,\n",
      " train_loss: 839.9679,\n",
      " train_mae: 25.3858,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1421/10000,\n",
      " train_loss: 839.9652,\n",
      " train_mae: 25.3857,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1422/10000,\n",
      " train_loss: 839.9623,\n",
      " train_mae: 25.3856,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1423/10000,\n",
      " train_loss: 839.9596,\n",
      " train_mae: 25.3855,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1424/10000,\n",
      " train_loss: 839.9573,\n",
      " train_mae: 25.3853,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1425/10000,\n",
      " train_loss: 839.9545,\n",
      " train_mae: 25.3852,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 1426/10000,\n",
      " train_loss: 839.9518,\n",
      " train_mae: 25.3851,\n",
      " epoch_time_duration: 0.0107\n",
      "\n",
      "epoch: 1427/10000,\n",
      " train_loss: 839.9490,\n",
      " train_mae: 25.3850,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 1428/10000,\n",
      " train_loss: 839.9464,\n",
      " train_mae: 25.3849,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 1429/10000,\n",
      " train_loss: 839.9439,\n",
      " train_mae: 25.3847,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 1430/10000,\n",
      " train_loss: 839.9413,\n",
      " train_mae: 25.3846,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 1431/10000,\n",
      " train_loss: 839.9385,\n",
      " train_mae: 25.3845,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 1432/10000,\n",
      " train_loss: 839.9360,\n",
      " train_mae: 25.3844,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1433/10000,\n",
      " train_loss: 839.9333,\n",
      " train_mae: 25.3843,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1434/10000,\n",
      " train_loss: 839.9305,\n",
      " train_mae: 25.3841,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1435/10000,\n",
      " train_loss: 839.9282,\n",
      " train_mae: 25.3840,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1436/10000,\n",
      " train_loss: 839.9257,\n",
      " train_mae: 25.3839,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1437/10000,\n",
      " train_loss: 839.9228,\n",
      " train_mae: 25.3838,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1438/10000,\n",
      " train_loss: 839.9205,\n",
      " train_mae: 25.3837,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1439/10000,\n",
      " train_loss: 839.9175,\n",
      " train_mae: 25.3835,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 1440/10000,\n",
      " train_loss: 839.9153,\n",
      " train_mae: 25.3834,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1441/10000,\n",
      " train_loss: 839.9127,\n",
      " train_mae: 25.3833,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1442/10000,\n",
      " train_loss: 839.9103,\n",
      " train_mae: 25.3832,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1443/10000,\n",
      " train_loss: 839.9076,\n",
      " train_mae: 25.3831,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1444/10000,\n",
      " train_loss: 839.9053,\n",
      " train_mae: 25.3830,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1445/10000,\n",
      " train_loss: 839.9025,\n",
      " train_mae: 25.3829,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1446/10000,\n",
      " train_loss: 839.9000,\n",
      " train_mae: 25.3827,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1447/10000,\n",
      " train_loss: 839.8975,\n",
      " train_mae: 25.3826,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 1448/10000,\n",
      " train_loss: 839.8952,\n",
      " train_mae: 25.3825,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1449/10000,\n",
      " train_loss: 839.8927,\n",
      " train_mae: 25.3824,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 1450/10000,\n",
      " train_loss: 839.8898,\n",
      " train_mae: 25.3823,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1451/10000,\n",
      " train_loss: 839.8876,\n",
      " train_mae: 25.3822,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 1452/10000,\n",
      " train_loss: 839.8852,\n",
      " train_mae: 25.3820,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1453/10000,\n",
      " train_loss: 839.8829,\n",
      " train_mae: 25.3819,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 1454/10000,\n",
      " train_loss: 839.8801,\n",
      " train_mae: 25.3818,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 1455/10000,\n",
      " train_loss: 839.8779,\n",
      " train_mae: 25.3817,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 1456/10000,\n",
      " train_loss: 839.8753,\n",
      " train_mae: 25.3816,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1457/10000,\n",
      " train_loss: 839.8728,\n",
      " train_mae: 25.3815,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1458/10000,\n",
      " train_loss: 839.8705,\n",
      " train_mae: 25.3814,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1459/10000,\n",
      " train_loss: 839.8679,\n",
      " train_mae: 25.3812,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1460/10000,\n",
      " train_loss: 839.8658,\n",
      " train_mae: 25.3812,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1461/10000,\n",
      " train_loss: 839.8632,\n",
      " train_mae: 25.3810,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 1462/10000,\n",
      " train_loss: 839.8610,\n",
      " train_mae: 25.3809,\n",
      " epoch_time_duration: 0.0129\n",
      "\n",
      "epoch: 1463/10000,\n",
      " train_loss: 839.8583,\n",
      " train_mae: 25.3808,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 1464/10000,\n",
      " train_loss: 839.8558,\n",
      " train_mae: 25.3806,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 1465/10000,\n",
      " train_loss: 839.8537,\n",
      " train_mae: 25.3806,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 1466/10000,\n",
      " train_loss: 839.8512,\n",
      " train_mae: 25.3804,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 1467/10000,\n",
      " train_loss: 839.8490,\n",
      " train_mae: 25.3804,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 1468/10000,\n",
      " train_loss: 839.8464,\n",
      " train_mae: 25.3802,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 1469/10000,\n",
      " train_loss: 839.8440,\n",
      " train_mae: 25.3801,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 1470/10000,\n",
      " train_loss: 839.8417,\n",
      " train_mae: 25.3800,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 1471/10000,\n",
      " train_loss: 839.8393,\n",
      " train_mae: 25.3799,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 1472/10000,\n",
      " train_loss: 839.8370,\n",
      " train_mae: 25.3798,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 1473/10000,\n",
      " train_loss: 839.8346,\n",
      " train_mae: 25.3797,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 1474/10000,\n",
      " train_loss: 839.8324,\n",
      " train_mae: 25.3796,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 1475/10000,\n",
      " train_loss: 839.8298,\n",
      " train_mae: 25.3795,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 1476/10000,\n",
      " train_loss: 839.8278,\n",
      " train_mae: 25.3793,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 1477/10000,\n",
      " train_loss: 839.8254,\n",
      " train_mae: 25.3793,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 1478/10000,\n",
      " train_loss: 839.8232,\n",
      " train_mae: 25.3791,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 1479/10000,\n",
      " train_loss: 839.8209,\n",
      " train_mae: 25.3790,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 1480/10000,\n",
      " train_loss: 839.8185,\n",
      " train_mae: 25.3789,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 1481/10000,\n",
      " train_loss: 839.8163,\n",
      " train_mae: 25.3788,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 1482/10000,\n",
      " train_loss: 839.8136,\n",
      " train_mae: 25.3787,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1483/10000,\n",
      " train_loss: 839.8116,\n",
      " train_mae: 25.3786,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1484/10000,\n",
      " train_loss: 839.8093,\n",
      " train_mae: 25.3785,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1485/10000,\n",
      " train_loss: 839.8071,\n",
      " train_mae: 25.3784,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1486/10000,\n",
      " train_loss: 839.8049,\n",
      " train_mae: 25.3782,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1487/10000,\n",
      " train_loss: 839.8027,\n",
      " train_mae: 25.3782,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1488/10000,\n",
      " train_loss: 839.8002,\n",
      " train_mae: 25.3780,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1489/10000,\n",
      " train_loss: 839.7982,\n",
      " train_mae: 25.3779,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1490/10000,\n",
      " train_loss: 839.7960,\n",
      " train_mae: 25.3778,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1491/10000,\n",
      " train_loss: 839.7936,\n",
      " train_mae: 25.3777,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1492/10000,\n",
      " train_loss: 839.7914,\n",
      " train_mae: 25.3776,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1493/10000,\n",
      " train_loss: 839.7892,\n",
      " train_mae: 25.3775,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 1494/10000,\n",
      " train_loss: 839.7872,\n",
      " train_mae: 25.3774,\n",
      " epoch_time_duration: 0.0080\n",
      "\n",
      "epoch: 1495/10000,\n",
      " train_loss: 839.7848,\n",
      " train_mae: 25.3773,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 1496/10000,\n",
      " train_loss: 839.7825,\n",
      " train_mae: 25.3772,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1497/10000,\n",
      " train_loss: 839.7804,\n",
      " train_mae: 25.3771,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1498/10000,\n",
      " train_loss: 839.7782,\n",
      " train_mae: 25.3770,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 1499/10000,\n",
      " train_loss: 839.7763,\n",
      " train_mae: 25.3769,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1500/10000,\n",
      " train_loss: 839.7740,\n",
      " train_mae: 25.3768,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1501/10000,\n",
      " train_loss: 839.7717,\n",
      " train_mae: 25.3767,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1502/10000,\n",
      " train_loss: 839.7696,\n",
      " train_mae: 25.3766,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 1503/10000,\n",
      " train_loss: 839.7673,\n",
      " train_mae: 25.3765,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1504/10000,\n",
      " train_loss: 839.7654,\n",
      " train_mae: 25.3764,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1505/10000,\n",
      " train_loss: 839.7631,\n",
      " train_mae: 25.3763,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1506/10000,\n",
      " train_loss: 839.7611,\n",
      " train_mae: 25.3761,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1507/10000,\n",
      " train_loss: 839.7588,\n",
      " train_mae: 25.3761,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1508/10000,\n",
      " train_loss: 839.7567,\n",
      " train_mae: 25.3759,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1509/10000,\n",
      " train_loss: 839.7548,\n",
      " train_mae: 25.3759,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1510/10000,\n",
      " train_loss: 839.7526,\n",
      " train_mae: 25.3757,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1511/10000,\n",
      " train_loss: 839.7505,\n",
      " train_mae: 25.3756,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1512/10000,\n",
      " train_loss: 839.7484,\n",
      " train_mae: 25.3756,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1513/10000,\n",
      " train_loss: 839.7464,\n",
      " train_mae: 25.3754,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1514/10000,\n",
      " train_loss: 839.7441,\n",
      " train_mae: 25.3753,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1515/10000,\n",
      " train_loss: 839.7421,\n",
      " train_mae: 25.3753,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1516/10000,\n",
      " train_loss: 839.7400,\n",
      " train_mae: 25.3751,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1517/10000,\n",
      " train_loss: 839.7380,\n",
      " train_mae: 25.3751,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1518/10000,\n",
      " train_loss: 839.7357,\n",
      " train_mae: 25.3749,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1519/10000,\n",
      " train_loss: 839.7340,\n",
      " train_mae: 25.3749,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1520/10000,\n",
      " train_loss: 839.7318,\n",
      " train_mae: 25.3747,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1521/10000,\n",
      " train_loss: 839.7297,\n",
      " train_mae: 25.3747,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1522/10000,\n",
      " train_loss: 839.7277,\n",
      " train_mae: 25.3745,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1523/10000,\n",
      " train_loss: 839.7253,\n",
      " train_mae: 25.3744,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1524/10000,\n",
      " train_loss: 839.7236,\n",
      " train_mae: 25.3744,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1525/10000,\n",
      " train_loss: 839.7216,\n",
      " train_mae: 25.3742,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1526/10000,\n",
      " train_loss: 839.7194,\n",
      " train_mae: 25.3742,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1527/10000,\n",
      " train_loss: 839.7177,\n",
      " train_mae: 25.3741,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1528/10000,\n",
      " train_loss: 839.7155,\n",
      " train_mae: 25.3739,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1529/10000,\n",
      " train_loss: 839.7137,\n",
      " train_mae: 25.3739,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1530/10000,\n",
      " train_loss: 839.7116,\n",
      " train_mae: 25.3737,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1531/10000,\n",
      " train_loss: 839.7095,\n",
      " train_mae: 25.3737,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1532/10000,\n",
      " train_loss: 839.7076,\n",
      " train_mae: 25.3736,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1533/10000,\n",
      " train_loss: 839.7054,\n",
      " train_mae: 25.3734,\n",
      " epoch_time_duration: 0.0095\n",
      "\n",
      "epoch: 1534/10000,\n",
      " train_loss: 839.7037,\n",
      " train_mae: 25.3734,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 1535/10000,\n",
      " train_loss: 839.7017,\n",
      " train_mae: 25.3732,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 1536/10000,\n",
      " train_loss: 839.6998,\n",
      " train_mae: 25.3732,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1537/10000,\n",
      " train_loss: 839.6978,\n",
      " train_mae: 25.3731,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 1538/10000,\n",
      " train_loss: 839.6959,\n",
      " train_mae: 25.3730,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 1539/10000,\n",
      " train_loss: 839.6940,\n",
      " train_mae: 25.3729,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 1540/10000,\n",
      " train_loss: 839.6920,\n",
      " train_mae: 25.3728,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1541/10000,\n",
      " train_loss: 839.6900,\n",
      " train_mae: 25.3727,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 1542/10000,\n",
      " train_loss: 839.6882,\n",
      " train_mae: 25.3726,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1543/10000,\n",
      " train_loss: 839.6860,\n",
      " train_mae: 25.3725,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1544/10000,\n",
      " train_loss: 839.6843,\n",
      " train_mae: 25.3725,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1545/10000,\n",
      " train_loss: 839.6823,\n",
      " train_mae: 25.3723,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1546/10000,\n",
      " train_loss: 839.6804,\n",
      " train_mae: 25.3722,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1547/10000,\n",
      " train_loss: 839.6783,\n",
      " train_mae: 25.3722,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1548/10000,\n",
      " train_loss: 839.6765,\n",
      " train_mae: 25.3720,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1549/10000,\n",
      " train_loss: 839.6745,\n",
      " train_mae: 25.3720,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1550/10000,\n",
      " train_loss: 839.6727,\n",
      " train_mae: 25.3718,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1551/10000,\n",
      " train_loss: 839.6710,\n",
      " train_mae: 25.3717,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1552/10000,\n",
      " train_loss: 839.6691,\n",
      " train_mae: 25.3717,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1553/10000,\n",
      " train_loss: 839.6672,\n",
      " train_mae: 25.3715,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1554/10000,\n",
      " train_loss: 839.6655,\n",
      " train_mae: 25.3715,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1555/10000,\n",
      " train_loss: 839.6633,\n",
      " train_mae: 25.3714,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1556/10000,\n",
      " train_loss: 839.6617,\n",
      " train_mae: 25.3713,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1557/10000,\n",
      " train_loss: 839.6597,\n",
      " train_mae: 25.3712,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1558/10000,\n",
      " train_loss: 839.6578,\n",
      " train_mae: 25.3711,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1559/10000,\n",
      " train_loss: 839.6560,\n",
      " train_mae: 25.3710,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1560/10000,\n",
      " train_loss: 839.6543,\n",
      " train_mae: 25.3709,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 1561/10000,\n",
      " train_loss: 839.6525,\n",
      " train_mae: 25.3708,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 1562/10000,\n",
      " train_loss: 839.6506,\n",
      " train_mae: 25.3707,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 1563/10000,\n",
      " train_loss: 839.6487,\n",
      " train_mae: 25.3706,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 1564/10000,\n",
      " train_loss: 839.6470,\n",
      " train_mae: 25.3706,\n",
      " epoch_time_duration: 0.0073\n",
      "\n",
      "epoch: 1565/10000,\n",
      " train_loss: 839.6451,\n",
      " train_mae: 25.3704,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 1566/10000,\n",
      " train_loss: 839.6433,\n",
      " train_mae: 25.3704,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 1567/10000,\n",
      " train_loss: 839.6416,\n",
      " train_mae: 25.3703,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 1568/10000,\n",
      " train_loss: 839.6396,\n",
      " train_mae: 25.3702,\n",
      " epoch_time_duration: 0.0124\n",
      "\n",
      "epoch: 1569/10000,\n",
      " train_loss: 839.6380,\n",
      " train_mae: 25.3701,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 1570/10000,\n",
      " train_loss: 839.6362,\n",
      " train_mae: 25.3700,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 1571/10000,\n",
      " train_loss: 839.6342,\n",
      " train_mae: 25.3699,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 1572/10000,\n",
      " train_loss: 839.6325,\n",
      " train_mae: 25.3699,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 1573/10000,\n",
      " train_loss: 839.6309,\n",
      " train_mae: 25.3697,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 1574/10000,\n",
      " train_loss: 839.6289,\n",
      " train_mae: 25.3697,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 1575/10000,\n",
      " train_loss: 839.6271,\n",
      " train_mae: 25.3696,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 1576/10000,\n",
      " train_loss: 839.6255,\n",
      " train_mae: 25.3695,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 1577/10000,\n",
      " train_loss: 839.6239,\n",
      " train_mae: 25.3694,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1578/10000,\n",
      " train_loss: 839.6221,\n",
      " train_mae: 25.3693,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 1579/10000,\n",
      " train_loss: 839.6202,\n",
      " train_mae: 25.3692,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 1580/10000,\n",
      " train_loss: 839.6186,\n",
      " train_mae: 25.3691,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1581/10000,\n",
      " train_loss: 839.6168,\n",
      " train_mae: 25.3690,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1582/10000,\n",
      " train_loss: 839.6151,\n",
      " train_mae: 25.3689,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1583/10000,\n",
      " train_loss: 839.6135,\n",
      " train_mae: 25.3689,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 1584/10000,\n",
      " train_loss: 839.6116,\n",
      " train_mae: 25.3688,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1585/10000,\n",
      " train_loss: 839.6099,\n",
      " train_mae: 25.3687,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1586/10000,\n",
      " train_loss: 839.6083,\n",
      " train_mae: 25.3686,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 1587/10000,\n",
      " train_loss: 839.6066,\n",
      " train_mae: 25.3685,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1588/10000,\n",
      " train_loss: 839.6048,\n",
      " train_mae: 25.3684,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1589/10000,\n",
      " train_loss: 839.6031,\n",
      " train_mae: 25.3683,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1590/10000,\n",
      " train_loss: 839.6012,\n",
      " train_mae: 25.3682,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1591/10000,\n",
      " train_loss: 839.5997,\n",
      " train_mae: 25.3682,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1592/10000,\n",
      " train_loss: 839.5980,\n",
      " train_mae: 25.3680,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1593/10000,\n",
      " train_loss: 839.5964,\n",
      " train_mae: 25.3680,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1594/10000,\n",
      " train_loss: 839.5944,\n",
      " train_mae: 25.3679,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1595/10000,\n",
      " train_loss: 839.5931,\n",
      " train_mae: 25.3678,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1596/10000,\n",
      " train_loss: 839.5912,\n",
      " train_mae: 25.3678,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1597/10000,\n",
      " train_loss: 839.5898,\n",
      " train_mae: 25.3676,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1598/10000,\n",
      " train_loss: 839.5881,\n",
      " train_mae: 25.3676,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1599/10000,\n",
      " train_loss: 839.5865,\n",
      " train_mae: 25.3675,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1600/10000,\n",
      " train_loss: 839.5848,\n",
      " train_mae: 25.3674,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1601/10000,\n",
      " train_loss: 839.5833,\n",
      " train_mae: 25.3673,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1602/10000,\n",
      " train_loss: 839.5814,\n",
      " train_mae: 25.3672,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1603/10000,\n",
      " train_loss: 839.5800,\n",
      " train_mae: 25.3672,\n",
      " epoch_time_duration: 0.0074\n",
      "\n",
      "epoch: 1604/10000,\n",
      " train_loss: 839.5784,\n",
      " train_mae: 25.3671,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 1605/10000,\n",
      " train_loss: 839.5766,\n",
      " train_mae: 25.3670,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 1606/10000,\n",
      " train_loss: 839.5751,\n",
      " train_mae: 25.3669,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1607/10000,\n",
      " train_loss: 839.5735,\n",
      " train_mae: 25.3668,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1608/10000,\n",
      " train_loss: 839.5719,\n",
      " train_mae: 25.3668,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1609/10000,\n",
      " train_loss: 839.5703,\n",
      " train_mae: 25.3667,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1610/10000,\n",
      " train_loss: 839.5687,\n",
      " train_mae: 25.3666,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1611/10000,\n",
      " train_loss: 839.5668,\n",
      " train_mae: 25.3665,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1612/10000,\n",
      " train_loss: 839.5656,\n",
      " train_mae: 25.3664,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1613/10000,\n",
      " train_loss: 839.5637,\n",
      " train_mae: 25.3663,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1614/10000,\n",
      " train_loss: 839.5623,\n",
      " train_mae: 25.3663,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1615/10000,\n",
      " train_loss: 839.5607,\n",
      " train_mae: 25.3662,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1616/10000,\n",
      " train_loss: 839.5591,\n",
      " train_mae: 25.3661,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1617/10000,\n",
      " train_loss: 839.5575,\n",
      " train_mae: 25.3660,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1618/10000,\n",
      " train_loss: 839.5558,\n",
      " train_mae: 25.3659,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1619/10000,\n",
      " train_loss: 839.5544,\n",
      " train_mae: 25.3659,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1620/10000,\n",
      " train_loss: 839.5529,\n",
      " train_mae: 25.3658,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1621/10000,\n",
      " train_loss: 839.5512,\n",
      " train_mae: 25.3657,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1622/10000,\n",
      " train_loss: 839.5499,\n",
      " train_mae: 25.3656,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1623/10000,\n",
      " train_loss: 839.5483,\n",
      " train_mae: 25.3655,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1624/10000,\n",
      " train_loss: 839.5469,\n",
      " train_mae: 25.3654,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1625/10000,\n",
      " train_loss: 839.5452,\n",
      " train_mae: 25.3654,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1626/10000,\n",
      " train_loss: 839.5436,\n",
      " train_mae: 25.3653,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1627/10000,\n",
      " train_loss: 839.5424,\n",
      " train_mae: 25.3652,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1628/10000,\n",
      " train_loss: 839.5408,\n",
      " train_mae: 25.3651,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1629/10000,\n",
      " train_loss: 839.5392,\n",
      " train_mae: 25.3651,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1630/10000,\n",
      " train_loss: 839.5377,\n",
      " train_mae: 25.3650,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1631/10000,\n",
      " train_loss: 839.5360,\n",
      " train_mae: 25.3649,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1632/10000,\n",
      " train_loss: 839.5344,\n",
      " train_mae: 25.3648,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 1633/10000,\n",
      " train_loss: 839.5333,\n",
      " train_mae: 25.3647,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 1634/10000,\n",
      " train_loss: 839.5317,\n",
      " train_mae: 25.3646,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1635/10000,\n",
      " train_loss: 839.5302,\n",
      " train_mae: 25.3646,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1636/10000,\n",
      " train_loss: 839.5286,\n",
      " train_mae: 25.3645,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 1637/10000,\n",
      " train_loss: 839.5271,\n",
      " train_mae: 25.3644,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 1638/10000,\n",
      " train_loss: 839.5258,\n",
      " train_mae: 25.3644,\n",
      " epoch_time_duration: 0.0084\n",
      "\n",
      "epoch: 1639/10000,\n",
      " train_loss: 839.5244,\n",
      " train_mae: 25.3643,\n",
      " epoch_time_duration: 0.0111\n",
      "\n",
      "epoch: 1640/10000,\n",
      " train_loss: 839.5228,\n",
      " train_mae: 25.3642,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 1641/10000,\n",
      " train_loss: 839.5212,\n",
      " train_mae: 25.3641,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 1642/10000,\n",
      " train_loss: 839.5198,\n",
      " train_mae: 25.3640,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1643/10000,\n",
      " train_loss: 839.5184,\n",
      " train_mae: 25.3640,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 1644/10000,\n",
      " train_loss: 839.5169,\n",
      " train_mae: 25.3639,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1645/10000,\n",
      " train_loss: 839.5156,\n",
      " train_mae: 25.3638,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1646/10000,\n",
      " train_loss: 839.5142,\n",
      " train_mae: 25.3638,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1647/10000,\n",
      " train_loss: 839.5128,\n",
      " train_mae: 25.3637,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1648/10000,\n",
      " train_loss: 839.5112,\n",
      " train_mae: 25.3636,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1649/10000,\n",
      " train_loss: 839.5098,\n",
      " train_mae: 25.3635,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1650/10000,\n",
      " train_loss: 839.5084,\n",
      " train_mae: 25.3634,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1651/10000,\n",
      " train_loss: 839.5069,\n",
      " train_mae: 25.3634,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 1652/10000,\n",
      " train_loss: 839.5054,\n",
      " train_mae: 25.3633,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1653/10000,\n",
      " train_loss: 839.5040,\n",
      " train_mae: 25.3632,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1654/10000,\n",
      " train_loss: 839.5026,\n",
      " train_mae: 25.3632,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1655/10000,\n",
      " train_loss: 839.5012,\n",
      " train_mae: 25.3630,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1656/10000,\n",
      " train_loss: 839.4998,\n",
      " train_mae: 25.3630,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1657/10000,\n",
      " train_loss: 839.4984,\n",
      " train_mae: 25.3629,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1658/10000,\n",
      " train_loss: 839.4971,\n",
      " train_mae: 25.3628,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1659/10000,\n",
      " train_loss: 839.4955,\n",
      " train_mae: 25.3628,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1660/10000,\n",
      " train_loss: 839.4943,\n",
      " train_mae: 25.3627,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1661/10000,\n",
      " train_loss: 839.4929,\n",
      " train_mae: 25.3626,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1662/10000,\n",
      " train_loss: 839.4916,\n",
      " train_mae: 25.3626,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1663/10000,\n",
      " train_loss: 839.4900,\n",
      " train_mae: 25.3625,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1664/10000,\n",
      " train_loss: 839.4888,\n",
      " train_mae: 25.3624,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1665/10000,\n",
      " train_loss: 839.4873,\n",
      " train_mae: 25.3623,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1666/10000,\n",
      " train_loss: 839.4860,\n",
      " train_mae: 25.3622,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1667/10000,\n",
      " train_loss: 839.4847,\n",
      " train_mae: 25.3622,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1668/10000,\n",
      " train_loss: 839.4835,\n",
      " train_mae: 25.3621,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1669/10000,\n",
      " train_loss: 839.4819,\n",
      " train_mae: 25.3620,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1670/10000,\n",
      " train_loss: 839.4807,\n",
      " train_mae: 25.3620,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 1671/10000,\n",
      " train_loss: 839.4795,\n",
      " train_mae: 25.3619,\n",
      " epoch_time_duration: 0.0137\n",
      "\n",
      "epoch: 1672/10000,\n",
      " train_loss: 839.4782,\n",
      " train_mae: 25.3618,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 1673/10000,\n",
      " train_loss: 839.4767,\n",
      " train_mae: 25.3618,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1674/10000,\n",
      " train_loss: 839.4753,\n",
      " train_mae: 25.3617,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 1675/10000,\n",
      " train_loss: 839.4740,\n",
      " train_mae: 25.3616,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 1676/10000,\n",
      " train_loss: 839.4726,\n",
      " train_mae: 25.3616,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1677/10000,\n",
      " train_loss: 839.4714,\n",
      " train_mae: 25.3615,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1678/10000,\n",
      " train_loss: 839.4702,\n",
      " train_mae: 25.3614,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 1679/10000,\n",
      " train_loss: 839.4689,\n",
      " train_mae: 25.3613,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 1680/10000,\n",
      " train_loss: 839.4675,\n",
      " train_mae: 25.3613,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 1681/10000,\n",
      " train_loss: 839.4662,\n",
      " train_mae: 25.3612,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1682/10000,\n",
      " train_loss: 839.4648,\n",
      " train_mae: 25.3611,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1683/10000,\n",
      " train_loss: 839.4636,\n",
      " train_mae: 25.3611,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1684/10000,\n",
      " train_loss: 839.4623,\n",
      " train_mae: 25.3610,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1685/10000,\n",
      " train_loss: 839.4609,\n",
      " train_mae: 25.3609,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 1686/10000,\n",
      " train_loss: 839.4596,\n",
      " train_mae: 25.3608,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1687/10000,\n",
      " train_loss: 839.4583,\n",
      " train_mae: 25.3608,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1688/10000,\n",
      " train_loss: 839.4570,\n",
      " train_mae: 25.3607,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1689/10000,\n",
      " train_loss: 839.4560,\n",
      " train_mae: 25.3607,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1690/10000,\n",
      " train_loss: 839.4545,\n",
      " train_mae: 25.3606,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1691/10000,\n",
      " train_loss: 839.4533,\n",
      " train_mae: 25.3605,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 1692/10000,\n",
      " train_loss: 839.4520,\n",
      " train_mae: 25.3605,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1693/10000,\n",
      " train_loss: 839.4506,\n",
      " train_mae: 25.3604,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1694/10000,\n",
      " train_loss: 839.4495,\n",
      " train_mae: 25.3603,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1695/10000,\n",
      " train_loss: 839.4482,\n",
      " train_mae: 25.3602,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1696/10000,\n",
      " train_loss: 839.4468,\n",
      " train_mae: 25.3602,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1697/10000,\n",
      " train_loss: 839.4459,\n",
      " train_mae: 25.3601,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1698/10000,\n",
      " train_loss: 839.4443,\n",
      " train_mae: 25.3600,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1699/10000,\n",
      " train_loss: 839.4431,\n",
      " train_mae: 25.3600,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1700/10000,\n",
      " train_loss: 839.4418,\n",
      " train_mae: 25.3599,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1701/10000,\n",
      " train_loss: 839.4407,\n",
      " train_mae: 25.3598,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1702/10000,\n",
      " train_loss: 839.4395,\n",
      " train_mae: 25.3598,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1703/10000,\n",
      " train_loss: 839.4382,\n",
      " train_mae: 25.3597,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 1704/10000,\n",
      " train_loss: 839.4370,\n",
      " train_mae: 25.3596,\n",
      " epoch_time_duration: 0.0107\n",
      "\n",
      "epoch: 1705/10000,\n",
      " train_loss: 839.4359,\n",
      " train_mae: 25.3596,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 1706/10000,\n",
      " train_loss: 839.4346,\n",
      " train_mae: 25.3595,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 1707/10000,\n",
      " train_loss: 839.4333,\n",
      " train_mae: 25.3594,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 1708/10000,\n",
      " train_loss: 839.4322,\n",
      " train_mae: 25.3594,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 1709/10000,\n",
      " train_loss: 839.4310,\n",
      " train_mae: 25.3593,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 1710/10000,\n",
      " train_loss: 839.4299,\n",
      " train_mae: 25.3592,\n",
      " epoch_time_duration: 0.0079\n",
      "\n",
      "epoch: 1711/10000,\n",
      " train_loss: 839.4286,\n",
      " train_mae: 25.3592,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 1712/10000,\n",
      " train_loss: 839.4273,\n",
      " train_mae: 25.3591,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 1713/10000,\n",
      " train_loss: 839.4263,\n",
      " train_mae: 25.3591,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 1714/10000,\n",
      " train_loss: 839.4250,\n",
      " train_mae: 25.3590,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1715/10000,\n",
      " train_loss: 839.4239,\n",
      " train_mae: 25.3589,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 1716/10000,\n",
      " train_loss: 839.4227,\n",
      " train_mae: 25.3589,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 1717/10000,\n",
      " train_loss: 839.4215,\n",
      " train_mae: 25.3588,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1718/10000,\n",
      " train_loss: 839.4202,\n",
      " train_mae: 25.3587,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1719/10000,\n",
      " train_loss: 839.4191,\n",
      " train_mae: 25.3587,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1720/10000,\n",
      " train_loss: 839.4179,\n",
      " train_mae: 25.3586,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 1721/10000,\n",
      " train_loss: 839.4166,\n",
      " train_mae: 25.3585,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1722/10000,\n",
      " train_loss: 839.4155,\n",
      " train_mae: 25.3585,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1723/10000,\n",
      " train_loss: 839.4146,\n",
      " train_mae: 25.3584,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1724/10000,\n",
      " train_loss: 839.4132,\n",
      " train_mae: 25.3584,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1725/10000,\n",
      " train_loss: 839.4120,\n",
      " train_mae: 25.3583,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1726/10000,\n",
      " train_loss: 839.4110,\n",
      " train_mae: 25.3582,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1727/10000,\n",
      " train_loss: 839.4097,\n",
      " train_mae: 25.3582,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 1728/10000,\n",
      " train_loss: 839.4088,\n",
      " train_mae: 25.3581,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1729/10000,\n",
      " train_loss: 839.4075,\n",
      " train_mae: 25.3580,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1730/10000,\n",
      " train_loss: 839.4064,\n",
      " train_mae: 25.3580,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1731/10000,\n",
      " train_loss: 839.4052,\n",
      " train_mae: 25.3579,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1732/10000,\n",
      " train_loss: 839.4041,\n",
      " train_mae: 25.3578,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1733/10000,\n",
      " train_loss: 839.4029,\n",
      " train_mae: 25.3578,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1734/10000,\n",
      " train_loss: 839.4017,\n",
      " train_mae: 25.3577,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1735/10000,\n",
      " train_loss: 839.4006,\n",
      " train_mae: 25.3577,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1736/10000,\n",
      " train_loss: 839.3998,\n",
      " train_mae: 25.3576,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1737/10000,\n",
      " train_loss: 839.3985,\n",
      " train_mae: 25.3575,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1738/10000,\n",
      " train_loss: 839.3973,\n",
      " train_mae: 25.3575,\n",
      " epoch_time_duration: 0.0084\n",
      "\n",
      "epoch: 1739/10000,\n",
      " train_loss: 839.3962,\n",
      " train_mae: 25.3574,\n",
      " epoch_time_duration: 0.0102\n",
      "\n",
      "epoch: 1740/10000,\n",
      " train_loss: 839.3951,\n",
      " train_mae: 25.3573,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 1741/10000,\n",
      " train_loss: 839.3940,\n",
      " train_mae: 25.3573,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 1742/10000,\n",
      " train_loss: 839.3929,\n",
      " train_mae: 25.3572,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1743/10000,\n",
      " train_loss: 839.3920,\n",
      " train_mae: 25.3572,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1744/10000,\n",
      " train_loss: 839.3908,\n",
      " train_mae: 25.3571,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1745/10000,\n",
      " train_loss: 839.3896,\n",
      " train_mae: 25.3570,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1746/10000,\n",
      " train_loss: 839.3884,\n",
      " train_mae: 25.3570,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1747/10000,\n",
      " train_loss: 839.3875,\n",
      " train_mae: 25.3570,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1748/10000,\n",
      " train_loss: 839.3862,\n",
      " train_mae: 25.3568,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1749/10000,\n",
      " train_loss: 839.3852,\n",
      " train_mae: 25.3568,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1750/10000,\n",
      " train_loss: 839.3842,\n",
      " train_mae: 25.3568,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1751/10000,\n",
      " train_loss: 839.3831,\n",
      " train_mae: 25.3567,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1752/10000,\n",
      " train_loss: 839.3821,\n",
      " train_mae: 25.3566,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1753/10000,\n",
      " train_loss: 839.3809,\n",
      " train_mae: 25.3566,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1754/10000,\n",
      " train_loss: 839.3799,\n",
      " train_mae: 25.3565,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1755/10000,\n",
      " train_loss: 839.3788,\n",
      " train_mae: 25.3565,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1756/10000,\n",
      " train_loss: 839.3777,\n",
      " train_mae: 25.3564,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1757/10000,\n",
      " train_loss: 839.3766,\n",
      " train_mae: 25.3563,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1758/10000,\n",
      " train_loss: 839.3755,\n",
      " train_mae: 25.3563,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1759/10000,\n",
      " train_loss: 839.3745,\n",
      " train_mae: 25.3562,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1760/10000,\n",
      " train_loss: 839.3736,\n",
      " train_mae: 25.3562,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1761/10000,\n",
      " train_loss: 839.3724,\n",
      " train_mae: 25.3561,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 1762/10000,\n",
      " train_loss: 839.3713,\n",
      " train_mae: 25.3560,\n",
      " epoch_time_duration: 0.0120\n",
      "\n",
      "epoch: 1763/10000,\n",
      " train_loss: 839.3702,\n",
      " train_mae: 25.3560,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1764/10000,\n",
      " train_loss: 839.3693,\n",
      " train_mae: 25.3560,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 1765/10000,\n",
      " train_loss: 839.3683,\n",
      " train_mae: 25.3558,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 1766/10000,\n",
      " train_loss: 839.3671,\n",
      " train_mae: 25.3558,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1767/10000,\n",
      " train_loss: 839.3661,\n",
      " train_mae: 25.3558,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1768/10000,\n",
      " train_loss: 839.3652,\n",
      " train_mae: 25.3557,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1769/10000,\n",
      " train_loss: 839.3641,\n",
      " train_mae: 25.3557,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 1770/10000,\n",
      " train_loss: 839.3629,\n",
      " train_mae: 25.3556,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 1771/10000,\n",
      " train_loss: 839.3620,\n",
      " train_mae: 25.3555,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 1772/10000,\n",
      " train_loss: 839.3609,\n",
      " train_mae: 25.3555,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 1773/10000,\n",
      " train_loss: 839.3600,\n",
      " train_mae: 25.3554,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 1774/10000,\n",
      " train_loss: 839.3589,\n",
      " train_mae: 25.3554,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 1775/10000,\n",
      " train_loss: 839.3580,\n",
      " train_mae: 25.3553,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1776/10000,\n",
      " train_loss: 839.3569,\n",
      " train_mae: 25.3552,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1777/10000,\n",
      " train_loss: 839.3560,\n",
      " train_mae: 25.3552,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1778/10000,\n",
      " train_loss: 839.3549,\n",
      " train_mae: 25.3552,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1779/10000,\n",
      " train_loss: 839.3539,\n",
      " train_mae: 25.3551,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1780/10000,\n",
      " train_loss: 839.3528,\n",
      " train_mae: 25.3550,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1781/10000,\n",
      " train_loss: 839.3520,\n",
      " train_mae: 25.3550,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1782/10000,\n",
      " train_loss: 839.3509,\n",
      " train_mae: 25.3549,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1783/10000,\n",
      " train_loss: 839.3500,\n",
      " train_mae: 25.3549,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1784/10000,\n",
      " train_loss: 839.3489,\n",
      " train_mae: 25.3548,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1785/10000,\n",
      " train_loss: 839.3480,\n",
      " train_mae: 25.3547,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1786/10000,\n",
      " train_loss: 839.3470,\n",
      " train_mae: 25.3547,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 1787/10000,\n",
      " train_loss: 839.3460,\n",
      " train_mae: 25.3547,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1788/10000,\n",
      " train_loss: 839.3452,\n",
      " train_mae: 25.3546,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1789/10000,\n",
      " train_loss: 839.3442,\n",
      " train_mae: 25.3545,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 1790/10000,\n",
      " train_loss: 839.3431,\n",
      " train_mae: 25.3545,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 1791/10000,\n",
      " train_loss: 839.3422,\n",
      " train_mae: 25.3544,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 1792/10000,\n",
      " train_loss: 839.3411,\n",
      " train_mae: 25.3544,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1793/10000,\n",
      " train_loss: 839.3401,\n",
      " train_mae: 25.3543,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1794/10000,\n",
      " train_loss: 839.3392,\n",
      " train_mae: 25.3543,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1795/10000,\n",
      " train_loss: 839.3383,\n",
      " train_mae: 25.3542,\n",
      " epoch_time_duration: 0.0123\n",
      "\n",
      "epoch: 1796/10000,\n",
      " train_loss: 839.3372,\n",
      " train_mae: 25.3542,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1797/10000,\n",
      " train_loss: 839.3364,\n",
      " train_mae: 25.3541,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1798/10000,\n",
      " train_loss: 839.3356,\n",
      " train_mae: 25.3541,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1799/10000,\n",
      " train_loss: 839.3345,\n",
      " train_mae: 25.3540,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1800/10000,\n",
      " train_loss: 839.3336,\n",
      " train_mae: 25.3539,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1801/10000,\n",
      " train_loss: 839.3326,\n",
      " train_mae: 25.3539,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1802/10000,\n",
      " train_loss: 839.3317,\n",
      " train_mae: 25.3538,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1803/10000,\n",
      " train_loss: 839.3307,\n",
      " train_mae: 25.3538,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1804/10000,\n",
      " train_loss: 839.3298,\n",
      " train_mae: 25.3537,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1805/10000,\n",
      " train_loss: 839.3289,\n",
      " train_mae: 25.3537,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1806/10000,\n",
      " train_loss: 839.3281,\n",
      " train_mae: 25.3536,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1807/10000,\n",
      " train_loss: 839.3271,\n",
      " train_mae: 25.3536,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1808/10000,\n",
      " train_loss: 839.3261,\n",
      " train_mae: 25.3535,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 1809/10000,\n",
      " train_loss: 839.3252,\n",
      " train_mae: 25.3535,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1810/10000,\n",
      " train_loss: 839.3243,\n",
      " train_mae: 25.3534,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 1811/10000,\n",
      " train_loss: 839.3234,\n",
      " train_mae: 25.3534,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 1812/10000,\n",
      " train_loss: 839.3224,\n",
      " train_mae: 25.3533,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1813/10000,\n",
      " train_loss: 839.3215,\n",
      " train_mae: 25.3533,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1814/10000,\n",
      " train_loss: 839.3206,\n",
      " train_mae: 25.3532,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1815/10000,\n",
      " train_loss: 839.3198,\n",
      " train_mae: 25.3532,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 1816/10000,\n",
      " train_loss: 839.3189,\n",
      " train_mae: 25.3531,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 1817/10000,\n",
      " train_loss: 839.3180,\n",
      " train_mae: 25.3531,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 1818/10000,\n",
      " train_loss: 839.3170,\n",
      " train_mae: 25.3530,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 1819/10000,\n",
      " train_loss: 839.3162,\n",
      " train_mae: 25.3530,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1820/10000,\n",
      " train_loss: 839.3153,\n",
      " train_mae: 25.3529,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 1821/10000,\n",
      " train_loss: 839.3145,\n",
      " train_mae: 25.3529,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1822/10000,\n",
      " train_loss: 839.3134,\n",
      " train_mae: 25.3528,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 1823/10000,\n",
      " train_loss: 839.3127,\n",
      " train_mae: 25.3528,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1824/10000,\n",
      " train_loss: 839.3118,\n",
      " train_mae: 25.3527,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1825/10000,\n",
      " train_loss: 839.3110,\n",
      " train_mae: 25.3527,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1826/10000,\n",
      " train_loss: 839.3100,\n",
      " train_mae: 25.3526,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 1827/10000,\n",
      " train_loss: 839.3093,\n",
      " train_mae: 25.3526,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 1828/10000,\n",
      " train_loss: 839.3082,\n",
      " train_mae: 25.3525,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 1829/10000,\n",
      " train_loss: 839.3075,\n",
      " train_mae: 25.3525,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1830/10000,\n",
      " train_loss: 839.3066,\n",
      " train_mae: 25.3524,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 1831/10000,\n",
      " train_loss: 839.3057,\n",
      " train_mae: 25.3524,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1832/10000,\n",
      " train_loss: 839.3049,\n",
      " train_mae: 25.3523,\n",
      " epoch_time_duration: 0.0099\n",
      "\n",
      "epoch: 1833/10000,\n",
      " train_loss: 839.3041,\n",
      " train_mae: 25.3523,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1834/10000,\n",
      " train_loss: 839.3033,\n",
      " train_mae: 25.3522,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 1835/10000,\n",
      " train_loss: 839.3022,\n",
      " train_mae: 25.3522,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1836/10000,\n",
      " train_loss: 839.3015,\n",
      " train_mae: 25.3521,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 1837/10000,\n",
      " train_loss: 839.3008,\n",
      " train_mae: 25.3521,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 1838/10000,\n",
      " train_loss: 839.2998,\n",
      " train_mae: 25.3520,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 1839/10000,\n",
      " train_loss: 839.2991,\n",
      " train_mae: 25.3520,\n",
      " epoch_time_duration: 0.0072\n",
      "\n",
      "epoch: 1840/10000,\n",
      " train_loss: 839.2980,\n",
      " train_mae: 25.3520,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 1841/10000,\n",
      " train_loss: 839.2972,\n",
      " train_mae: 25.3519,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 1842/10000,\n",
      " train_loss: 839.2964,\n",
      " train_mae: 25.3519,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1843/10000,\n",
      " train_loss: 839.2956,\n",
      " train_mae: 25.3518,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1844/10000,\n",
      " train_loss: 839.2949,\n",
      " train_mae: 25.3518,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 1845/10000,\n",
      " train_loss: 839.2941,\n",
      " train_mae: 25.3517,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 1846/10000,\n",
      " train_loss: 839.2931,\n",
      " train_mae: 25.3517,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1847/10000,\n",
      " train_loss: 839.2923,\n",
      " train_mae: 25.3516,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 1848/10000,\n",
      " train_loss: 839.2914,\n",
      " train_mae: 25.3516,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1849/10000,\n",
      " train_loss: 839.2906,\n",
      " train_mae: 25.3515,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1850/10000,\n",
      " train_loss: 839.2900,\n",
      " train_mae: 25.3515,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1851/10000,\n",
      " train_loss: 839.2890,\n",
      " train_mae: 25.3514,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1852/10000,\n",
      " train_loss: 839.2883,\n",
      " train_mae: 25.3514,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1853/10000,\n",
      " train_loss: 839.2874,\n",
      " train_mae: 25.3513,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 1854/10000,\n",
      " train_loss: 839.2866,\n",
      " train_mae: 25.3513,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1855/10000,\n",
      " train_loss: 839.2857,\n",
      " train_mae: 25.3512,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1856/10000,\n",
      " train_loss: 839.2849,\n",
      " train_mae: 25.3512,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 1857/10000,\n",
      " train_loss: 839.2842,\n",
      " train_mae: 25.3512,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 1858/10000,\n",
      " train_loss: 839.2834,\n",
      " train_mae: 25.3511,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 1859/10000,\n",
      " train_loss: 839.2826,\n",
      " train_mae: 25.3511,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 1860/10000,\n",
      " train_loss: 839.2818,\n",
      " train_mae: 25.3510,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 1861/10000,\n",
      " train_loss: 839.2809,\n",
      " train_mae: 25.3510,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 1862/10000,\n",
      " train_loss: 839.2802,\n",
      " train_mae: 25.3509,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 1863/10000,\n",
      " train_loss: 839.2795,\n",
      " train_mae: 25.3509,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1864/10000,\n",
      " train_loss: 839.2785,\n",
      " train_mae: 25.3508,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 1865/10000,\n",
      " train_loss: 839.2778,\n",
      " train_mae: 25.3508,\n",
      " epoch_time_duration: 0.0105\n",
      "\n",
      "epoch: 1866/10000,\n",
      " train_loss: 839.2770,\n",
      " train_mae: 25.3508,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1867/10000,\n",
      " train_loss: 839.2763,\n",
      " train_mae: 25.3507,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 1868/10000,\n",
      " train_loss: 839.2755,\n",
      " train_mae: 25.3507,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 1869/10000,\n",
      " train_loss: 839.2746,\n",
      " train_mae: 25.3506,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1870/10000,\n",
      " train_loss: 839.2740,\n",
      " train_mae: 25.3506,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1871/10000,\n",
      " train_loss: 839.2732,\n",
      " train_mae: 25.3505,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 1872/10000,\n",
      " train_loss: 839.2723,\n",
      " train_mae: 25.3505,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 1873/10000,\n",
      " train_loss: 839.2716,\n",
      " train_mae: 25.3504,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1874/10000,\n",
      " train_loss: 839.2709,\n",
      " train_mae: 25.3504,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1875/10000,\n",
      " train_loss: 839.2701,\n",
      " train_mae: 25.3504,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1876/10000,\n",
      " train_loss: 839.2693,\n",
      " train_mae: 25.3503,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 1877/10000,\n",
      " train_loss: 839.2687,\n",
      " train_mae: 25.3503,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 1878/10000,\n",
      " train_loss: 839.2678,\n",
      " train_mae: 25.3502,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 1879/10000,\n",
      " train_loss: 839.2670,\n",
      " train_mae: 25.3502,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 1880/10000,\n",
      " train_loss: 839.2662,\n",
      " train_mae: 25.3501,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1881/10000,\n",
      " train_loss: 839.2656,\n",
      " train_mae: 25.3501,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 1882/10000,\n",
      " train_loss: 839.2648,\n",
      " train_mae: 25.3500,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1883/10000,\n",
      " train_loss: 839.2641,\n",
      " train_mae: 25.3500,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1884/10000,\n",
      " train_loss: 839.2633,\n",
      " train_mae: 25.3500,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1885/10000,\n",
      " train_loss: 839.2626,\n",
      " train_mae: 25.3499,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1886/10000,\n",
      " train_loss: 839.2618,\n",
      " train_mae: 25.3499,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1887/10000,\n",
      " train_loss: 839.2610,\n",
      " train_mae: 25.3498,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1888/10000,\n",
      " train_loss: 839.2602,\n",
      " train_mae: 25.3498,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1889/10000,\n",
      " train_loss: 839.2595,\n",
      " train_mae: 25.3498,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 1890/10000,\n",
      " train_loss: 839.2589,\n",
      " train_mae: 25.3497,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 1891/10000,\n",
      " train_loss: 839.2581,\n",
      " train_mae: 25.3497,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1892/10000,\n",
      " train_loss: 839.2573,\n",
      " train_mae: 25.3496,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1893/10000,\n",
      " train_loss: 839.2566,\n",
      " train_mae: 25.3496,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 1894/10000,\n",
      " train_loss: 839.2560,\n",
      " train_mae: 25.3496,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 1895/10000,\n",
      " train_loss: 839.2552,\n",
      " train_mae: 25.3495,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 1896/10000,\n",
      " train_loss: 839.2546,\n",
      " train_mae: 25.3495,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 1897/10000,\n",
      " train_loss: 839.2537,\n",
      " train_mae: 25.3494,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1898/10000,\n",
      " train_loss: 839.2531,\n",
      " train_mae: 25.3494,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1899/10000,\n",
      " train_loss: 839.2524,\n",
      " train_mae: 25.3493,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1900/10000,\n",
      " train_loss: 839.2516,\n",
      " train_mae: 25.3493,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1901/10000,\n",
      " train_loss: 839.2509,\n",
      " train_mae: 25.3493,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 1902/10000,\n",
      " train_loss: 839.2502,\n",
      " train_mae: 25.3492,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 1903/10000,\n",
      " train_loss: 839.2494,\n",
      " train_mae: 25.3492,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1904/10000,\n",
      " train_loss: 839.2487,\n",
      " train_mae: 25.3491,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 1905/10000,\n",
      " train_loss: 839.2481,\n",
      " train_mae: 25.3491,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 1906/10000,\n",
      " train_loss: 839.2473,\n",
      " train_mae: 25.3491,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 1907/10000,\n",
      " train_loss: 839.2466,\n",
      " train_mae: 25.3490,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 1908/10000,\n",
      " train_loss: 839.2460,\n",
      " train_mae: 25.3490,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 1909/10000,\n",
      " train_loss: 839.2451,\n",
      " train_mae: 25.3489,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1910/10000,\n",
      " train_loss: 839.2444,\n",
      " train_mae: 25.3489,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1911/10000,\n",
      " train_loss: 839.2438,\n",
      " train_mae: 25.3489,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1912/10000,\n",
      " train_loss: 839.2432,\n",
      " train_mae: 25.3488,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1913/10000,\n",
      " train_loss: 839.2424,\n",
      " train_mae: 25.3488,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1914/10000,\n",
      " train_loss: 839.2418,\n",
      " train_mae: 25.3487,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1915/10000,\n",
      " train_loss: 839.2411,\n",
      " train_mae: 25.3487,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1916/10000,\n",
      " train_loss: 839.2403,\n",
      " train_mae: 25.3487,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1917/10000,\n",
      " train_loss: 839.2397,\n",
      " train_mae: 25.3486,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 1918/10000,\n",
      " train_loss: 839.2390,\n",
      " train_mae: 25.3486,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 1919/10000,\n",
      " train_loss: 839.2383,\n",
      " train_mae: 25.3485,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 1920/10000,\n",
      " train_loss: 839.2376,\n",
      " train_mae: 25.3485,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1921/10000,\n",
      " train_loss: 839.2369,\n",
      " train_mae: 25.3485,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 1922/10000,\n",
      " train_loss: 839.2362,\n",
      " train_mae: 25.3484,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1923/10000,\n",
      " train_loss: 839.2356,\n",
      " train_mae: 25.3484,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 1924/10000,\n",
      " train_loss: 839.2348,\n",
      " train_mae: 25.3483,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1925/10000,\n",
      " train_loss: 839.2343,\n",
      " train_mae: 25.3483,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 1926/10000,\n",
      " train_loss: 839.2335,\n",
      " train_mae: 25.3483,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 1927/10000,\n",
      " train_loss: 839.2328,\n",
      " train_mae: 25.3482,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1928/10000,\n",
      " train_loss: 839.2321,\n",
      " train_mae: 25.3482,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 1929/10000,\n",
      " train_loss: 839.2314,\n",
      " train_mae: 25.3482,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1930/10000,\n",
      " train_loss: 839.2307,\n",
      " train_mae: 25.3481,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1931/10000,\n",
      " train_loss: 839.2300,\n",
      " train_mae: 25.3481,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1932/10000,\n",
      " train_loss: 839.2295,\n",
      " train_mae: 25.3480,\n",
      " epoch_time_duration: 0.0087\n",
      "\n",
      "epoch: 1933/10000,\n",
      " train_loss: 839.2288,\n",
      " train_mae: 25.3480,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 1934/10000,\n",
      " train_loss: 839.2281,\n",
      " train_mae: 25.3479,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 1935/10000,\n",
      " train_loss: 839.2273,\n",
      " train_mae: 25.3479,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 1936/10000,\n",
      " train_loss: 839.2267,\n",
      " train_mae: 25.3479,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 1937/10000,\n",
      " train_loss: 839.2260,\n",
      " train_mae: 25.3478,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1938/10000,\n",
      " train_loss: 839.2255,\n",
      " train_mae: 25.3478,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1939/10000,\n",
      " train_loss: 839.2247,\n",
      " train_mae: 25.3478,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1940/10000,\n",
      " train_loss: 839.2240,\n",
      " train_mae: 25.3477,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1941/10000,\n",
      " train_loss: 839.2234,\n",
      " train_mae: 25.3477,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1942/10000,\n",
      " train_loss: 839.2228,\n",
      " train_mae: 25.3477,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1943/10000,\n",
      " train_loss: 839.2221,\n",
      " train_mae: 25.3476,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1944/10000,\n",
      " train_loss: 839.2216,\n",
      " train_mae: 25.3476,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 1945/10000,\n",
      " train_loss: 839.2209,\n",
      " train_mae: 25.3475,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1946/10000,\n",
      " train_loss: 839.2202,\n",
      " train_mae: 25.3475,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 1947/10000,\n",
      " train_loss: 839.2197,\n",
      " train_mae: 25.3475,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1948/10000,\n",
      " train_loss: 839.2189,\n",
      " train_mae: 25.3474,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1949/10000,\n",
      " train_loss: 839.2183,\n",
      " train_mae: 25.3474,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1950/10000,\n",
      " train_loss: 839.2177,\n",
      " train_mae: 25.3474,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 1951/10000,\n",
      " train_loss: 839.2169,\n",
      " train_mae: 25.3473,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 1952/10000,\n",
      " train_loss: 839.2164,\n",
      " train_mae: 25.3473,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1953/10000,\n",
      " train_loss: 839.2157,\n",
      " train_mae: 25.3473,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 1954/10000,\n",
      " train_loss: 839.2151,\n",
      " train_mae: 25.3472,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1955/10000,\n",
      " train_loss: 839.2145,\n",
      " train_mae: 25.3472,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 1956/10000,\n",
      " train_loss: 839.2138,\n",
      " train_mae: 25.3471,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1957/10000,\n",
      " train_loss: 839.2132,\n",
      " train_mae: 25.3471,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1958/10000,\n",
      " train_loss: 839.2125,\n",
      " train_mae: 25.3471,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1959/10000,\n",
      " train_loss: 839.2120,\n",
      " train_mae: 25.3470,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1960/10000,\n",
      " train_loss: 839.2113,\n",
      " train_mae: 25.3470,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1961/10000,\n",
      " train_loss: 839.2107,\n",
      " train_mae: 25.3470,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 1962/10000,\n",
      " train_loss: 839.2101,\n",
      " train_mae: 25.3469,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 1963/10000,\n",
      " train_loss: 839.2094,\n",
      " train_mae: 25.3469,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 1964/10000,\n",
      " train_loss: 839.2089,\n",
      " train_mae: 25.3469,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 1965/10000,\n",
      " train_loss: 839.2083,\n",
      " train_mae: 25.3468,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1966/10000,\n",
      " train_loss: 839.2076,\n",
      " train_mae: 25.3468,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1967/10000,\n",
      " train_loss: 839.2069,\n",
      " train_mae: 25.3468,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1968/10000,\n",
      " train_loss: 839.2064,\n",
      " train_mae: 25.3467,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 1969/10000,\n",
      " train_loss: 839.2057,\n",
      " train_mae: 25.3467,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 1970/10000,\n",
      " train_loss: 839.2052,\n",
      " train_mae: 25.3466,\n",
      " epoch_time_duration: 0.0098\n",
      "\n",
      "epoch: 1971/10000,\n",
      " train_loss: 839.2045,\n",
      " train_mae: 25.3466,\n",
      " epoch_time_duration: 0.0074\n",
      "\n",
      "epoch: 1972/10000,\n",
      " train_loss: 839.2040,\n",
      " train_mae: 25.3466,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 1973/10000,\n",
      " train_loss: 839.2034,\n",
      " train_mae: 25.3465,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 1974/10000,\n",
      " train_loss: 839.2026,\n",
      " train_mae: 25.3465,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 1975/10000,\n",
      " train_loss: 839.2021,\n",
      " train_mae: 25.3465,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 1976/10000,\n",
      " train_loss: 839.2015,\n",
      " train_mae: 25.3464,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 1977/10000,\n",
      " train_loss: 839.2009,\n",
      " train_mae: 25.3464,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1978/10000,\n",
      " train_loss: 839.2003,\n",
      " train_mae: 25.3464,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1979/10000,\n",
      " train_loss: 839.1998,\n",
      " train_mae: 25.3463,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1980/10000,\n",
      " train_loss: 839.1992,\n",
      " train_mae: 25.3463,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1981/10000,\n",
      " train_loss: 839.1986,\n",
      " train_mae: 25.3463,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 1982/10000,\n",
      " train_loss: 839.1980,\n",
      " train_mae: 25.3462,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1983/10000,\n",
      " train_loss: 839.1973,\n",
      " train_mae: 25.3462,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1984/10000,\n",
      " train_loss: 839.1967,\n",
      " train_mae: 25.3462,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 1985/10000,\n",
      " train_loss: 839.1962,\n",
      " train_mae: 25.3461,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 1986/10000,\n",
      " train_loss: 839.1955,\n",
      " train_mae: 25.3461,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 1987/10000,\n",
      " train_loss: 839.1951,\n",
      " train_mae: 25.3461,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 1988/10000,\n",
      " train_loss: 839.1945,\n",
      " train_mae: 25.3460,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 1989/10000,\n",
      " train_loss: 839.1938,\n",
      " train_mae: 25.3460,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 1990/10000,\n",
      " train_loss: 839.1934,\n",
      " train_mae: 25.3460,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1991/10000,\n",
      " train_loss: 839.1927,\n",
      " train_mae: 25.3459,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 1992/10000,\n",
      " train_loss: 839.1921,\n",
      " train_mae: 25.3459,\n",
      " epoch_time_duration: 0.0083\n",
      "\n",
      "epoch: 1993/10000,\n",
      " train_loss: 839.1915,\n",
      " train_mae: 25.3459,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 1994/10000,\n",
      " train_loss: 839.1909,\n",
      " train_mae: 25.3458,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 1995/10000,\n",
      " train_loss: 839.1903,\n",
      " train_mae: 25.3458,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 1996/10000,\n",
      " train_loss: 839.1898,\n",
      " train_mae: 25.3458,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 1997/10000,\n",
      " train_loss: 839.1892,\n",
      " train_mae: 25.3457,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 1998/10000,\n",
      " train_loss: 839.1887,\n",
      " train_mae: 25.3457,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 1999/10000,\n",
      " train_loss: 839.1880,\n",
      " train_mae: 25.3457,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2000/10000,\n",
      " train_loss: 839.1876,\n",
      " train_mae: 25.3456,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2001/10000,\n",
      " train_loss: 839.1870,\n",
      " train_mae: 25.3456,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2002/10000,\n",
      " train_loss: 839.1864,\n",
      " train_mae: 25.3456,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2003/10000,\n",
      " train_loss: 839.1857,\n",
      " train_mae: 25.3456,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2004/10000,\n",
      " train_loss: 839.1852,\n",
      " train_mae: 25.3455,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2005/10000,\n",
      " train_loss: 839.1847,\n",
      " train_mae: 25.3455,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2006/10000,\n",
      " train_loss: 839.1841,\n",
      " train_mae: 25.3455,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2007/10000,\n",
      " train_loss: 839.1836,\n",
      " train_mae: 25.3454,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2008/10000,\n",
      " train_loss: 839.1830,\n",
      " train_mae: 25.3454,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2009/10000,\n",
      " train_loss: 839.1823,\n",
      " train_mae: 25.3454,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2010/10000,\n",
      " train_loss: 839.1818,\n",
      " train_mae: 25.3453,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2011/10000,\n",
      " train_loss: 839.1813,\n",
      " train_mae: 25.3453,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2012/10000,\n",
      " train_loss: 839.1808,\n",
      " train_mae: 25.3453,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2013/10000,\n",
      " train_loss: 839.1802,\n",
      " train_mae: 25.3452,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2014/10000,\n",
      " train_loss: 839.1797,\n",
      " train_mae: 25.3452,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2015/10000,\n",
      " train_loss: 839.1791,\n",
      " train_mae: 25.3452,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2016/10000,\n",
      " train_loss: 839.1786,\n",
      " train_mae: 25.3451,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2017/10000,\n",
      " train_loss: 839.1780,\n",
      " train_mae: 25.3451,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2018/10000,\n",
      " train_loss: 839.1775,\n",
      " train_mae: 25.3451,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2019/10000,\n",
      " train_loss: 839.1771,\n",
      " train_mae: 25.3450,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2020/10000,\n",
      " train_loss: 839.1765,\n",
      " train_mae: 25.3450,\n",
      " epoch_time_duration: 0.0025\n",
      "\n",
      "epoch: 2021/10000,\n",
      " train_loss: 839.1759,\n",
      " train_mae: 25.3450,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2022/10000,\n",
      " train_loss: 839.1752,\n",
      " train_mae: 25.3450,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2023/10000,\n",
      " train_loss: 839.1749,\n",
      " train_mae: 25.3449,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2024/10000,\n",
      " train_loss: 839.1744,\n",
      " train_mae: 25.3449,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2025/10000,\n",
      " train_loss: 839.1738,\n",
      " train_mae: 25.3449,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2026/10000,\n",
      " train_loss: 839.1732,\n",
      " train_mae: 25.3448,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2027/10000,\n",
      " train_loss: 839.1726,\n",
      " train_mae: 25.3448,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2028/10000,\n",
      " train_loss: 839.1722,\n",
      " train_mae: 25.3448,\n",
      " epoch_time_duration: 0.0084\n",
      "\n",
      "epoch: 2029/10000,\n",
      " train_loss: 839.1716,\n",
      " train_mae: 25.3447,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2030/10000,\n",
      " train_loss: 839.1712,\n",
      " train_mae: 25.3447,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 2031/10000,\n",
      " train_loss: 839.1706,\n",
      " train_mae: 25.3447,\n",
      " epoch_time_duration: 0.0077\n",
      "\n",
      "epoch: 2032/10000,\n",
      " train_loss: 839.1701,\n",
      " train_mae: 25.3447,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 2033/10000,\n",
      " train_loss: 839.1694,\n",
      " train_mae: 25.3446,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 2034/10000,\n",
      " train_loss: 839.1691,\n",
      " train_mae: 25.3446,\n",
      " epoch_time_duration: 0.0091\n",
      "\n",
      "epoch: 2035/10000,\n",
      " train_loss: 839.1684,\n",
      " train_mae: 25.3446,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 2036/10000,\n",
      " train_loss: 839.1680,\n",
      " train_mae: 25.3445,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 2037/10000,\n",
      " train_loss: 839.1674,\n",
      " train_mae: 25.3445,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 2038/10000,\n",
      " train_loss: 839.1670,\n",
      " train_mae: 25.3445,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 2039/10000,\n",
      " train_loss: 839.1666,\n",
      " train_mae: 25.3445,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 2040/10000,\n",
      " train_loss: 839.1659,\n",
      " train_mae: 25.3444,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2041/10000,\n",
      " train_loss: 839.1654,\n",
      " train_mae: 25.3444,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2042/10000,\n",
      " train_loss: 839.1649,\n",
      " train_mae: 25.3444,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2043/10000,\n",
      " train_loss: 839.1644,\n",
      " train_mae: 25.3443,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2044/10000,\n",
      " train_loss: 839.1639,\n",
      " train_mae: 25.3443,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2045/10000,\n",
      " train_loss: 839.1633,\n",
      " train_mae: 25.3443,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2046/10000,\n",
      " train_loss: 839.1628,\n",
      " train_mae: 25.3442,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2047/10000,\n",
      " train_loss: 839.1624,\n",
      " train_mae: 25.3442,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2048/10000,\n",
      " train_loss: 839.1618,\n",
      " train_mae: 25.3442,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2049/10000,\n",
      " train_loss: 839.1614,\n",
      " train_mae: 25.3442,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2050/10000,\n",
      " train_loss: 839.1608,\n",
      " train_mae: 25.3441,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2051/10000,\n",
      " train_loss: 839.1603,\n",
      " train_mae: 25.3441,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2052/10000,\n",
      " train_loss: 839.1598,\n",
      " train_mae: 25.3441,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2053/10000,\n",
      " train_loss: 839.1593,\n",
      " train_mae: 25.3440,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2054/10000,\n",
      " train_loss: 839.1588,\n",
      " train_mae: 25.3440,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2055/10000,\n",
      " train_loss: 839.1583,\n",
      " train_mae: 25.3440,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2056/10000,\n",
      " train_loss: 839.1578,\n",
      " train_mae: 25.3440,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2057/10000,\n",
      " train_loss: 839.1573,\n",
      " train_mae: 25.3439,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2058/10000,\n",
      " train_loss: 839.1569,\n",
      " train_mae: 25.3439,\n",
      " epoch_time_duration: 0.0094\n",
      "\n",
      "epoch: 2059/10000,\n",
      " train_loss: 839.1563,\n",
      " train_mae: 25.3439,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 2060/10000,\n",
      " train_loss: 839.1557,\n",
      " train_mae: 25.3439,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2061/10000,\n",
      " train_loss: 839.1553,\n",
      " train_mae: 25.3438,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 2062/10000,\n",
      " train_loss: 839.1548,\n",
      " train_mae: 25.3438,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 2063/10000,\n",
      " train_loss: 839.1544,\n",
      " train_mae: 25.3438,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 2064/10000,\n",
      " train_loss: 839.1539,\n",
      " train_mae: 25.3437,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2065/10000,\n",
      " train_loss: 839.1534,\n",
      " train_mae: 25.3437,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2066/10000,\n",
      " train_loss: 839.1529,\n",
      " train_mae: 25.3437,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2067/10000,\n",
      " train_loss: 839.1525,\n",
      " train_mae: 25.3437,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2068/10000,\n",
      " train_loss: 839.1519,\n",
      " train_mae: 25.3436,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 2069/10000,\n",
      " train_loss: 839.1515,\n",
      " train_mae: 25.3436,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2070/10000,\n",
      " train_loss: 839.1510,\n",
      " train_mae: 25.3436,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2071/10000,\n",
      " train_loss: 839.1505,\n",
      " train_mae: 25.3435,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2072/10000,\n",
      " train_loss: 839.1500,\n",
      " train_mae: 25.3435,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2073/10000,\n",
      " train_loss: 839.1497,\n",
      " train_mae: 25.3435,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2074/10000,\n",
      " train_loss: 839.1491,\n",
      " train_mae: 25.3435,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2075/10000,\n",
      " train_loss: 839.1487,\n",
      " train_mae: 25.3434,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2076/10000,\n",
      " train_loss: 839.1481,\n",
      " train_mae: 25.3434,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2077/10000,\n",
      " train_loss: 839.1477,\n",
      " train_mae: 25.3434,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2078/10000,\n",
      " train_loss: 839.1472,\n",
      " train_mae: 25.3434,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2079/10000,\n",
      " train_loss: 839.1467,\n",
      " train_mae: 25.3433,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2080/10000,\n",
      " train_loss: 839.1462,\n",
      " train_mae: 25.3433,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2081/10000,\n",
      " train_loss: 839.1458,\n",
      " train_mae: 25.3433,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2082/10000,\n",
      " train_loss: 839.1454,\n",
      " train_mae: 25.3433,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 2083/10000,\n",
      " train_loss: 839.1448,\n",
      " train_mae: 25.3432,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2084/10000,\n",
      " train_loss: 839.1443,\n",
      " train_mae: 25.3432,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2085/10000,\n",
      " train_loss: 839.1439,\n",
      " train_mae: 25.3432,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2086/10000,\n",
      " train_loss: 839.1436,\n",
      " train_mae: 25.3432,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2087/10000,\n",
      " train_loss: 839.1431,\n",
      " train_mae: 25.3431,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2088/10000,\n",
      " train_loss: 839.1425,\n",
      " train_mae: 25.3431,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2089/10000,\n",
      " train_loss: 839.1420,\n",
      " train_mae: 25.3431,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2090/10000,\n",
      " train_loss: 839.1416,\n",
      " train_mae: 25.3430,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2091/10000,\n",
      " train_loss: 839.1412,\n",
      " train_mae: 25.3430,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2092/10000,\n",
      " train_loss: 839.1407,\n",
      " train_mae: 25.3430,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2093/10000,\n",
      " train_loss: 839.1403,\n",
      " train_mae: 25.3430,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 2094/10000,\n",
      " train_loss: 839.1398,\n",
      " train_mae: 25.3429,\n",
      " epoch_time_duration: 0.0117\n",
      "\n",
      "epoch: 2095/10000,\n",
      " train_loss: 839.1393,\n",
      " train_mae: 25.3429,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 2096/10000,\n",
      " train_loss: 839.1389,\n",
      " train_mae: 25.3429,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2097/10000,\n",
      " train_loss: 839.1384,\n",
      " train_mae: 25.3429,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2098/10000,\n",
      " train_loss: 839.1381,\n",
      " train_mae: 25.3428,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2099/10000,\n",
      " train_loss: 839.1376,\n",
      " train_mae: 25.3428,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 2100/10000,\n",
      " train_loss: 839.1371,\n",
      " train_mae: 25.3428,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 2101/10000,\n",
      " train_loss: 839.1367,\n",
      " train_mae: 25.3428,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 2102/10000,\n",
      " train_loss: 839.1362,\n",
      " train_mae: 25.3427,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 2103/10000,\n",
      " train_loss: 839.1358,\n",
      " train_mae: 25.3427,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 2104/10000,\n",
      " train_loss: 839.1353,\n",
      " train_mae: 25.3427,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2105/10000,\n",
      " train_loss: 839.1348,\n",
      " train_mae: 25.3427,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2106/10000,\n",
      " train_loss: 839.1344,\n",
      " train_mae: 25.3427,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2107/10000,\n",
      " train_loss: 839.1340,\n",
      " train_mae: 25.3426,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2108/10000,\n",
      " train_loss: 839.1337,\n",
      " train_mae: 25.3426,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2109/10000,\n",
      " train_loss: 839.1331,\n",
      " train_mae: 25.3426,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2110/10000,\n",
      " train_loss: 839.1328,\n",
      " train_mae: 25.3426,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2111/10000,\n",
      " train_loss: 839.1322,\n",
      " train_mae: 25.3425,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2112/10000,\n",
      " train_loss: 839.1318,\n",
      " train_mae: 25.3425,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2113/10000,\n",
      " train_loss: 839.1313,\n",
      " train_mae: 25.3425,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2114/10000,\n",
      " train_loss: 839.1310,\n",
      " train_mae: 25.3425,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2115/10000,\n",
      " train_loss: 839.1305,\n",
      " train_mae: 25.3424,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2116/10000,\n",
      " train_loss: 839.1300,\n",
      " train_mae: 25.3424,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2117/10000,\n",
      " train_loss: 839.1295,\n",
      " train_mae: 25.3424,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2118/10000,\n",
      " train_loss: 839.1292,\n",
      " train_mae: 25.3424,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2119/10000,\n",
      " train_loss: 839.1287,\n",
      " train_mae: 25.3423,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2120/10000,\n",
      " train_loss: 839.1283,\n",
      " train_mae: 25.3423,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2121/10000,\n",
      " train_loss: 839.1279,\n",
      " train_mae: 25.3423,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2122/10000,\n",
      " train_loss: 839.1275,\n",
      " train_mae: 25.3423,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2123/10000,\n",
      " train_loss: 839.1270,\n",
      " train_mae: 25.3422,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2124/10000,\n",
      " train_loss: 839.1266,\n",
      " train_mae: 25.3422,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2125/10000,\n",
      " train_loss: 839.1262,\n",
      " train_mae: 25.3422,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2126/10000,\n",
      " train_loss: 839.1257,\n",
      " train_mae: 25.3422,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2127/10000,\n",
      " train_loss: 839.1254,\n",
      " train_mae: 25.3421,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 2128/10000,\n",
      " train_loss: 839.1249,\n",
      " train_mae: 25.3421,\n",
      " epoch_time_duration: 0.0089\n",
      "\n",
      "epoch: 2129/10000,\n",
      " train_loss: 839.1245,\n",
      " train_mae: 25.3421,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 2130/10000,\n",
      " train_loss: 839.1241,\n",
      " train_mae: 25.3421,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2131/10000,\n",
      " train_loss: 839.1237,\n",
      " train_mae: 25.3420,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2132/10000,\n",
      " train_loss: 839.1233,\n",
      " train_mae: 25.3420,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2133/10000,\n",
      " train_loss: 839.1228,\n",
      " train_mae: 25.3420,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2134/10000,\n",
      " train_loss: 839.1224,\n",
      " train_mae: 25.3420,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2135/10000,\n",
      " train_loss: 839.1221,\n",
      " train_mae: 25.3420,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2136/10000,\n",
      " train_loss: 839.1216,\n",
      " train_mae: 25.3419,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2137/10000,\n",
      " train_loss: 839.1212,\n",
      " train_mae: 25.3419,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2138/10000,\n",
      " train_loss: 839.1207,\n",
      " train_mae: 25.3419,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2139/10000,\n",
      " train_loss: 839.1204,\n",
      " train_mae: 25.3419,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2140/10000,\n",
      " train_loss: 839.1200,\n",
      " train_mae: 25.3419,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2141/10000,\n",
      " train_loss: 839.1197,\n",
      " train_mae: 25.3418,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2142/10000,\n",
      " train_loss: 839.1192,\n",
      " train_mae: 25.3418,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2143/10000,\n",
      " train_loss: 839.1188,\n",
      " train_mae: 25.3418,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2144/10000,\n",
      " train_loss: 839.1183,\n",
      " train_mae: 25.3418,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2145/10000,\n",
      " train_loss: 839.1180,\n",
      " train_mae: 25.3417,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2146/10000,\n",
      " train_loss: 839.1176,\n",
      " train_mae: 25.3417,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2147/10000,\n",
      " train_loss: 839.1171,\n",
      " train_mae: 25.3417,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2148/10000,\n",
      " train_loss: 839.1166,\n",
      " train_mae: 25.3417,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2149/10000,\n",
      " train_loss: 839.1163,\n",
      " train_mae: 25.3417,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2150/10000,\n",
      " train_loss: 839.1159,\n",
      " train_mae: 25.3416,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2151/10000,\n",
      " train_loss: 839.1155,\n",
      " train_mae: 25.3416,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2152/10000,\n",
      " train_loss: 839.1152,\n",
      " train_mae: 25.3416,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2153/10000,\n",
      " train_loss: 839.1147,\n",
      " train_mae: 25.3416,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2154/10000,\n",
      " train_loss: 839.1144,\n",
      " train_mae: 25.3415,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 2155/10000,\n",
      " train_loss: 839.1139,\n",
      " train_mae: 25.3415,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 2156/10000,\n",
      " train_loss: 839.1136,\n",
      " train_mae: 25.3415,\n",
      " epoch_time_duration: 0.0081\n",
      "\n",
      "epoch: 2157/10000,\n",
      " train_loss: 839.1132,\n",
      " train_mae: 25.3415,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 2158/10000,\n",
      " train_loss: 839.1127,\n",
      " train_mae: 25.3414,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2159/10000,\n",
      " train_loss: 839.1124,\n",
      " train_mae: 25.3414,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2160/10000,\n",
      " train_loss: 839.1121,\n",
      " train_mae: 25.3414,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2161/10000,\n",
      " train_loss: 839.1116,\n",
      " train_mae: 25.3414,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2162/10000,\n",
      " train_loss: 839.1112,\n",
      " train_mae: 25.3414,\n",
      " epoch_time_duration: 0.0077\n",
      "\n",
      "epoch: 2163/10000,\n",
      " train_loss: 839.1108,\n",
      " train_mae: 25.3413,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 2164/10000,\n",
      " train_loss: 839.1104,\n",
      " train_mae: 25.3413,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 2165/10000,\n",
      " train_loss: 839.1099,\n",
      " train_mae: 25.3413,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2166/10000,\n",
      " train_loss: 839.1097,\n",
      " train_mae: 25.3413,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 2167/10000,\n",
      " train_loss: 839.1093,\n",
      " train_mae: 25.3413,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2168/10000,\n",
      " train_loss: 839.1088,\n",
      " train_mae: 25.3412,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 2169/10000,\n",
      " train_loss: 839.1085,\n",
      " train_mae: 25.3412,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2170/10000,\n",
      " train_loss: 839.1081,\n",
      " train_mae: 25.3412,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2171/10000,\n",
      " train_loss: 839.1077,\n",
      " train_mae: 25.3412,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2172/10000,\n",
      " train_loss: 839.1073,\n",
      " train_mae: 25.3411,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2173/10000,\n",
      " train_loss: 839.1069,\n",
      " train_mae: 25.3411,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2174/10000,\n",
      " train_loss: 839.1066,\n",
      " train_mae: 25.3411,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2175/10000,\n",
      " train_loss: 839.1061,\n",
      " train_mae: 25.3411,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2176/10000,\n",
      " train_loss: 839.1060,\n",
      " train_mae: 25.3411,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 2177/10000,\n",
      " train_loss: 839.1055,\n",
      " train_mae: 25.3411,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 2178/10000,\n",
      " train_loss: 839.1049,\n",
      " train_mae: 25.3410,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2179/10000,\n",
      " train_loss: 839.1047,\n",
      " train_mae: 25.3410,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2180/10000,\n",
      " train_loss: 839.1044,\n",
      " train_mae: 25.3410,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2181/10000,\n",
      " train_loss: 839.1039,\n",
      " train_mae: 25.3410,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2182/10000,\n",
      " train_loss: 839.1036,\n",
      " train_mae: 25.3409,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2183/10000,\n",
      " train_loss: 839.1031,\n",
      " train_mae: 25.3409,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2184/10000,\n",
      " train_loss: 839.1028,\n",
      " train_mae: 25.3409,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2185/10000,\n",
      " train_loss: 839.1024,\n",
      " train_mae: 25.3409,\n",
      " epoch_time_duration: 0.0077\n",
      "\n",
      "epoch: 2186/10000,\n",
      " train_loss: 839.1021,\n",
      " train_mae: 25.3409,\n",
      " epoch_time_duration: 0.0073\n",
      "\n",
      "epoch: 2187/10000,\n",
      " train_loss: 839.1017,\n",
      " train_mae: 25.3408,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 2188/10000,\n",
      " train_loss: 839.1013,\n",
      " train_mae: 25.3408,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 2189/10000,\n",
      " train_loss: 839.1010,\n",
      " train_mae: 25.3408,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2190/10000,\n",
      " train_loss: 839.1006,\n",
      " train_mae: 25.3408,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 2191/10000,\n",
      " train_loss: 839.1002,\n",
      " train_mae: 25.3408,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2192/10000,\n",
      " train_loss: 839.0999,\n",
      " train_mae: 25.3407,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2193/10000,\n",
      " train_loss: 839.0995,\n",
      " train_mae: 25.3407,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2194/10000,\n",
      " train_loss: 839.0991,\n",
      " train_mae: 25.3407,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 2195/10000,\n",
      " train_loss: 839.0988,\n",
      " train_mae: 25.3407,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2196/10000,\n",
      " train_loss: 839.0983,\n",
      " train_mae: 25.3407,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2197/10000,\n",
      " train_loss: 839.0980,\n",
      " train_mae: 25.3406,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2198/10000,\n",
      " train_loss: 839.0977,\n",
      " train_mae: 25.3406,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 2199/10000,\n",
      " train_loss: 839.0972,\n",
      " train_mae: 25.3406,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2200/10000,\n",
      " train_loss: 839.0969,\n",
      " train_mae: 25.3406,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2201/10000,\n",
      " train_loss: 839.0966,\n",
      " train_mae: 25.3406,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 2202/10000,\n",
      " train_loss: 839.0963,\n",
      " train_mae: 25.3405,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2203/10000,\n",
      " train_loss: 839.0958,\n",
      " train_mae: 25.3405,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2204/10000,\n",
      " train_loss: 839.0955,\n",
      " train_mae: 25.3405,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2205/10000,\n",
      " train_loss: 839.0951,\n",
      " train_mae: 25.3405,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2206/10000,\n",
      " train_loss: 839.0948,\n",
      " train_mae: 25.3405,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 2207/10000,\n",
      " train_loss: 839.0944,\n",
      " train_mae: 25.3405,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2208/10000,\n",
      " train_loss: 839.0941,\n",
      " train_mae: 25.3404,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2209/10000,\n",
      " train_loss: 839.0938,\n",
      " train_mae: 25.3404,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2210/10000,\n",
      " train_loss: 839.0934,\n",
      " train_mae: 25.3404,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 2211/10000,\n",
      " train_loss: 839.0930,\n",
      " train_mae: 25.3404,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 2212/10000,\n",
      " train_loss: 839.0928,\n",
      " train_mae: 25.3404,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2213/10000,\n",
      " train_loss: 839.0923,\n",
      " train_mae: 25.3403,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2214/10000,\n",
      " train_loss: 839.0920,\n",
      " train_mae: 25.3403,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2215/10000,\n",
      " train_loss: 839.0917,\n",
      " train_mae: 25.3403,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2216/10000,\n",
      " train_loss: 839.0912,\n",
      " train_mae: 25.3403,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2217/10000,\n",
      " train_loss: 839.0909,\n",
      " train_mae: 25.3403,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2218/10000,\n",
      " train_loss: 839.0907,\n",
      " train_mae: 25.3402,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2219/10000,\n",
      " train_loss: 839.0902,\n",
      " train_mae: 25.3402,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2220/10000,\n",
      " train_loss: 839.0900,\n",
      " train_mae: 25.3402,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2221/10000,\n",
      " train_loss: 839.0895,\n",
      " train_mae: 25.3402,\n",
      " epoch_time_duration: 0.0105\n",
      "\n",
      "epoch: 2222/10000,\n",
      " train_loss: 839.0892,\n",
      " train_mae: 25.3402,\n",
      " epoch_time_duration: 0.0142\n",
      "\n",
      "epoch: 2223/10000,\n",
      " train_loss: 839.0889,\n",
      " train_mae: 25.3402,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 2224/10000,\n",
      " train_loss: 839.0885,\n",
      " train_mae: 25.3401,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 2225/10000,\n",
      " train_loss: 839.0882,\n",
      " train_mae: 25.3401,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 2226/10000,\n",
      " train_loss: 839.0878,\n",
      " train_mae: 25.3401,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 2227/10000,\n",
      " train_loss: 839.0875,\n",
      " train_mae: 25.3401,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 2228/10000,\n",
      " train_loss: 839.0872,\n",
      " train_mae: 25.3401,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2229/10000,\n",
      " train_loss: 839.0869,\n",
      " train_mae: 25.3400,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2230/10000,\n",
      " train_loss: 839.0865,\n",
      " train_mae: 25.3400,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2231/10000,\n",
      " train_loss: 839.0862,\n",
      " train_mae: 25.3400,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2232/10000,\n",
      " train_loss: 839.0859,\n",
      " train_mae: 25.3400,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 2233/10000,\n",
      " train_loss: 839.0855,\n",
      " train_mae: 25.3400,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 2234/10000,\n",
      " train_loss: 839.0851,\n",
      " train_mae: 25.3400,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2235/10000,\n",
      " train_loss: 839.0848,\n",
      " train_mae: 25.3399,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2236/10000,\n",
      " train_loss: 839.0845,\n",
      " train_mae: 25.3399,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 2237/10000,\n",
      " train_loss: 839.0842,\n",
      " train_mae: 25.3399,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2238/10000,\n",
      " train_loss: 839.0837,\n",
      " train_mae: 25.3399,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 2239/10000,\n",
      " train_loss: 839.0834,\n",
      " train_mae: 25.3399,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2240/10000,\n",
      " train_loss: 839.0831,\n",
      " train_mae: 25.3398,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 2241/10000,\n",
      " train_loss: 839.0829,\n",
      " train_mae: 25.3398,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2242/10000,\n",
      " train_loss: 839.0825,\n",
      " train_mae: 25.3398,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2243/10000,\n",
      " train_loss: 839.0822,\n",
      " train_mae: 25.3398,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2244/10000,\n",
      " train_loss: 839.0819,\n",
      " train_mae: 25.3398,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2245/10000,\n",
      " train_loss: 839.0815,\n",
      " train_mae: 25.3398,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2246/10000,\n",
      " train_loss: 839.0812,\n",
      " train_mae: 25.3397,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2247/10000,\n",
      " train_loss: 839.0809,\n",
      " train_mae: 25.3397,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2248/10000,\n",
      " train_loss: 839.0805,\n",
      " train_mae: 25.3397,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2249/10000,\n",
      " train_loss: 839.0802,\n",
      " train_mae: 25.3397,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2250/10000,\n",
      " train_loss: 839.0798,\n",
      " train_mae: 25.3397,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2251/10000,\n",
      " train_loss: 839.0795,\n",
      " train_mae: 25.3397,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2252/10000,\n",
      " train_loss: 839.0792,\n",
      " train_mae: 25.3396,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 2253/10000,\n",
      " train_loss: 839.0788,\n",
      " train_mae: 25.3396,\n",
      " epoch_time_duration: 0.0122\n",
      "\n",
      "epoch: 2254/10000,\n",
      " train_loss: 839.0787,\n",
      " train_mae: 25.3396,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2255/10000,\n",
      " train_loss: 839.0783,\n",
      " train_mae: 25.3396,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2256/10000,\n",
      " train_loss: 839.0779,\n",
      " train_mae: 25.3396,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2257/10000,\n",
      " train_loss: 839.0776,\n",
      " train_mae: 25.3396,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2258/10000,\n",
      " train_loss: 839.0773,\n",
      " train_mae: 25.3395,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2259/10000,\n",
      " train_loss: 839.0770,\n",
      " train_mae: 25.3395,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2260/10000,\n",
      " train_loss: 839.0767,\n",
      " train_mae: 25.3395,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2261/10000,\n",
      " train_loss: 839.0764,\n",
      " train_mae: 25.3395,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2262/10000,\n",
      " train_loss: 839.0761,\n",
      " train_mae: 25.3395,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2263/10000,\n",
      " train_loss: 839.0756,\n",
      " train_mae: 25.3394,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2264/10000,\n",
      " train_loss: 839.0754,\n",
      " train_mae: 25.3394,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2265/10000,\n",
      " train_loss: 839.0751,\n",
      " train_mae: 25.3394,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2266/10000,\n",
      " train_loss: 839.0748,\n",
      " train_mae: 25.3394,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2267/10000,\n",
      " train_loss: 839.0745,\n",
      " train_mae: 25.3394,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2268/10000,\n",
      " train_loss: 839.0742,\n",
      " train_mae: 25.3394,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2269/10000,\n",
      " train_loss: 839.0739,\n",
      " train_mae: 25.3394,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2270/10000,\n",
      " train_loss: 839.0735,\n",
      " train_mae: 25.3393,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2271/10000,\n",
      " train_loss: 839.0732,\n",
      " train_mae: 25.3393,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2272/10000,\n",
      " train_loss: 839.0729,\n",
      " train_mae: 25.3393,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2273/10000,\n",
      " train_loss: 839.0726,\n",
      " train_mae: 25.3393,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2274/10000,\n",
      " train_loss: 839.0723,\n",
      " train_mae: 25.3393,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2275/10000,\n",
      " train_loss: 839.0720,\n",
      " train_mae: 25.3393,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2276/10000,\n",
      " train_loss: 839.0717,\n",
      " train_mae: 25.3392,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2277/10000,\n",
      " train_loss: 839.0714,\n",
      " train_mae: 25.3392,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2278/10000,\n",
      " train_loss: 839.0710,\n",
      " train_mae: 25.3392,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 2279/10000,\n",
      " train_loss: 839.0707,\n",
      " train_mae: 25.3392,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2280/10000,\n",
      " train_loss: 839.0704,\n",
      " train_mae: 25.3392,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2281/10000,\n",
      " train_loss: 839.0703,\n",
      " train_mae: 25.3392,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2282/10000,\n",
      " train_loss: 839.0698,\n",
      " train_mae: 25.3392,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 2283/10000,\n",
      " train_loss: 839.0696,\n",
      " train_mae: 25.3391,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 2284/10000,\n",
      " train_loss: 839.0693,\n",
      " train_mae: 25.3391,\n",
      " epoch_time_duration: 0.0081\n",
      "\n",
      "epoch: 2285/10000,\n",
      " train_loss: 839.0689,\n",
      " train_mae: 25.3391,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 2286/10000,\n",
      " train_loss: 839.0687,\n",
      " train_mae: 25.3391,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 2287/10000,\n",
      " train_loss: 839.0684,\n",
      " train_mae: 25.3391,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 2288/10000,\n",
      " train_loss: 839.0681,\n",
      " train_mae: 25.3390,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2289/10000,\n",
      " train_loss: 839.0677,\n",
      " train_mae: 25.3390,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2290/10000,\n",
      " train_loss: 839.0674,\n",
      " train_mae: 25.3390,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2291/10000,\n",
      " train_loss: 839.0671,\n",
      " train_mae: 25.3390,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2292/10000,\n",
      " train_loss: 839.0668,\n",
      " train_mae: 25.3390,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2293/10000,\n",
      " train_loss: 839.0666,\n",
      " train_mae: 25.3390,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2294/10000,\n",
      " train_loss: 839.0663,\n",
      " train_mae: 25.3389,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2295/10000,\n",
      " train_loss: 839.0660,\n",
      " train_mae: 25.3389,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2296/10000,\n",
      " train_loss: 839.0657,\n",
      " train_mae: 25.3389,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2297/10000,\n",
      " train_loss: 839.0654,\n",
      " train_mae: 25.3389,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2298/10000,\n",
      " train_loss: 839.0651,\n",
      " train_mae: 25.3389,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2299/10000,\n",
      " train_loss: 839.0649,\n",
      " train_mae: 25.3389,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2300/10000,\n",
      " train_loss: 839.0645,\n",
      " train_mae: 25.3389,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2301/10000,\n",
      " train_loss: 839.0641,\n",
      " train_mae: 25.3388,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2302/10000,\n",
      " train_loss: 839.0639,\n",
      " train_mae: 25.3388,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2303/10000,\n",
      " train_loss: 839.0637,\n",
      " train_mae: 25.3388,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2304/10000,\n",
      " train_loss: 839.0634,\n",
      " train_mae: 25.3388,\n",
      " epoch_time_duration: 0.0025\n",
      "\n",
      "epoch: 2305/10000,\n",
      " train_loss: 839.0630,\n",
      " train_mae: 25.3388,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2306/10000,\n",
      " train_loss: 839.0628,\n",
      " train_mae: 25.3388,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2307/10000,\n",
      " train_loss: 839.0626,\n",
      " train_mae: 25.3388,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 2308/10000,\n",
      " train_loss: 839.0621,\n",
      " train_mae: 25.3387,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2309/10000,\n",
      " train_loss: 839.0619,\n",
      " train_mae: 25.3387,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2310/10000,\n",
      " train_loss: 839.0616,\n",
      " train_mae: 25.3387,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2311/10000,\n",
      " train_loss: 839.0614,\n",
      " train_mae: 25.3387,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2312/10000,\n",
      " train_loss: 839.0611,\n",
      " train_mae: 25.3387,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2313/10000,\n",
      " train_loss: 839.0607,\n",
      " train_mae: 25.3387,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 2314/10000,\n",
      " train_loss: 839.0605,\n",
      " train_mae: 25.3387,\n",
      " epoch_time_duration: 0.0082\n",
      "\n",
      "epoch: 2315/10000,\n",
      " train_loss: 839.0601,\n",
      " train_mae: 25.3386,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 2316/10000,\n",
      " train_loss: 839.0599,\n",
      " train_mae: 25.3386,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 2317/10000,\n",
      " train_loss: 839.0597,\n",
      " train_mae: 25.3386,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2318/10000,\n",
      " train_loss: 839.0593,\n",
      " train_mae: 25.3386,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 2319/10000,\n",
      " train_loss: 839.0590,\n",
      " train_mae: 25.3386,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 2320/10000,\n",
      " train_loss: 839.0588,\n",
      " train_mae: 25.3386,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2321/10000,\n",
      " train_loss: 839.0585,\n",
      " train_mae: 25.3386,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2322/10000,\n",
      " train_loss: 839.0582,\n",
      " train_mae: 25.3385,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2323/10000,\n",
      " train_loss: 839.0580,\n",
      " train_mae: 25.3385,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2324/10000,\n",
      " train_loss: 839.0577,\n",
      " train_mae: 25.3385,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2325/10000,\n",
      " train_loss: 839.0573,\n",
      " train_mae: 25.3385,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2326/10000,\n",
      " train_loss: 839.0571,\n",
      " train_mae: 25.3385,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2327/10000,\n",
      " train_loss: 839.0568,\n",
      " train_mae: 25.3385,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2328/10000,\n",
      " train_loss: 839.0566,\n",
      " train_mae: 25.3385,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2329/10000,\n",
      " train_loss: 839.0563,\n",
      " train_mae: 25.3384,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2330/10000,\n",
      " train_loss: 839.0560,\n",
      " train_mae: 25.3384,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2331/10000,\n",
      " train_loss: 839.0557,\n",
      " train_mae: 25.3384,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2332/10000,\n",
      " train_loss: 839.0555,\n",
      " train_mae: 25.3384,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2333/10000,\n",
      " train_loss: 839.0552,\n",
      " train_mae: 25.3384,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2334/10000,\n",
      " train_loss: 839.0549,\n",
      " train_mae: 25.3384,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2335/10000,\n",
      " train_loss: 839.0547,\n",
      " train_mae: 25.3384,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2336/10000,\n",
      " train_loss: 839.0543,\n",
      " train_mae: 25.3383,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2337/10000,\n",
      " train_loss: 839.0541,\n",
      " train_mae: 25.3383,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2338/10000,\n",
      " train_loss: 839.0538,\n",
      " train_mae: 25.3383,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2339/10000,\n",
      " train_loss: 839.0536,\n",
      " train_mae: 25.3383,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2340/10000,\n",
      " train_loss: 839.0533,\n",
      " train_mae: 25.3383,\n",
      " epoch_time_duration: 0.0075\n",
      "\n",
      "epoch: 2341/10000,\n",
      " train_loss: 839.0530,\n",
      " train_mae: 25.3383,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 2342/10000,\n",
      " train_loss: 839.0528,\n",
      " train_mae: 25.3383,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 2343/10000,\n",
      " train_loss: 839.0525,\n",
      " train_mae: 25.3382,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 2344/10000,\n",
      " train_loss: 839.0522,\n",
      " train_mae: 25.3382,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2345/10000,\n",
      " train_loss: 839.0520,\n",
      " train_mae: 25.3382,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 2346/10000,\n",
      " train_loss: 839.0517,\n",
      " train_mae: 25.3382,\n",
      " epoch_time_duration: 0.0078\n",
      "\n",
      "epoch: 2347/10000,\n",
      " train_loss: 839.0515,\n",
      " train_mae: 25.3382,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2348/10000,\n",
      " train_loss: 839.0511,\n",
      " train_mae: 25.3382,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 2349/10000,\n",
      " train_loss: 839.0509,\n",
      " train_mae: 25.3382,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2350/10000,\n",
      " train_loss: 839.0507,\n",
      " train_mae: 25.3382,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 2351/10000,\n",
      " train_loss: 839.0504,\n",
      " train_mae: 25.3381,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 2352/10000,\n",
      " train_loss: 839.0501,\n",
      " train_mae: 25.3381,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 2353/10000,\n",
      " train_loss: 839.0499,\n",
      " train_mae: 25.3381,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 2354/10000,\n",
      " train_loss: 839.0496,\n",
      " train_mae: 25.3381,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 2355/10000,\n",
      " train_loss: 839.0493,\n",
      " train_mae: 25.3381,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2356/10000,\n",
      " train_loss: 839.0491,\n",
      " train_mae: 25.3381,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2357/10000,\n",
      " train_loss: 839.0488,\n",
      " train_mae: 25.3381,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2358/10000,\n",
      " train_loss: 839.0485,\n",
      " train_mae: 25.3380,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2359/10000,\n",
      " train_loss: 839.0482,\n",
      " train_mae: 25.3380,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2360/10000,\n",
      " train_loss: 839.0480,\n",
      " train_mae: 25.3380,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2361/10000,\n",
      " train_loss: 839.0478,\n",
      " train_mae: 25.3380,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2362/10000,\n",
      " train_loss: 839.0475,\n",
      " train_mae: 25.3380,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2363/10000,\n",
      " train_loss: 839.0472,\n",
      " train_mae: 25.3380,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2364/10000,\n",
      " train_loss: 839.0470,\n",
      " train_mae: 25.3380,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2365/10000,\n",
      " train_loss: 839.0468,\n",
      " train_mae: 25.3379,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2366/10000,\n",
      " train_loss: 839.0463,\n",
      " train_mae: 25.3379,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 2367/10000,\n",
      " train_loss: 839.0462,\n",
      " train_mae: 25.3379,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2368/10000,\n",
      " train_loss: 839.0460,\n",
      " train_mae: 25.3379,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2369/10000,\n",
      " train_loss: 839.0458,\n",
      " train_mae: 25.3379,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2370/10000,\n",
      " train_loss: 839.0455,\n",
      " train_mae: 25.3379,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2371/10000,\n",
      " train_loss: 839.0452,\n",
      " train_mae: 25.3379,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 2372/10000,\n",
      " train_loss: 839.0449,\n",
      " train_mae: 25.3379,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 2373/10000,\n",
      " train_loss: 839.0447,\n",
      " train_mae: 25.3378,\n",
      " epoch_time_duration: 0.0138\n",
      "\n",
      "epoch: 2374/10000,\n",
      " train_loss: 839.0444,\n",
      " train_mae: 25.3378,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2375/10000,\n",
      " train_loss: 839.0442,\n",
      " train_mae: 25.3378,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 2376/10000,\n",
      " train_loss: 839.0439,\n",
      " train_mae: 25.3378,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2377/10000,\n",
      " train_loss: 839.0436,\n",
      " train_mae: 25.3378,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2378/10000,\n",
      " train_loss: 839.0434,\n",
      " train_mae: 25.3378,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2379/10000,\n",
      " train_loss: 839.0432,\n",
      " train_mae: 25.3378,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2380/10000,\n",
      " train_loss: 839.0429,\n",
      " train_mae: 25.3378,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2381/10000,\n",
      " train_loss: 839.0426,\n",
      " train_mae: 25.3377,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2382/10000,\n",
      " train_loss: 839.0424,\n",
      " train_mae: 25.3377,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2383/10000,\n",
      " train_loss: 839.0422,\n",
      " train_mae: 25.3377,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2384/10000,\n",
      " train_loss: 839.0419,\n",
      " train_mae: 25.3377,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2385/10000,\n",
      " train_loss: 839.0416,\n",
      " train_mae: 25.3377,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2386/10000,\n",
      " train_loss: 839.0414,\n",
      " train_mae: 25.3377,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2387/10000,\n",
      " train_loss: 839.0412,\n",
      " train_mae: 25.3377,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2388/10000,\n",
      " train_loss: 839.0410,\n",
      " train_mae: 25.3377,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2389/10000,\n",
      " train_loss: 839.0406,\n",
      " train_mae: 25.3376,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2390/10000,\n",
      " train_loss: 839.0405,\n",
      " train_mae: 25.3376,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2391/10000,\n",
      " train_loss: 839.0402,\n",
      " train_mae: 25.3376,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2392/10000,\n",
      " train_loss: 839.0399,\n",
      " train_mae: 25.3376,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2393/10000,\n",
      " train_loss: 839.0397,\n",
      " train_mae: 25.3376,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 2394/10000,\n",
      " train_loss: 839.0394,\n",
      " train_mae: 25.3376,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 2395/10000,\n",
      " train_loss: 839.0392,\n",
      " train_mae: 25.3376,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 2396/10000,\n",
      " train_loss: 839.0388,\n",
      " train_mae: 25.3376,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 2397/10000,\n",
      " train_loss: 839.0387,\n",
      " train_mae: 25.3376,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 2398/10000,\n",
      " train_loss: 839.0384,\n",
      " train_mae: 25.3375,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 2399/10000,\n",
      " train_loss: 839.0382,\n",
      " train_mae: 25.3375,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2400/10000,\n",
      " train_loss: 839.0380,\n",
      " train_mae: 25.3375,\n",
      " epoch_time_duration: 0.0079\n",
      "\n",
      "epoch: 2401/10000,\n",
      " train_loss: 839.0378,\n",
      " train_mae: 25.3375,\n",
      " epoch_time_duration: 0.0082\n",
      "\n",
      "epoch: 2402/10000,\n",
      " train_loss: 839.0375,\n",
      " train_mae: 25.3375,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 2403/10000,\n",
      " train_loss: 839.0373,\n",
      " train_mae: 25.3375,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2404/10000,\n",
      " train_loss: 839.0370,\n",
      " train_mae: 25.3375,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2405/10000,\n",
      " train_loss: 839.0367,\n",
      " train_mae: 25.3375,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2406/10000,\n",
      " train_loss: 839.0366,\n",
      " train_mae: 25.3374,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 2407/10000,\n",
      " train_loss: 839.0364,\n",
      " train_mae: 25.3374,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2408/10000,\n",
      " train_loss: 839.0361,\n",
      " train_mae: 25.3374,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2409/10000,\n",
      " train_loss: 839.0358,\n",
      " train_mae: 25.3374,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2410/10000,\n",
      " train_loss: 839.0356,\n",
      " train_mae: 25.3374,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2411/10000,\n",
      " train_loss: 839.0354,\n",
      " train_mae: 25.3374,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2412/10000,\n",
      " train_loss: 839.0351,\n",
      " train_mae: 25.3374,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2413/10000,\n",
      " train_loss: 839.0349,\n",
      " train_mae: 25.3374,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2414/10000,\n",
      " train_loss: 839.0346,\n",
      " train_mae: 25.3373,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2415/10000,\n",
      " train_loss: 839.0344,\n",
      " train_mae: 25.3373,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 2416/10000,\n",
      " train_loss: 839.0342,\n",
      " train_mae: 25.3373,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2417/10000,\n",
      " train_loss: 839.0339,\n",
      " train_mae: 25.3373,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2418/10000,\n",
      " train_loss: 839.0338,\n",
      " train_mae: 25.3373,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2419/10000,\n",
      " train_loss: 839.0336,\n",
      " train_mae: 25.3373,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2420/10000,\n",
      " train_loss: 839.0333,\n",
      " train_mae: 25.3373,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2421/10000,\n",
      " train_loss: 839.0330,\n",
      " train_mae: 25.3373,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2422/10000,\n",
      " train_loss: 839.0328,\n",
      " train_mae: 25.3373,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2423/10000,\n",
      " train_loss: 839.0326,\n",
      " train_mae: 25.3373,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2424/10000,\n",
      " train_loss: 839.0323,\n",
      " train_mae: 25.3372,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2425/10000,\n",
      " train_loss: 839.0322,\n",
      " train_mae: 25.3372,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2426/10000,\n",
      " train_loss: 839.0319,\n",
      " train_mae: 25.3372,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2427/10000,\n",
      " train_loss: 839.0316,\n",
      " train_mae: 25.3372,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2428/10000,\n",
      " train_loss: 839.0314,\n",
      " train_mae: 25.3372,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2429/10000,\n",
      " train_loss: 839.0312,\n",
      " train_mae: 25.3372,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2430/10000,\n",
      " train_loss: 839.0309,\n",
      " train_mae: 25.3372,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2431/10000,\n",
      " train_loss: 839.0307,\n",
      " train_mae: 25.3372,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2432/10000,\n",
      " train_loss: 839.0305,\n",
      " train_mae: 25.3372,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2433/10000,\n",
      " train_loss: 839.0303,\n",
      " train_mae: 25.3371,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2434/10000,\n",
      " train_loss: 839.0302,\n",
      " train_mae: 25.3371,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2435/10000,\n",
      " train_loss: 839.0298,\n",
      " train_mae: 25.3371,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2436/10000,\n",
      " train_loss: 839.0297,\n",
      " train_mae: 25.3371,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2437/10000,\n",
      " train_loss: 839.0295,\n",
      " train_mae: 25.3371,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2438/10000,\n",
      " train_loss: 839.0292,\n",
      " train_mae: 25.3371,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2439/10000,\n",
      " train_loss: 839.0289,\n",
      " train_mae: 25.3371,\n",
      " epoch_time_duration: 0.0113\n",
      "\n",
      "epoch: 2440/10000,\n",
      " train_loss: 839.0287,\n",
      " train_mae: 25.3371,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 2441/10000,\n",
      " train_loss: 839.0286,\n",
      " train_mae: 25.3370,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 2442/10000,\n",
      " train_loss: 839.0282,\n",
      " train_mae: 25.3370,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 2443/10000,\n",
      " train_loss: 839.0281,\n",
      " train_mae: 25.3370,\n",
      " epoch_time_duration: 0.0077\n",
      "\n",
      "epoch: 2444/10000,\n",
      " train_loss: 839.0278,\n",
      " train_mae: 25.3370,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 2445/10000,\n",
      " train_loss: 839.0276,\n",
      " train_mae: 25.3370,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 2446/10000,\n",
      " train_loss: 839.0274,\n",
      " train_mae: 25.3370,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 2447/10000,\n",
      " train_loss: 839.0272,\n",
      " train_mae: 25.3370,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 2448/10000,\n",
      " train_loss: 839.0270,\n",
      " train_mae: 25.3370,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2449/10000,\n",
      " train_loss: 839.0267,\n",
      " train_mae: 25.3370,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2450/10000,\n",
      " train_loss: 839.0265,\n",
      " train_mae: 25.3370,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2451/10000,\n",
      " train_loss: 839.0262,\n",
      " train_mae: 25.3369,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2452/10000,\n",
      " train_loss: 839.0260,\n",
      " train_mae: 25.3369,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2453/10000,\n",
      " train_loss: 839.0258,\n",
      " train_mae: 25.3369,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2454/10000,\n",
      " train_loss: 839.0256,\n",
      " train_mae: 25.3369,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2455/10000,\n",
      " train_loss: 839.0255,\n",
      " train_mae: 25.3369,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2456/10000,\n",
      " train_loss: 839.0251,\n",
      " train_mae: 25.3369,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2457/10000,\n",
      " train_loss: 839.0250,\n",
      " train_mae: 25.3369,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2458/10000,\n",
      " train_loss: 839.0248,\n",
      " train_mae: 25.3369,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2459/10000,\n",
      " train_loss: 839.0246,\n",
      " train_mae: 25.3369,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2460/10000,\n",
      " train_loss: 839.0244,\n",
      " train_mae: 25.3369,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2461/10000,\n",
      " train_loss: 839.0241,\n",
      " train_mae: 25.3368,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2462/10000,\n",
      " train_loss: 839.0240,\n",
      " train_mae: 25.3368,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2463/10000,\n",
      " train_loss: 839.0237,\n",
      " train_mae: 25.3368,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2464/10000,\n",
      " train_loss: 839.0235,\n",
      " train_mae: 25.3368,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2465/10000,\n",
      " train_loss: 839.0232,\n",
      " train_mae: 25.3368,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2466/10000,\n",
      " train_loss: 839.0231,\n",
      " train_mae: 25.3368,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2467/10000,\n",
      " train_loss: 839.0228,\n",
      " train_mae: 25.3368,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2468/10000,\n",
      " train_loss: 839.0227,\n",
      " train_mae: 25.3368,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2469/10000,\n",
      " train_loss: 839.0225,\n",
      " train_mae: 25.3368,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2470/10000,\n",
      " train_loss: 839.0222,\n",
      " train_mae: 25.3367,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2471/10000,\n",
      " train_loss: 839.0220,\n",
      " train_mae: 25.3367,\n",
      " epoch_time_duration: 0.0103\n",
      "\n",
      "epoch: 2472/10000,\n",
      " train_loss: 839.0218,\n",
      " train_mae: 25.3367,\n",
      " epoch_time_duration: 0.0091\n",
      "\n",
      "epoch: 2473/10000,\n",
      " train_loss: 839.0215,\n",
      " train_mae: 25.3367,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 2474/10000,\n",
      " train_loss: 839.0214,\n",
      " train_mae: 25.3367,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 2475/10000,\n",
      " train_loss: 839.0212,\n",
      " train_mae: 25.3367,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 2476/10000,\n",
      " train_loss: 839.0209,\n",
      " train_mae: 25.3367,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 2477/10000,\n",
      " train_loss: 839.0208,\n",
      " train_mae: 25.3367,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2478/10000,\n",
      " train_loss: 839.0206,\n",
      " train_mae: 25.3367,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2479/10000,\n",
      " train_loss: 839.0204,\n",
      " train_mae: 25.3367,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2480/10000,\n",
      " train_loss: 839.0201,\n",
      " train_mae: 25.3367,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2481/10000,\n",
      " train_loss: 839.0199,\n",
      " train_mae: 25.3366,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2482/10000,\n",
      " train_loss: 839.0197,\n",
      " train_mae: 25.3366,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2483/10000,\n",
      " train_loss: 839.0195,\n",
      " train_mae: 25.3366,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2484/10000,\n",
      " train_loss: 839.0193,\n",
      " train_mae: 25.3366,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2485/10000,\n",
      " train_loss: 839.0190,\n",
      " train_mae: 25.3366,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2486/10000,\n",
      " train_loss: 839.0189,\n",
      " train_mae: 25.3366,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 2487/10000,\n",
      " train_loss: 839.0187,\n",
      " train_mae: 25.3366,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2488/10000,\n",
      " train_loss: 839.0184,\n",
      " train_mae: 25.3366,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2489/10000,\n",
      " train_loss: 839.0182,\n",
      " train_mae: 25.3366,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2490/10000,\n",
      " train_loss: 839.0180,\n",
      " train_mae: 25.3365,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2491/10000,\n",
      " train_loss: 839.0179,\n",
      " train_mae: 25.3365,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2492/10000,\n",
      " train_loss: 839.0176,\n",
      " train_mae: 25.3365,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2493/10000,\n",
      " train_loss: 839.0175,\n",
      " train_mae: 25.3365,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2494/10000,\n",
      " train_loss: 839.0172,\n",
      " train_mae: 25.3365,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 2495/10000,\n",
      " train_loss: 839.0170,\n",
      " train_mae: 25.3365,\n",
      " epoch_time_duration: 0.0081\n",
      "\n",
      "epoch: 2496/10000,\n",
      " train_loss: 839.0168,\n",
      " train_mae: 25.3365,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 2497/10000,\n",
      " train_loss: 839.0167,\n",
      " train_mae: 25.3365,\n",
      " epoch_time_duration: 0.0154\n",
      "\n",
      "epoch: 2498/10000,\n",
      " train_loss: 839.0164,\n",
      " train_mae: 25.3365,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 2499/10000,\n",
      " train_loss: 839.0162,\n",
      " train_mae: 25.3365,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 2500/10000,\n",
      " train_loss: 839.0161,\n",
      " train_mae: 25.3364,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 2501/10000,\n",
      " train_loss: 839.0158,\n",
      " train_mae: 25.3364,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2502/10000,\n",
      " train_loss: 839.0157,\n",
      " train_mae: 25.3364,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2503/10000,\n",
      " train_loss: 839.0155,\n",
      " train_mae: 25.3364,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2504/10000,\n",
      " train_loss: 839.0153,\n",
      " train_mae: 25.3364,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2505/10000,\n",
      " train_loss: 839.0150,\n",
      " train_mae: 25.3364,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2506/10000,\n",
      " train_loss: 839.0149,\n",
      " train_mae: 25.3364,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2507/10000,\n",
      " train_loss: 839.0146,\n",
      " train_mae: 25.3364,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2508/10000,\n",
      " train_loss: 839.0145,\n",
      " train_mae: 25.3364,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2509/10000,\n",
      " train_loss: 839.0143,\n",
      " train_mae: 25.3364,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2510/10000,\n",
      " train_loss: 839.0141,\n",
      " train_mae: 25.3364,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2511/10000,\n",
      " train_loss: 839.0139,\n",
      " train_mae: 25.3363,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2512/10000,\n",
      " train_loss: 839.0137,\n",
      " train_mae: 25.3363,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2513/10000,\n",
      " train_loss: 839.0134,\n",
      " train_mae: 25.3363,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2514/10000,\n",
      " train_loss: 839.0132,\n",
      " train_mae: 25.3363,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2515/10000,\n",
      " train_loss: 839.0131,\n",
      " train_mae: 25.3363,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2516/10000,\n",
      " train_loss: 839.0129,\n",
      " train_mae: 25.3363,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2517/10000,\n",
      " train_loss: 839.0128,\n",
      " train_mae: 25.3363,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 2518/10000,\n",
      " train_loss: 839.0126,\n",
      " train_mae: 25.3363,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2519/10000,\n",
      " train_loss: 839.0123,\n",
      " train_mae: 25.3363,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2520/10000,\n",
      " train_loss: 839.0121,\n",
      " train_mae: 25.3363,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2521/10000,\n",
      " train_loss: 839.0119,\n",
      " train_mae: 25.3363,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2522/10000,\n",
      " train_loss: 839.0118,\n",
      " train_mae: 25.3363,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2523/10000,\n",
      " train_loss: 839.0115,\n",
      " train_mae: 25.3362,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2524/10000,\n",
      " train_loss: 839.0114,\n",
      " train_mae: 25.3362,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2525/10000,\n",
      " train_loss: 839.0112,\n",
      " train_mae: 25.3362,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2526/10000,\n",
      " train_loss: 839.0110,\n",
      " train_mae: 25.3362,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2527/10000,\n",
      " train_loss: 839.0108,\n",
      " train_mae: 25.3362,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2528/10000,\n",
      " train_loss: 839.0106,\n",
      " train_mae: 25.3362,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2529/10000,\n",
      " train_loss: 839.0104,\n",
      " train_mae: 25.3362,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2530/10000,\n",
      " train_loss: 839.0102,\n",
      " train_mae: 25.3362,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2531/10000,\n",
      " train_loss: 839.0100,\n",
      " train_mae: 25.3362,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2532/10000,\n",
      " train_loss: 839.0098,\n",
      " train_mae: 25.3362,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2533/10000,\n",
      " train_loss: 839.0097,\n",
      " train_mae: 25.3362,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2534/10000,\n",
      " train_loss: 839.0095,\n",
      " train_mae: 25.3361,\n",
      " epoch_time_duration: 0.0092\n",
      "\n",
      "epoch: 2535/10000,\n",
      " train_loss: 839.0092,\n",
      " train_mae: 25.3361,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 2536/10000,\n",
      " train_loss: 839.0091,\n",
      " train_mae: 25.3361,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 2537/10000,\n",
      " train_loss: 839.0089,\n",
      " train_mae: 25.3361,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 2538/10000,\n",
      " train_loss: 839.0087,\n",
      " train_mae: 25.3361,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 2539/10000,\n",
      " train_loss: 839.0085,\n",
      " train_mae: 25.3361,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 2540/10000,\n",
      " train_loss: 839.0084,\n",
      " train_mae: 25.3361,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2541/10000,\n",
      " train_loss: 839.0081,\n",
      " train_mae: 25.3361,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 2542/10000,\n",
      " train_loss: 839.0079,\n",
      " train_mae: 25.3361,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2543/10000,\n",
      " train_loss: 839.0078,\n",
      " train_mae: 25.3361,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2544/10000,\n",
      " train_loss: 839.0076,\n",
      " train_mae: 25.3361,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2545/10000,\n",
      " train_loss: 839.0074,\n",
      " train_mae: 25.3361,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2546/10000,\n",
      " train_loss: 839.0072,\n",
      " train_mae: 25.3360,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 2547/10000,\n",
      " train_loss: 839.0071,\n",
      " train_mae: 25.3360,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 2548/10000,\n",
      " train_loss: 839.0068,\n",
      " train_mae: 25.3360,\n",
      " epoch_time_duration: 0.0072\n",
      "\n",
      "epoch: 2549/10000,\n",
      " train_loss: 839.0066,\n",
      " train_mae: 25.3360,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 2550/10000,\n",
      " train_loss: 839.0065,\n",
      " train_mae: 25.3360,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 2551/10000,\n",
      " train_loss: 839.0063,\n",
      " train_mae: 25.3360,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2552/10000,\n",
      " train_loss: 839.0060,\n",
      " train_mae: 25.3360,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 2553/10000,\n",
      " train_loss: 839.0059,\n",
      " train_mae: 25.3360,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2554/10000,\n",
      " train_loss: 839.0057,\n",
      " train_mae: 25.3360,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2555/10000,\n",
      " train_loss: 839.0055,\n",
      " train_mae: 25.3360,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 2556/10000,\n",
      " train_loss: 839.0054,\n",
      " train_mae: 25.3360,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 2557/10000,\n",
      " train_loss: 839.0052,\n",
      " train_mae: 25.3360,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2558/10000,\n",
      " train_loss: 839.0051,\n",
      " train_mae: 25.3359,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2559/10000,\n",
      " train_loss: 839.0048,\n",
      " train_mae: 25.3359,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2560/10000,\n",
      " train_loss: 839.0046,\n",
      " train_mae: 25.3359,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2561/10000,\n",
      " train_loss: 839.0045,\n",
      " train_mae: 25.3359,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2562/10000,\n",
      " train_loss: 839.0043,\n",
      " train_mae: 25.3359,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2563/10000,\n",
      " train_loss: 839.0042,\n",
      " train_mae: 25.3359,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2564/10000,\n",
      " train_loss: 839.0040,\n",
      " train_mae: 25.3359,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2565/10000,\n",
      " train_loss: 839.0038,\n",
      " train_mae: 25.3359,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2566/10000,\n",
      " train_loss: 839.0035,\n",
      " train_mae: 25.3359,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2567/10000,\n",
      " train_loss: 839.0035,\n",
      " train_mae: 25.3359,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2568/10000,\n",
      " train_loss: 839.0032,\n",
      " train_mae: 25.3359,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2569/10000,\n",
      " train_loss: 839.0031,\n",
      " train_mae: 25.3359,\n",
      " epoch_time_duration: 0.0101\n",
      "\n",
      "epoch: 2570/10000,\n",
      " train_loss: 839.0029,\n",
      " train_mae: 25.3358,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2571/10000,\n",
      " train_loss: 839.0027,\n",
      " train_mae: 25.3358,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2572/10000,\n",
      " train_loss: 839.0025,\n",
      " train_mae: 25.3358,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 2573/10000,\n",
      " train_loss: 839.0023,\n",
      " train_mae: 25.3358,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2574/10000,\n",
      " train_loss: 839.0022,\n",
      " train_mae: 25.3358,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2575/10000,\n",
      " train_loss: 839.0020,\n",
      " train_mae: 25.3358,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2576/10000,\n",
      " train_loss: 839.0019,\n",
      " train_mae: 25.3358,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2577/10000,\n",
      " train_loss: 839.0016,\n",
      " train_mae: 25.3358,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2578/10000,\n",
      " train_loss: 839.0014,\n",
      " train_mae: 25.3358,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2579/10000,\n",
      " train_loss: 839.0013,\n",
      " train_mae: 25.3358,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2580/10000,\n",
      " train_loss: 839.0012,\n",
      " train_mae: 25.3358,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2581/10000,\n",
      " train_loss: 839.0010,\n",
      " train_mae: 25.3358,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2582/10000,\n",
      " train_loss: 839.0009,\n",
      " train_mae: 25.3357,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2583/10000,\n",
      " train_loss: 839.0006,\n",
      " train_mae: 25.3357,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2584/10000,\n",
      " train_loss: 839.0005,\n",
      " train_mae: 25.3357,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2585/10000,\n",
      " train_loss: 839.0002,\n",
      " train_mae: 25.3357,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2586/10000,\n",
      " train_loss: 839.0001,\n",
      " train_mae: 25.3357,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2587/10000,\n",
      " train_loss: 838.9999,\n",
      " train_mae: 25.3357,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2588/10000,\n",
      " train_loss: 838.9998,\n",
      " train_mae: 25.3357,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2589/10000,\n",
      " train_loss: 838.9996,\n",
      " train_mae: 25.3357,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2590/10000,\n",
      " train_loss: 838.9994,\n",
      " train_mae: 25.3357,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2591/10000,\n",
      " train_loss: 838.9993,\n",
      " train_mae: 25.3357,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 2592/10000,\n",
      " train_loss: 838.9991,\n",
      " train_mae: 25.3357,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2593/10000,\n",
      " train_loss: 838.9989,\n",
      " train_mae: 25.3357,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 2594/10000,\n",
      " train_loss: 838.9987,\n",
      " train_mae: 25.3357,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2595/10000,\n",
      " train_loss: 838.9986,\n",
      " train_mae: 25.3356,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 2596/10000,\n",
      " train_loss: 838.9984,\n",
      " train_mae: 25.3356,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2597/10000,\n",
      " train_loss: 838.9982,\n",
      " train_mae: 25.3356,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 2598/10000,\n",
      " train_loss: 838.9980,\n",
      " train_mae: 25.3356,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2599/10000,\n",
      " train_loss: 838.9979,\n",
      " train_mae: 25.3356,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2600/10000,\n",
      " train_loss: 838.9977,\n",
      " train_mae: 25.3356,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2601/10000,\n",
      " train_loss: 838.9975,\n",
      " train_mae: 25.3356,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 2602/10000,\n",
      " train_loss: 838.9974,\n",
      " train_mae: 25.3356,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 2603/10000,\n",
      " train_loss: 838.9972,\n",
      " train_mae: 25.3356,\n",
      " epoch_time_duration: 0.0084\n",
      "\n",
      "epoch: 2604/10000,\n",
      " train_loss: 838.9971,\n",
      " train_mae: 25.3356,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 2605/10000,\n",
      " train_loss: 838.9969,\n",
      " train_mae: 25.3356,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 2606/10000,\n",
      " train_loss: 838.9966,\n",
      " train_mae: 25.3356,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 2607/10000,\n",
      " train_loss: 838.9965,\n",
      " train_mae: 25.3356,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2608/10000,\n",
      " train_loss: 838.9964,\n",
      " train_mae: 25.3356,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 2609/10000,\n",
      " train_loss: 838.9963,\n",
      " train_mae: 25.3355,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 2610/10000,\n",
      " train_loss: 838.9960,\n",
      " train_mae: 25.3355,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 2611/10000,\n",
      " train_loss: 838.9958,\n",
      " train_mae: 25.3355,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2612/10000,\n",
      " train_loss: 838.9957,\n",
      " train_mae: 25.3355,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2613/10000,\n",
      " train_loss: 838.9955,\n",
      " train_mae: 25.3355,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2614/10000,\n",
      " train_loss: 838.9954,\n",
      " train_mae: 25.3355,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2615/10000,\n",
      " train_loss: 838.9952,\n",
      " train_mae: 25.3355,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2616/10000,\n",
      " train_loss: 838.9951,\n",
      " train_mae: 25.3355,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2617/10000,\n",
      " train_loss: 838.9950,\n",
      " train_mae: 25.3355,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2618/10000,\n",
      " train_loss: 838.9948,\n",
      " train_mae: 25.3355,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2619/10000,\n",
      " train_loss: 838.9946,\n",
      " train_mae: 25.3355,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2620/10000,\n",
      " train_loss: 838.9944,\n",
      " train_mae: 25.3355,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2621/10000,\n",
      " train_loss: 838.9943,\n",
      " train_mae: 25.3355,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2622/10000,\n",
      " train_loss: 838.9941,\n",
      " train_mae: 25.3354,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2623/10000,\n",
      " train_loss: 838.9940,\n",
      " train_mae: 25.3354,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2624/10000,\n",
      " train_loss: 838.9938,\n",
      " train_mae: 25.3354,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 2625/10000,\n",
      " train_loss: 838.9936,\n",
      " train_mae: 25.3354,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 2626/10000,\n",
      " train_loss: 838.9935,\n",
      " train_mae: 25.3354,\n",
      " epoch_time_duration: 0.0089\n",
      "\n",
      "epoch: 2627/10000,\n",
      " train_loss: 838.9933,\n",
      " train_mae: 25.3354,\n",
      " epoch_time_duration: 0.0092\n",
      "\n",
      "epoch: 2628/10000,\n",
      " train_loss: 838.9932,\n",
      " train_mae: 25.3354,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 2629/10000,\n",
      " train_loss: 838.9930,\n",
      " train_mae: 25.3354,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2630/10000,\n",
      " train_loss: 838.9928,\n",
      " train_mae: 25.3354,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2631/10000,\n",
      " train_loss: 838.9926,\n",
      " train_mae: 25.3354,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 2632/10000,\n",
      " train_loss: 838.9925,\n",
      " train_mae: 25.3354,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2633/10000,\n",
      " train_loss: 838.9924,\n",
      " train_mae: 25.3354,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 2634/10000,\n",
      " train_loss: 838.9922,\n",
      " train_mae: 25.3354,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 2635/10000,\n",
      " train_loss: 838.9921,\n",
      " train_mae: 25.3354,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 2636/10000,\n",
      " train_loss: 838.9919,\n",
      " train_mae: 25.3353,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2637/10000,\n",
      " train_loss: 838.9916,\n",
      " train_mae: 25.3353,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2638/10000,\n",
      " train_loss: 838.9916,\n",
      " train_mae: 25.3353,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2639/10000,\n",
      " train_loss: 838.9914,\n",
      " train_mae: 25.3353,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2640/10000,\n",
      " train_loss: 838.9913,\n",
      " train_mae: 25.3353,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2641/10000,\n",
      " train_loss: 838.9911,\n",
      " train_mae: 25.3353,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2642/10000,\n",
      " train_loss: 838.9909,\n",
      " train_mae: 25.3353,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 2643/10000,\n",
      " train_loss: 838.9908,\n",
      " train_mae: 25.3353,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2644/10000,\n",
      " train_loss: 838.9906,\n",
      " train_mae: 25.3353,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2645/10000,\n",
      " train_loss: 838.9905,\n",
      " train_mae: 25.3353,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2646/10000,\n",
      " train_loss: 838.9903,\n",
      " train_mae: 25.3353,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2647/10000,\n",
      " train_loss: 838.9902,\n",
      " train_mae: 25.3353,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 2648/10000,\n",
      " train_loss: 838.9899,\n",
      " train_mae: 25.3353,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2649/10000,\n",
      " train_loss: 838.9898,\n",
      " train_mae: 25.3353,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2650/10000,\n",
      " train_loss: 838.9897,\n",
      " train_mae: 25.3353,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2651/10000,\n",
      " train_loss: 838.9895,\n",
      " train_mae: 25.3352,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 2652/10000,\n",
      " train_loss: 838.9894,\n",
      " train_mae: 25.3352,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 2653/10000,\n",
      " train_loss: 838.9893,\n",
      " train_mae: 25.3352,\n",
      " epoch_time_duration: 0.0114\n",
      "\n",
      "epoch: 2654/10000,\n",
      " train_loss: 838.9891,\n",
      " train_mae: 25.3352,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2655/10000,\n",
      " train_loss: 838.9890,\n",
      " train_mae: 25.3352,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2656/10000,\n",
      " train_loss: 838.9888,\n",
      " train_mae: 25.3352,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 2657/10000,\n",
      " train_loss: 838.9886,\n",
      " train_mae: 25.3352,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 2658/10000,\n",
      " train_loss: 838.9885,\n",
      " train_mae: 25.3352,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 2659/10000,\n",
      " train_loss: 838.9884,\n",
      " train_mae: 25.3352,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 2660/10000,\n",
      " train_loss: 838.9882,\n",
      " train_mae: 25.3352,\n",
      " epoch_time_duration: 0.0079\n",
      "\n",
      "epoch: 2661/10000,\n",
      " train_loss: 838.9880,\n",
      " train_mae: 25.3352,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2662/10000,\n",
      " train_loss: 838.9879,\n",
      " train_mae: 25.3352,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 2663/10000,\n",
      " train_loss: 838.9877,\n",
      " train_mae: 25.3352,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2664/10000,\n",
      " train_loss: 838.9875,\n",
      " train_mae: 25.3352,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2665/10000,\n",
      " train_loss: 838.9874,\n",
      " train_mae: 25.3352,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2666/10000,\n",
      " train_loss: 838.9873,\n",
      " train_mae: 25.3351,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2667/10000,\n",
      " train_loss: 838.9871,\n",
      " train_mae: 25.3351,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2668/10000,\n",
      " train_loss: 838.9869,\n",
      " train_mae: 25.3351,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2669/10000,\n",
      " train_loss: 838.9868,\n",
      " train_mae: 25.3351,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2670/10000,\n",
      " train_loss: 838.9867,\n",
      " train_mae: 25.3351,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2671/10000,\n",
      " train_loss: 838.9865,\n",
      " train_mae: 25.3351,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2672/10000,\n",
      " train_loss: 838.9863,\n",
      " train_mae: 25.3351,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2673/10000,\n",
      " train_loss: 838.9863,\n",
      " train_mae: 25.3351,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2674/10000,\n",
      " train_loss: 838.9860,\n",
      " train_mae: 25.3351,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2675/10000,\n",
      " train_loss: 838.9859,\n",
      " train_mae: 25.3351,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2676/10000,\n",
      " train_loss: 838.9858,\n",
      " train_mae: 25.3351,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2677/10000,\n",
      " train_loss: 838.9856,\n",
      " train_mae: 25.3351,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2678/10000,\n",
      " train_loss: 838.9855,\n",
      " train_mae: 25.3351,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2679/10000,\n",
      " train_loss: 838.9853,\n",
      " train_mae: 25.3351,\n",
      " epoch_time_duration: 0.0116\n",
      "\n",
      "epoch: 2680/10000,\n",
      " train_loss: 838.9852,\n",
      " train_mae: 25.3351,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 2681/10000,\n",
      " train_loss: 838.9850,\n",
      " train_mae: 25.3350,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 2682/10000,\n",
      " train_loss: 838.9849,\n",
      " train_mae: 25.3350,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 2683/10000,\n",
      " train_loss: 838.9847,\n",
      " train_mae: 25.3350,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 2684/10000,\n",
      " train_loss: 838.9846,\n",
      " train_mae: 25.3350,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 2685/10000,\n",
      " train_loss: 838.9844,\n",
      " train_mae: 25.3350,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 2686/10000,\n",
      " train_loss: 838.9843,\n",
      " train_mae: 25.3350,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2687/10000,\n",
      " train_loss: 838.9841,\n",
      " train_mae: 25.3350,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2688/10000,\n",
      " train_loss: 838.9840,\n",
      " train_mae: 25.3350,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 2689/10000,\n",
      " train_loss: 838.9838,\n",
      " train_mae: 25.3350,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2690/10000,\n",
      " train_loss: 838.9838,\n",
      " train_mae: 25.3350,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2691/10000,\n",
      " train_loss: 838.9835,\n",
      " train_mae: 25.3350,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2692/10000,\n",
      " train_loss: 838.9835,\n",
      " train_mae: 25.3350,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2693/10000,\n",
      " train_loss: 838.9833,\n",
      " train_mae: 25.3350,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2694/10000,\n",
      " train_loss: 838.9832,\n",
      " train_mae: 25.3350,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2695/10000,\n",
      " train_loss: 838.9830,\n",
      " train_mae: 25.3350,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2696/10000,\n",
      " train_loss: 838.9828,\n",
      " train_mae: 25.3349,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2697/10000,\n",
      " train_loss: 838.9827,\n",
      " train_mae: 25.3349,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2698/10000,\n",
      " train_loss: 838.9826,\n",
      " train_mae: 25.3349,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2699/10000,\n",
      " train_loss: 838.9824,\n",
      " train_mae: 25.3349,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2700/10000,\n",
      " train_loss: 838.9823,\n",
      " train_mae: 25.3349,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2701/10000,\n",
      " train_loss: 838.9821,\n",
      " train_mae: 25.3349,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2702/10000,\n",
      " train_loss: 838.9819,\n",
      " train_mae: 25.3349,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 2703/10000,\n",
      " train_loss: 838.9818,\n",
      " train_mae: 25.3349,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2704/10000,\n",
      " train_loss: 838.9817,\n",
      " train_mae: 25.3349,\n",
      " epoch_time_duration: 0.0167\n",
      "\n",
      "epoch: 2705/10000,\n",
      " train_loss: 838.9816,\n",
      " train_mae: 25.3349,\n",
      " epoch_time_duration: 0.0116\n",
      "\n",
      "epoch: 2706/10000,\n",
      " train_loss: 838.9814,\n",
      " train_mae: 25.3349,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 2707/10000,\n",
      " train_loss: 838.9813,\n",
      " train_mae: 25.3349,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 2708/10000,\n",
      " train_loss: 838.9811,\n",
      " train_mae: 25.3349,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 2709/10000,\n",
      " train_loss: 838.9810,\n",
      " train_mae: 25.3349,\n",
      " epoch_time_duration: 0.0072\n",
      "\n",
      "epoch: 2710/10000,\n",
      " train_loss: 838.9808,\n",
      " train_mae: 25.3349,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 2711/10000,\n",
      " train_loss: 838.9807,\n",
      " train_mae: 25.3349,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 2712/10000,\n",
      " train_loss: 838.9805,\n",
      " train_mae: 25.3349,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 2713/10000,\n",
      " train_loss: 838.9804,\n",
      " train_mae: 25.3349,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2714/10000,\n",
      " train_loss: 838.9802,\n",
      " train_mae: 25.3348,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2715/10000,\n",
      " train_loss: 838.9801,\n",
      " train_mae: 25.3348,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2716/10000,\n",
      " train_loss: 838.9799,\n",
      " train_mae: 25.3348,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2717/10000,\n",
      " train_loss: 838.9798,\n",
      " train_mae: 25.3348,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2718/10000,\n",
      " train_loss: 838.9797,\n",
      " train_mae: 25.3348,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2719/10000,\n",
      " train_loss: 838.9796,\n",
      " train_mae: 25.3348,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2720/10000,\n",
      " train_loss: 838.9795,\n",
      " train_mae: 25.3348,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2721/10000,\n",
      " train_loss: 838.9792,\n",
      " train_mae: 25.3348,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2722/10000,\n",
      " train_loss: 838.9791,\n",
      " train_mae: 25.3348,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2723/10000,\n",
      " train_loss: 838.9791,\n",
      " train_mae: 25.3348,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2724/10000,\n",
      " train_loss: 838.9788,\n",
      " train_mae: 25.3348,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2725/10000,\n",
      " train_loss: 838.9787,\n",
      " train_mae: 25.3348,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2726/10000,\n",
      " train_loss: 838.9786,\n",
      " train_mae: 25.3348,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2727/10000,\n",
      " train_loss: 838.9785,\n",
      " train_mae: 25.3348,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2728/10000,\n",
      " train_loss: 838.9783,\n",
      " train_mae: 25.3348,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2729/10000,\n",
      " train_loss: 838.9781,\n",
      " train_mae: 25.3348,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2730/10000,\n",
      " train_loss: 838.9780,\n",
      " train_mae: 25.3348,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2731/10000,\n",
      " train_loss: 838.9779,\n",
      " train_mae: 25.3347,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2732/10000,\n",
      " train_loss: 838.9778,\n",
      " train_mae: 25.3347,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2733/10000,\n",
      " train_loss: 838.9777,\n",
      " train_mae: 25.3347,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2734/10000,\n",
      " train_loss: 838.9775,\n",
      " train_mae: 25.3347,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2735/10000,\n",
      " train_loss: 838.9774,\n",
      " train_mae: 25.3347,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 2736/10000,\n",
      " train_loss: 838.9772,\n",
      " train_mae: 25.3347,\n",
      " epoch_time_duration: 0.0121\n",
      "\n",
      "epoch: 2737/10000,\n",
      " train_loss: 838.9771,\n",
      " train_mae: 25.3347,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 2738/10000,\n",
      " train_loss: 838.9769,\n",
      " train_mae: 25.3347,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 2739/10000,\n",
      " train_loss: 838.9768,\n",
      " train_mae: 25.3347,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 2740/10000,\n",
      " train_loss: 838.9767,\n",
      " train_mae: 25.3347,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 2741/10000,\n",
      " train_loss: 838.9766,\n",
      " train_mae: 25.3347,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 2742/10000,\n",
      " train_loss: 838.9764,\n",
      " train_mae: 25.3347,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2743/10000,\n",
      " train_loss: 838.9763,\n",
      " train_mae: 25.3347,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 2744/10000,\n",
      " train_loss: 838.9762,\n",
      " train_mae: 25.3347,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2745/10000,\n",
      " train_loss: 838.9760,\n",
      " train_mae: 25.3347,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 2746/10000,\n",
      " train_loss: 838.9759,\n",
      " train_mae: 25.3347,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2747/10000,\n",
      " train_loss: 838.9758,\n",
      " train_mae: 25.3347,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2748/10000,\n",
      " train_loss: 838.9757,\n",
      " train_mae: 25.3346,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2749/10000,\n",
      " train_loss: 838.9755,\n",
      " train_mae: 25.3346,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2750/10000,\n",
      " train_loss: 838.9755,\n",
      " train_mae: 25.3346,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2751/10000,\n",
      " train_loss: 838.9752,\n",
      " train_mae: 25.3346,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2752/10000,\n",
      " train_loss: 838.9750,\n",
      " train_mae: 25.3346,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2753/10000,\n",
      " train_loss: 838.9750,\n",
      " train_mae: 25.3346,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2754/10000,\n",
      " train_loss: 838.9748,\n",
      " train_mae: 25.3346,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2755/10000,\n",
      " train_loss: 838.9747,\n",
      " train_mae: 25.3346,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2756/10000,\n",
      " train_loss: 838.9745,\n",
      " train_mae: 25.3346,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2757/10000,\n",
      " train_loss: 838.9744,\n",
      " train_mae: 25.3346,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2758/10000,\n",
      " train_loss: 838.9743,\n",
      " train_mae: 25.3346,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2759/10000,\n",
      " train_loss: 838.9741,\n",
      " train_mae: 25.3346,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2760/10000,\n",
      " train_loss: 838.9741,\n",
      " train_mae: 25.3346,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2761/10000,\n",
      " train_loss: 838.9740,\n",
      " train_mae: 25.3346,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2762/10000,\n",
      " train_loss: 838.9738,\n",
      " train_mae: 25.3346,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2763/10000,\n",
      " train_loss: 838.9737,\n",
      " train_mae: 25.3346,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2764/10000,\n",
      " train_loss: 838.9735,\n",
      " train_mae: 25.3346,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 2765/10000,\n",
      " train_loss: 838.9734,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0129\n",
      "\n",
      "epoch: 2766/10000,\n",
      " train_loss: 838.9733,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2767/10000,\n",
      " train_loss: 838.9731,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 2768/10000,\n",
      " train_loss: 838.9731,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 2769/10000,\n",
      " train_loss: 838.9729,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 2770/10000,\n",
      " train_loss: 838.9728,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 2771/10000,\n",
      " train_loss: 838.9727,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2772/10000,\n",
      " train_loss: 838.9724,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2773/10000,\n",
      " train_loss: 838.9724,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2774/10000,\n",
      " train_loss: 838.9723,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2775/10000,\n",
      " train_loss: 838.9721,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2776/10000,\n",
      " train_loss: 838.9720,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2777/10000,\n",
      " train_loss: 838.9719,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2778/10000,\n",
      " train_loss: 838.9717,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2779/10000,\n",
      " train_loss: 838.9717,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2780/10000,\n",
      " train_loss: 838.9716,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2781/10000,\n",
      " train_loss: 838.9714,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2782/10000,\n",
      " train_loss: 838.9713,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2783/10000,\n",
      " train_loss: 838.9711,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2784/10000,\n",
      " train_loss: 838.9710,\n",
      " train_mae: 25.3345,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2785/10000,\n",
      " train_loss: 838.9709,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2786/10000,\n",
      " train_loss: 838.9708,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2787/10000,\n",
      " train_loss: 838.9706,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2788/10000,\n",
      " train_loss: 838.9705,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2789/10000,\n",
      " train_loss: 838.9703,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2790/10000,\n",
      " train_loss: 838.9702,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0093\n",
      "\n",
      "epoch: 2791/10000,\n",
      " train_loss: 838.9701,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0073\n",
      "\n",
      "epoch: 2792/10000,\n",
      " train_loss: 838.9700,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 2793/10000,\n",
      " train_loss: 838.9699,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 2794/10000,\n",
      " train_loss: 838.9698,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2795/10000,\n",
      " train_loss: 838.9697,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2796/10000,\n",
      " train_loss: 838.9694,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2797/10000,\n",
      " train_loss: 838.9694,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2798/10000,\n",
      " train_loss: 838.9694,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2799/10000,\n",
      " train_loss: 838.9691,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2800/10000,\n",
      " train_loss: 838.9691,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2801/10000,\n",
      " train_loss: 838.9689,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2802/10000,\n",
      " train_loss: 838.9688,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2803/10000,\n",
      " train_loss: 838.9688,\n",
      " train_mae: 25.3344,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2804/10000,\n",
      " train_loss: 838.9685,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2805/10000,\n",
      " train_loss: 838.9684,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2806/10000,\n",
      " train_loss: 838.9682,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0078\n",
      "\n",
      "epoch: 2807/10000,\n",
      " train_loss: 838.9682,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 2808/10000,\n",
      " train_loss: 838.9680,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 2809/10000,\n",
      " train_loss: 838.9680,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 2810/10000,\n",
      " train_loss: 838.9678,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 2811/10000,\n",
      " train_loss: 838.9677,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 2812/10000,\n",
      " train_loss: 838.9675,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0078\n",
      "\n",
      "epoch: 2813/10000,\n",
      " train_loss: 838.9675,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2814/10000,\n",
      " train_loss: 838.9673,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2815/10000,\n",
      " train_loss: 838.9672,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 2816/10000,\n",
      " train_loss: 838.9671,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0078\n",
      "\n",
      "epoch: 2817/10000,\n",
      " train_loss: 838.9670,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 2818/10000,\n",
      " train_loss: 838.9669,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 2819/10000,\n",
      " train_loss: 838.9668,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 2820/10000,\n",
      " train_loss: 838.9667,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2821/10000,\n",
      " train_loss: 838.9665,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2822/10000,\n",
      " train_loss: 838.9662,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2823/10000,\n",
      " train_loss: 838.9662,\n",
      " train_mae: 25.3343,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2824/10000,\n",
      " train_loss: 838.9661,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2825/10000,\n",
      " train_loss: 838.9661,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2826/10000,\n",
      " train_loss: 838.9659,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2827/10000,\n",
      " train_loss: 838.9659,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2828/10000,\n",
      " train_loss: 838.9657,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2829/10000,\n",
      " train_loss: 838.9657,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2830/10000,\n",
      " train_loss: 838.9655,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2831/10000,\n",
      " train_loss: 838.9654,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2832/10000,\n",
      " train_loss: 838.9653,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2833/10000,\n",
      " train_loss: 838.9651,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2834/10000,\n",
      " train_loss: 838.9650,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2835/10000,\n",
      " train_loss: 838.9649,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2836/10000,\n",
      " train_loss: 838.9648,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2837/10000,\n",
      " train_loss: 838.9646,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2838/10000,\n",
      " train_loss: 838.9645,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2839/10000,\n",
      " train_loss: 838.9644,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2840/10000,\n",
      " train_loss: 838.9643,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2841/10000,\n",
      " train_loss: 838.9642,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2842/10000,\n",
      " train_loss: 838.9641,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2843/10000,\n",
      " train_loss: 838.9640,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2844/10000,\n",
      " train_loss: 838.9639,\n",
      " train_mae: 25.3342,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2845/10000,\n",
      " train_loss: 838.9637,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2846/10000,\n",
      " train_loss: 838.9636,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2847/10000,\n",
      " train_loss: 838.9634,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0088\n",
      "\n",
      "epoch: 2848/10000,\n",
      " train_loss: 838.9634,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0111\n",
      "\n",
      "epoch: 2849/10000,\n",
      " train_loss: 838.9633,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2850/10000,\n",
      " train_loss: 838.9632,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2851/10000,\n",
      " train_loss: 838.9630,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 2852/10000,\n",
      " train_loss: 838.9630,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 2853/10000,\n",
      " train_loss: 838.9628,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2854/10000,\n",
      " train_loss: 838.9626,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 2855/10000,\n",
      " train_loss: 838.9626,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2856/10000,\n",
      " train_loss: 838.9625,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2857/10000,\n",
      " train_loss: 838.9623,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 2858/10000,\n",
      " train_loss: 838.9623,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 2859/10000,\n",
      " train_loss: 838.9622,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 2860/10000,\n",
      " train_loss: 838.9620,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2861/10000,\n",
      " train_loss: 838.9619,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 2862/10000,\n",
      " train_loss: 838.9618,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2863/10000,\n",
      " train_loss: 838.9617,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2864/10000,\n",
      " train_loss: 838.9616,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 2865/10000,\n",
      " train_loss: 838.9615,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 2866/10000,\n",
      " train_loss: 838.9614,\n",
      " train_mae: 25.3341,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2867/10000,\n",
      " train_loss: 838.9612,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2868/10000,\n",
      " train_loss: 838.9612,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2869/10000,\n",
      " train_loss: 838.9611,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2870/10000,\n",
      " train_loss: 838.9609,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2871/10000,\n",
      " train_loss: 838.9609,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2872/10000,\n",
      " train_loss: 838.9607,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2873/10000,\n",
      " train_loss: 838.9606,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 2874/10000,\n",
      " train_loss: 838.9604,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0149\n",
      "\n",
      "epoch: 2875/10000,\n",
      " train_loss: 838.9604,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0082\n",
      "\n",
      "epoch: 2876/10000,\n",
      " train_loss: 838.9603,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 2877/10000,\n",
      " train_loss: 838.9601,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 2878/10000,\n",
      " train_loss: 838.9601,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 2879/10000,\n",
      " train_loss: 838.9600,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 2880/10000,\n",
      " train_loss: 838.9598,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 2881/10000,\n",
      " train_loss: 838.9597,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2882/10000,\n",
      " train_loss: 838.9596,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2883/10000,\n",
      " train_loss: 838.9595,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2884/10000,\n",
      " train_loss: 838.9594,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2885/10000,\n",
      " train_loss: 838.9593,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2886/10000,\n",
      " train_loss: 838.9592,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2887/10000,\n",
      " train_loss: 838.9590,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2888/10000,\n",
      " train_loss: 838.9590,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2889/10000,\n",
      " train_loss: 838.9589,\n",
      " train_mae: 25.3340,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 2890/10000,\n",
      " train_loss: 838.9587,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2891/10000,\n",
      " train_loss: 838.9586,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2892/10000,\n",
      " train_loss: 838.9586,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2893/10000,\n",
      " train_loss: 838.9584,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2894/10000,\n",
      " train_loss: 838.9584,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2895/10000,\n",
      " train_loss: 838.9583,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2896/10000,\n",
      " train_loss: 838.9582,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2897/10000,\n",
      " train_loss: 838.9580,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2898/10000,\n",
      " train_loss: 838.9580,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2899/10000,\n",
      " train_loss: 838.9579,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2900/10000,\n",
      " train_loss: 838.9577,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2901/10000,\n",
      " train_loss: 838.9576,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0096\n",
      "\n",
      "epoch: 2902/10000,\n",
      " train_loss: 838.9575,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 2903/10000,\n",
      " train_loss: 838.9575,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 2904/10000,\n",
      " train_loss: 838.9573,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 2905/10000,\n",
      " train_loss: 838.9572,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2906/10000,\n",
      " train_loss: 838.9572,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 2907/10000,\n",
      " train_loss: 838.9571,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0077\n",
      "\n",
      "epoch: 2908/10000,\n",
      " train_loss: 838.9569,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 2909/10000,\n",
      " train_loss: 838.9568,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2910/10000,\n",
      " train_loss: 838.9567,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 2911/10000,\n",
      " train_loss: 838.9565,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 2912/10000,\n",
      " train_loss: 838.9564,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2913/10000,\n",
      " train_loss: 838.9564,\n",
      " train_mae: 25.3339,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 2914/10000,\n",
      " train_loss: 838.9563,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2915/10000,\n",
      " train_loss: 838.9562,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2916/10000,\n",
      " train_loss: 838.9561,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2917/10000,\n",
      " train_loss: 838.9561,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2918/10000,\n",
      " train_loss: 838.9559,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2919/10000,\n",
      " train_loss: 838.9557,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2920/10000,\n",
      " train_loss: 838.9556,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2921/10000,\n",
      " train_loss: 838.9556,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2922/10000,\n",
      " train_loss: 838.9554,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 2923/10000,\n",
      " train_loss: 838.9554,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2924/10000,\n",
      " train_loss: 838.9553,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2925/10000,\n",
      " train_loss: 838.9552,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2926/10000,\n",
      " train_loss: 838.9551,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0082\n",
      "\n",
      "epoch: 2927/10000,\n",
      " train_loss: 838.9550,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0085\n",
      "\n",
      "epoch: 2928/10000,\n",
      " train_loss: 838.9548,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2929/10000,\n",
      " train_loss: 838.9547,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2930/10000,\n",
      " train_loss: 838.9547,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 2931/10000,\n",
      " train_loss: 838.9545,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 2932/10000,\n",
      " train_loss: 838.9545,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 2933/10000,\n",
      " train_loss: 838.9543,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 2934/10000,\n",
      " train_loss: 838.9543,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 2935/10000,\n",
      " train_loss: 838.9542,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 2936/10000,\n",
      " train_loss: 838.9541,\n",
      " train_mae: 25.3338,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 2937/10000,\n",
      " train_loss: 838.9540,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2938/10000,\n",
      " train_loss: 838.9538,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 2939/10000,\n",
      " train_loss: 838.9537,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2940/10000,\n",
      " train_loss: 838.9537,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2941/10000,\n",
      " train_loss: 838.9536,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2942/10000,\n",
      " train_loss: 838.9535,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 2943/10000,\n",
      " train_loss: 838.9534,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2944/10000,\n",
      " train_loss: 838.9533,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 2945/10000,\n",
      " train_loss: 838.9532,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 2946/10000,\n",
      " train_loss: 838.9531,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 2947/10000,\n",
      " train_loss: 838.9530,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 2948/10000,\n",
      " train_loss: 838.9529,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 2949/10000,\n",
      " train_loss: 838.9527,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 2950/10000,\n",
      " train_loss: 838.9528,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 2951/10000,\n",
      " train_loss: 838.9526,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 2952/10000,\n",
      " train_loss: 838.9525,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0116\n",
      "\n",
      "epoch: 2953/10000,\n",
      " train_loss: 838.9524,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2954/10000,\n",
      " train_loss: 838.9523,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2955/10000,\n",
      " train_loss: 838.9523,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 2956/10000,\n",
      " train_loss: 838.9521,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2957/10000,\n",
      " train_loss: 838.9520,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2958/10000,\n",
      " train_loss: 838.9519,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2959/10000,\n",
      " train_loss: 838.9518,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2960/10000,\n",
      " train_loss: 838.9517,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 2961/10000,\n",
      " train_loss: 838.9516,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 2962/10000,\n",
      " train_loss: 838.9515,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 2963/10000,\n",
      " train_loss: 838.9514,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 2964/10000,\n",
      " train_loss: 838.9514,\n",
      " train_mae: 25.3337,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2965/10000,\n",
      " train_loss: 838.9513,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2966/10000,\n",
      " train_loss: 838.9512,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 2967/10000,\n",
      " train_loss: 838.9511,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 2968/10000,\n",
      " train_loss: 838.9509,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 2969/10000,\n",
      " train_loss: 838.9509,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 2970/10000,\n",
      " train_loss: 838.9507,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2971/10000,\n",
      " train_loss: 838.9506,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2972/10000,\n",
      " train_loss: 838.9506,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2973/10000,\n",
      " train_loss: 838.9505,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2974/10000,\n",
      " train_loss: 838.9504,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2975/10000,\n",
      " train_loss: 838.9503,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2976/10000,\n",
      " train_loss: 838.9502,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2977/10000,\n",
      " train_loss: 838.9502,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0114\n",
      "\n",
      "epoch: 2978/10000,\n",
      " train_loss: 838.9501,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0098\n",
      "\n",
      "epoch: 2979/10000,\n",
      " train_loss: 838.9500,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 2980/10000,\n",
      " train_loss: 838.9499,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 2981/10000,\n",
      " train_loss: 838.9498,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 2982/10000,\n",
      " train_loss: 838.9496,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 2983/10000,\n",
      " train_loss: 838.9495,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 2984/10000,\n",
      " train_loss: 838.9495,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 2985/10000,\n",
      " train_loss: 838.9494,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 2986/10000,\n",
      " train_loss: 838.9493,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2987/10000,\n",
      " train_loss: 838.9492,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2988/10000,\n",
      " train_loss: 838.9491,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 2989/10000,\n",
      " train_loss: 838.9490,\n",
      " train_mae: 25.3336,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2990/10000,\n",
      " train_loss: 838.9489,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 2991/10000,\n",
      " train_loss: 838.9489,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2992/10000,\n",
      " train_loss: 838.9487,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2993/10000,\n",
      " train_loss: 838.9486,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2994/10000,\n",
      " train_loss: 838.9485,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 2995/10000,\n",
      " train_loss: 838.9485,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2996/10000,\n",
      " train_loss: 838.9484,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 2997/10000,\n",
      " train_loss: 838.9484,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 2998/10000,\n",
      " train_loss: 838.9482,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 2999/10000,\n",
      " train_loss: 838.9481,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3000/10000,\n",
      " train_loss: 838.9480,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3001/10000,\n",
      " train_loss: 838.9479,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3002/10000,\n",
      " train_loss: 838.9478,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3003/10000,\n",
      " train_loss: 838.9477,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3004/10000,\n",
      " train_loss: 838.9476,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0146\n",
      "\n",
      "epoch: 3005/10000,\n",
      " train_loss: 838.9476,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0099\n",
      "\n",
      "epoch: 3006/10000,\n",
      " train_loss: 838.9474,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 3007/10000,\n",
      " train_loss: 838.9473,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 3008/10000,\n",
      " train_loss: 838.9473,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0080\n",
      "\n",
      "epoch: 3009/10000,\n",
      " train_loss: 838.9472,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0076\n",
      "\n",
      "epoch: 3010/10000,\n",
      " train_loss: 838.9471,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 3011/10000,\n",
      " train_loss: 838.9470,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3012/10000,\n",
      " train_loss: 838.9469,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3013/10000,\n",
      " train_loss: 838.9468,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3014/10000,\n",
      " train_loss: 838.9467,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3015/10000,\n",
      " train_loss: 838.9467,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3016/10000,\n",
      " train_loss: 838.9465,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3017/10000,\n",
      " train_loss: 838.9464,\n",
      " train_mae: 25.3335,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3018/10000,\n",
      " train_loss: 838.9464,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3019/10000,\n",
      " train_loss: 838.9464,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3020/10000,\n",
      " train_loss: 838.9463,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3021/10000,\n",
      " train_loss: 838.9462,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3022/10000,\n",
      " train_loss: 838.9460,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3023/10000,\n",
      " train_loss: 838.9459,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0134\n",
      "\n",
      "epoch: 3024/10000,\n",
      " train_loss: 838.9459,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 3025/10000,\n",
      " train_loss: 838.9457,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3026/10000,\n",
      " train_loss: 838.9456,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 3027/10000,\n",
      " train_loss: 838.9456,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3028/10000,\n",
      " train_loss: 838.9455,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3029/10000,\n",
      " train_loss: 838.9455,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3030/10000,\n",
      " train_loss: 838.9454,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3031/10000,\n",
      " train_loss: 838.9453,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3032/10000,\n",
      " train_loss: 838.9452,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 3033/10000,\n",
      " train_loss: 838.9451,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3034/10000,\n",
      " train_loss: 838.9449,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3035/10000,\n",
      " train_loss: 838.9449,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3036/10000,\n",
      " train_loss: 838.9448,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3037/10000,\n",
      " train_loss: 838.9447,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3038/10000,\n",
      " train_loss: 838.9446,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3039/10000,\n",
      " train_loss: 838.9446,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3040/10000,\n",
      " train_loss: 838.9445,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3041/10000,\n",
      " train_loss: 838.9444,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3042/10000,\n",
      " train_loss: 838.9443,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3043/10000,\n",
      " train_loss: 838.9442,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3044/10000,\n",
      " train_loss: 838.9441,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3045/10000,\n",
      " train_loss: 838.9441,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3046/10000,\n",
      " train_loss: 838.9440,\n",
      " train_mae: 25.3334,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3047/10000,\n",
      " train_loss: 838.9439,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3048/10000,\n",
      " train_loss: 838.9438,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0122\n",
      "\n",
      "epoch: 3049/10000,\n",
      " train_loss: 838.9437,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 3050/10000,\n",
      " train_loss: 838.9436,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0102\n",
      "\n",
      "epoch: 3051/10000,\n",
      " train_loss: 838.9435,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3052/10000,\n",
      " train_loss: 838.9435,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3053/10000,\n",
      " train_loss: 838.9434,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 3054/10000,\n",
      " train_loss: 838.9434,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3055/10000,\n",
      " train_loss: 838.9431,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 3056/10000,\n",
      " train_loss: 838.9431,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3057/10000,\n",
      " train_loss: 838.9430,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3058/10000,\n",
      " train_loss: 838.9429,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3059/10000,\n",
      " train_loss: 838.9429,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3060/10000,\n",
      " train_loss: 838.9427,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 3061/10000,\n",
      " train_loss: 838.9427,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 3062/10000,\n",
      " train_loss: 838.9426,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3063/10000,\n",
      " train_loss: 838.9425,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3064/10000,\n",
      " train_loss: 838.9424,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3065/10000,\n",
      " train_loss: 838.9424,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3066/10000,\n",
      " train_loss: 838.9423,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3067/10000,\n",
      " train_loss: 838.9423,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3068/10000,\n",
      " train_loss: 838.9421,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3069/10000,\n",
      " train_loss: 838.9420,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3070/10000,\n",
      " train_loss: 838.9420,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3071/10000,\n",
      " train_loss: 838.9418,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3072/10000,\n",
      " train_loss: 838.9418,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 3073/10000,\n",
      " train_loss: 838.9417,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3074/10000,\n",
      " train_loss: 838.9416,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3075/10000,\n",
      " train_loss: 838.9416,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3076/10000,\n",
      " train_loss: 838.9415,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3077/10000,\n",
      " train_loss: 838.9413,\n",
      " train_mae: 25.3333,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3078/10000,\n",
      " train_loss: 838.9413,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 3079/10000,\n",
      " train_loss: 838.9412,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3080/10000,\n",
      " train_loss: 838.9412,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3081/10000,\n",
      " train_loss: 838.9410,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3082/10000,\n",
      " train_loss: 838.9409,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3083/10000,\n",
      " train_loss: 838.9409,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0111\n",
      "\n",
      "epoch: 3084/10000,\n",
      " train_loss: 838.9408,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 3085/10000,\n",
      " train_loss: 838.9407,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3086/10000,\n",
      " train_loss: 838.9407,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3087/10000,\n",
      " train_loss: 838.9406,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3088/10000,\n",
      " train_loss: 838.9406,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3089/10000,\n",
      " train_loss: 838.9404,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0079\n",
      "\n",
      "epoch: 3090/10000,\n",
      " train_loss: 838.9404,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 3091/10000,\n",
      " train_loss: 838.9402,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 3092/10000,\n",
      " train_loss: 838.9401,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0093\n",
      "\n",
      "epoch: 3093/10000,\n",
      " train_loss: 838.9401,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 3094/10000,\n",
      " train_loss: 838.9400,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 3095/10000,\n",
      " train_loss: 838.9399,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 3096/10000,\n",
      " train_loss: 838.9398,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 3097/10000,\n",
      " train_loss: 838.9398,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3098/10000,\n",
      " train_loss: 838.9397,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3099/10000,\n",
      " train_loss: 838.9396,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3100/10000,\n",
      " train_loss: 838.9395,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3101/10000,\n",
      " train_loss: 838.9395,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3102/10000,\n",
      " train_loss: 838.9394,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3103/10000,\n",
      " train_loss: 838.9393,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3104/10000,\n",
      " train_loss: 838.9392,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3105/10000,\n",
      " train_loss: 838.9391,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3106/10000,\n",
      " train_loss: 838.9391,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 3107/10000,\n",
      " train_loss: 838.9389,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3108/10000,\n",
      " train_loss: 838.9388,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3109/10000,\n",
      " train_loss: 838.9388,\n",
      " train_mae: 25.3332,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3110/10000,\n",
      " train_loss: 838.9388,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3111/10000,\n",
      " train_loss: 838.9387,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3112/10000,\n",
      " train_loss: 838.9386,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3113/10000,\n",
      " train_loss: 838.9386,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3114/10000,\n",
      " train_loss: 838.9385,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 3115/10000,\n",
      " train_loss: 838.9384,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 3116/10000,\n",
      " train_loss: 838.9382,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3117/10000,\n",
      " train_loss: 838.9382,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0129\n",
      "\n",
      "epoch: 3118/10000,\n",
      " train_loss: 838.9382,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0075\n",
      "\n",
      "epoch: 3119/10000,\n",
      " train_loss: 838.9380,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 3120/10000,\n",
      " train_loss: 838.9379,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 3121/10000,\n",
      " train_loss: 838.9379,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 3122/10000,\n",
      " train_loss: 838.9379,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 3123/10000,\n",
      " train_loss: 838.9378,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3124/10000,\n",
      " train_loss: 838.9377,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3125/10000,\n",
      " train_loss: 838.9376,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 3126/10000,\n",
      " train_loss: 838.9376,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3127/10000,\n",
      " train_loss: 838.9374,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3128/10000,\n",
      " train_loss: 838.9374,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3129/10000,\n",
      " train_loss: 838.9374,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3130/10000,\n",
      " train_loss: 838.9373,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3131/10000,\n",
      " train_loss: 838.9371,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3132/10000,\n",
      " train_loss: 838.9371,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3133/10000,\n",
      " train_loss: 838.9370,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3134/10000,\n",
      " train_loss: 838.9370,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3135/10000,\n",
      " train_loss: 838.9368,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 3136/10000,\n",
      " train_loss: 838.9368,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 3137/10000,\n",
      " train_loss: 838.9367,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 3138/10000,\n",
      " train_loss: 838.9366,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 3139/10000,\n",
      " train_loss: 838.9366,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3140/10000,\n",
      " train_loss: 838.9365,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3141/10000,\n",
      " train_loss: 838.9365,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3142/10000,\n",
      " train_loss: 838.9364,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3143/10000,\n",
      " train_loss: 838.9363,\n",
      " train_mae: 25.3331,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 3144/10000,\n",
      " train_loss: 838.9362,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3145/10000,\n",
      " train_loss: 838.9362,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3146/10000,\n",
      " train_loss: 838.9361,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3147/10000,\n",
      " train_loss: 838.9360,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3148/10000,\n",
      " train_loss: 838.9359,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3149/10000,\n",
      " train_loss: 838.9359,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 3150/10000,\n",
      " train_loss: 838.9357,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0075\n",
      "\n",
      "epoch: 3151/10000,\n",
      " train_loss: 838.9357,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3152/10000,\n",
      " train_loss: 838.9356,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3153/10000,\n",
      " train_loss: 838.9355,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3154/10000,\n",
      " train_loss: 838.9355,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3155/10000,\n",
      " train_loss: 838.9353,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3156/10000,\n",
      " train_loss: 838.9352,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3157/10000,\n",
      " train_loss: 838.9352,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3158/10000,\n",
      " train_loss: 838.9352,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3159/10000,\n",
      " train_loss: 838.9350,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 3160/10000,\n",
      " train_loss: 838.9350,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 3161/10000,\n",
      " train_loss: 838.9349,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 3162/10000,\n",
      " train_loss: 838.9349,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 3163/10000,\n",
      " train_loss: 838.9348,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3164/10000,\n",
      " train_loss: 838.9348,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3165/10000,\n",
      " train_loss: 838.9347,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3166/10000,\n",
      " train_loss: 838.9346,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 3167/10000,\n",
      " train_loss: 838.9346,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 3168/10000,\n",
      " train_loss: 838.9344,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 3169/10000,\n",
      " train_loss: 838.9344,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0025\n",
      "\n",
      "epoch: 3170/10000,\n",
      " train_loss: 838.9343,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 3171/10000,\n",
      " train_loss: 838.9343,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3172/10000,\n",
      " train_loss: 838.9341,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 3173/10000,\n",
      " train_loss: 838.9341,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 3174/10000,\n",
      " train_loss: 838.9340,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3175/10000,\n",
      " train_loss: 838.9340,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 3176/10000,\n",
      " train_loss: 838.9339,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 3177/10000,\n",
      " train_loss: 838.9338,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 3178/10000,\n",
      " train_loss: 838.9338,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 3179/10000,\n",
      " train_loss: 838.9337,\n",
      " train_mae: 25.3330,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3180/10000,\n",
      " train_loss: 838.9335,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3181/10000,\n",
      " train_loss: 838.9336,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3182/10000,\n",
      " train_loss: 838.9335,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3183/10000,\n",
      " train_loss: 838.9335,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0104\n",
      "\n",
      "epoch: 3184/10000,\n",
      " train_loss: 838.9333,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3185/10000,\n",
      " train_loss: 838.9332,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3186/10000,\n",
      " train_loss: 838.9332,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3187/10000,\n",
      " train_loss: 838.9330,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 3188/10000,\n",
      " train_loss: 838.9330,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 3189/10000,\n",
      " train_loss: 838.9330,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 3190/10000,\n",
      " train_loss: 838.9329,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 3191/10000,\n",
      " train_loss: 838.9329,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3192/10000,\n",
      " train_loss: 838.9329,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3193/10000,\n",
      " train_loss: 838.9327,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3194/10000,\n",
      " train_loss: 838.9327,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3195/10000,\n",
      " train_loss: 838.9326,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3196/10000,\n",
      " train_loss: 838.9325,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3197/10000,\n",
      " train_loss: 838.9325,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3198/10000,\n",
      " train_loss: 838.9324,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3199/10000,\n",
      " train_loss: 838.9323,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3200/10000,\n",
      " train_loss: 838.9322,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3201/10000,\n",
      " train_loss: 838.9322,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3202/10000,\n",
      " train_loss: 838.9321,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3203/10000,\n",
      " train_loss: 838.9320,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3204/10000,\n",
      " train_loss: 838.9319,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3205/10000,\n",
      " train_loss: 838.9319,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3206/10000,\n",
      " train_loss: 838.9318,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3207/10000,\n",
      " train_loss: 838.9318,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3208/10000,\n",
      " train_loss: 838.9317,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3209/10000,\n",
      " train_loss: 838.9316,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3210/10000,\n",
      " train_loss: 838.9316,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3211/10000,\n",
      " train_loss: 838.9315,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3212/10000,\n",
      " train_loss: 838.9314,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3213/10000,\n",
      " train_loss: 838.9313,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3214/10000,\n",
      " train_loss: 838.9313,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3215/10000,\n",
      " train_loss: 838.9311,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3216/10000,\n",
      " train_loss: 838.9311,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3217/10000,\n",
      " train_loss: 838.9311,\n",
      " train_mae: 25.3329,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3218/10000,\n",
      " train_loss: 838.9310,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3219/10000,\n",
      " train_loss: 838.9310,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3220/10000,\n",
      " train_loss: 838.9309,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3221/10000,\n",
      " train_loss: 838.9308,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3222/10000,\n",
      " train_loss: 838.9308,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0096\n",
      "\n",
      "epoch: 3223/10000,\n",
      " train_loss: 838.9307,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3224/10000,\n",
      " train_loss: 838.9307,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3225/10000,\n",
      " train_loss: 838.9305,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3226/10000,\n",
      " train_loss: 838.9305,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3227/10000,\n",
      " train_loss: 838.9304,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0088\n",
      "\n",
      "epoch: 3228/10000,\n",
      " train_loss: 838.9304,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 3229/10000,\n",
      " train_loss: 838.9302,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 3230/10000,\n",
      " train_loss: 838.9302,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3231/10000,\n",
      " train_loss: 838.9301,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3232/10000,\n",
      " train_loss: 838.9301,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3233/10000,\n",
      " train_loss: 838.9300,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3234/10000,\n",
      " train_loss: 838.9300,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3235/10000,\n",
      " train_loss: 838.9299,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 3236/10000,\n",
      " train_loss: 838.9299,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 3237/10000,\n",
      " train_loss: 838.9297,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3238/10000,\n",
      " train_loss: 838.9297,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3239/10000,\n",
      " train_loss: 838.9296,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 3240/10000,\n",
      " train_loss: 838.9296,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3241/10000,\n",
      " train_loss: 838.9294,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 3242/10000,\n",
      " train_loss: 838.9294,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3243/10000,\n",
      " train_loss: 838.9294,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 3244/10000,\n",
      " train_loss: 838.9293,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3245/10000,\n",
      " train_loss: 838.9291,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3246/10000,\n",
      " train_loss: 838.9291,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3247/10000,\n",
      " train_loss: 838.9291,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3248/10000,\n",
      " train_loss: 838.9290,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3249/10000,\n",
      " train_loss: 838.9290,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3250/10000,\n",
      " train_loss: 838.9290,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0081\n",
      "\n",
      "epoch: 3251/10000,\n",
      " train_loss: 838.9288,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 3252/10000,\n",
      " train_loss: 838.9288,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 3253/10000,\n",
      " train_loss: 838.9287,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 3254/10000,\n",
      " train_loss: 838.9286,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3255/10000,\n",
      " train_loss: 838.9286,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 3256/10000,\n",
      " train_loss: 838.9286,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3257/10000,\n",
      " train_loss: 838.9284,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3258/10000,\n",
      " train_loss: 838.9284,\n",
      " train_mae: 25.3328,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 3259/10000,\n",
      " train_loss: 838.9283,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3260/10000,\n",
      " train_loss: 838.9283,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3261/10000,\n",
      " train_loss: 838.9283,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3262/10000,\n",
      " train_loss: 838.9282,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3263/10000,\n",
      " train_loss: 838.9281,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3264/10000,\n",
      " train_loss: 838.9280,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3265/10000,\n",
      " train_loss: 838.9280,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3266/10000,\n",
      " train_loss: 838.9279,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3267/10000,\n",
      " train_loss: 838.9279,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3268/10000,\n",
      " train_loss: 838.9278,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 3269/10000,\n",
      " train_loss: 838.9277,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3270/10000,\n",
      " train_loss: 838.9276,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3271/10000,\n",
      " train_loss: 838.9275,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 3272/10000,\n",
      " train_loss: 838.9275,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3273/10000,\n",
      " train_loss: 838.9275,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3274/10000,\n",
      " train_loss: 838.9274,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3275/10000,\n",
      " train_loss: 838.9273,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 3276/10000,\n",
      " train_loss: 838.9273,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 3277/10000,\n",
      " train_loss: 838.9272,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3278/10000,\n",
      " train_loss: 838.9271,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 3279/10000,\n",
      " train_loss: 838.9271,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3280/10000,\n",
      " train_loss: 838.9271,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3281/10000,\n",
      " train_loss: 838.9270,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3282/10000,\n",
      " train_loss: 838.9269,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3283/10000,\n",
      " train_loss: 838.9268,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3284/10000,\n",
      " train_loss: 838.9268,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3285/10000,\n",
      " train_loss: 838.9268,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3286/10000,\n",
      " train_loss: 838.9267,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3287/10000,\n",
      " train_loss: 838.9266,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 3288/10000,\n",
      " train_loss: 838.9265,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0077\n",
      "\n",
      "epoch: 3289/10000,\n",
      " train_loss: 838.9265,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 3290/10000,\n",
      " train_loss: 838.9265,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3291/10000,\n",
      " train_loss: 838.9263,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 3292/10000,\n",
      " train_loss: 838.9263,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 3293/10000,\n",
      " train_loss: 838.9262,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3294/10000,\n",
      " train_loss: 838.9261,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3295/10000,\n",
      " train_loss: 838.9261,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 3296/10000,\n",
      " train_loss: 838.9261,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 3297/10000,\n",
      " train_loss: 838.9260,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3298/10000,\n",
      " train_loss: 838.9260,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 3299/10000,\n",
      " train_loss: 838.9258,\n",
      " train_mae: 25.3327,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3300/10000,\n",
      " train_loss: 838.9258,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3301/10000,\n",
      " train_loss: 838.9258,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 3302/10000,\n",
      " train_loss: 838.9257,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3303/10000,\n",
      " train_loss: 838.9257,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3304/10000,\n",
      " train_loss: 838.9255,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3305/10000,\n",
      " train_loss: 838.9255,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3306/10000,\n",
      " train_loss: 838.9255,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3307/10000,\n",
      " train_loss: 838.9254,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3308/10000,\n",
      " train_loss: 838.9252,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3309/10000,\n",
      " train_loss: 838.9252,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3310/10000,\n",
      " train_loss: 838.9252,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3311/10000,\n",
      " train_loss: 838.9252,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3312/10000,\n",
      " train_loss: 838.9252,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3313/10000,\n",
      " train_loss: 838.9250,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3314/10000,\n",
      " train_loss: 838.9250,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3315/10000,\n",
      " train_loss: 838.9249,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3316/10000,\n",
      " train_loss: 838.9249,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3317/10000,\n",
      " train_loss: 838.9248,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3318/10000,\n",
      " train_loss: 838.9247,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0096\n",
      "\n",
      "epoch: 3319/10000,\n",
      " train_loss: 838.9247,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 3320/10000,\n",
      " train_loss: 838.9246,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3321/10000,\n",
      " train_loss: 838.9245,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 3322/10000,\n",
      " train_loss: 838.9245,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3323/10000,\n",
      " train_loss: 838.9244,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 3324/10000,\n",
      " train_loss: 838.9244,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 3325/10000,\n",
      " train_loss: 838.9243,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3326/10000,\n",
      " train_loss: 838.9243,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3327/10000,\n",
      " train_loss: 838.9242,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 3328/10000,\n",
      " train_loss: 838.9241,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 3329/10000,\n",
      " train_loss: 838.9241,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3330/10000,\n",
      " train_loss: 838.9240,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3331/10000,\n",
      " train_loss: 838.9240,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3332/10000,\n",
      " train_loss: 838.9239,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3333/10000,\n",
      " train_loss: 838.9239,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3334/10000,\n",
      " train_loss: 838.9238,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3335/10000,\n",
      " train_loss: 838.9237,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3336/10000,\n",
      " train_loss: 838.9237,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3337/10000,\n",
      " train_loss: 838.9236,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 3338/10000,\n",
      " train_loss: 838.9235,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3339/10000,\n",
      " train_loss: 838.9235,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 3340/10000,\n",
      " train_loss: 838.9235,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 3341/10000,\n",
      " train_loss: 838.9233,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3342/10000,\n",
      " train_loss: 838.9233,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3343/10000,\n",
      " train_loss: 838.9233,\n",
      " train_mae: 25.3326,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3344/10000,\n",
      " train_loss: 838.9232,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3345/10000,\n",
      " train_loss: 838.9232,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3346/10000,\n",
      " train_loss: 838.9231,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3347/10000,\n",
      " train_loss: 838.9230,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3348/10000,\n",
      " train_loss: 838.9230,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3349/10000,\n",
      " train_loss: 838.9229,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3350/10000,\n",
      " train_loss: 838.9229,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 3351/10000,\n",
      " train_loss: 838.9229,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3352/10000,\n",
      " train_loss: 838.9228,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0081\n",
      "\n",
      "epoch: 3353/10000,\n",
      " train_loss: 838.9227,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0085\n",
      "\n",
      "epoch: 3354/10000,\n",
      " train_loss: 838.9227,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 3355/10000,\n",
      " train_loss: 838.9226,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 3356/10000,\n",
      " train_loss: 838.9225,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3357/10000,\n",
      " train_loss: 838.9224,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3358/10000,\n",
      " train_loss: 838.9224,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3359/10000,\n",
      " train_loss: 838.9223,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3360/10000,\n",
      " train_loss: 838.9223,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3361/10000,\n",
      " train_loss: 838.9223,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3362/10000,\n",
      " train_loss: 838.9222,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0139\n",
      "\n",
      "epoch: 3363/10000,\n",
      " train_loss: 838.9222,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 3364/10000,\n",
      " train_loss: 838.9221,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3365/10000,\n",
      " train_loss: 838.9221,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 3366/10000,\n",
      " train_loss: 838.9220,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3367/10000,\n",
      " train_loss: 838.9219,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 3368/10000,\n",
      " train_loss: 838.9218,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3369/10000,\n",
      " train_loss: 838.9218,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3370/10000,\n",
      " train_loss: 838.9218,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3371/10000,\n",
      " train_loss: 838.9218,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3372/10000,\n",
      " train_loss: 838.9216,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3373/10000,\n",
      " train_loss: 838.9216,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3374/10000,\n",
      " train_loss: 838.9215,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3375/10000,\n",
      " train_loss: 838.9215,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3376/10000,\n",
      " train_loss: 838.9213,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3377/10000,\n",
      " train_loss: 838.9213,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3378/10000,\n",
      " train_loss: 838.9213,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3379/10000,\n",
      " train_loss: 838.9213,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3380/10000,\n",
      " train_loss: 838.9213,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3381/10000,\n",
      " train_loss: 838.9211,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3382/10000,\n",
      " train_loss: 838.9211,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3383/10000,\n",
      " train_loss: 838.9211,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3384/10000,\n",
      " train_loss: 838.9210,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3385/10000,\n",
      " train_loss: 838.9210,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3386/10000,\n",
      " train_loss: 838.9209,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0076\n",
      "\n",
      "epoch: 3387/10000,\n",
      " train_loss: 838.9208,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0072\n",
      "\n",
      "epoch: 3388/10000,\n",
      " train_loss: 838.9208,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3389/10000,\n",
      " train_loss: 838.9208,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 3390/10000,\n",
      " train_loss: 838.9208,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3391/10000,\n",
      " train_loss: 838.9206,\n",
      " train_mae: 25.3325,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3392/10000,\n",
      " train_loss: 838.9205,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3393/10000,\n",
      " train_loss: 838.9205,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3394/10000,\n",
      " train_loss: 838.9205,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3395/10000,\n",
      " train_loss: 838.9205,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3396/10000,\n",
      " train_loss: 838.9203,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3397/10000,\n",
      " train_loss: 838.9203,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3398/10000,\n",
      " train_loss: 838.9203,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3399/10000,\n",
      " train_loss: 838.9202,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3400/10000,\n",
      " train_loss: 838.9202,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3401/10000,\n",
      " train_loss: 838.9201,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3402/10000,\n",
      " train_loss: 838.9200,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3403/10000,\n",
      " train_loss: 838.9200,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3404/10000,\n",
      " train_loss: 838.9200,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3405/10000,\n",
      " train_loss: 838.9199,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3406/10000,\n",
      " train_loss: 838.9199,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3407/10000,\n",
      " train_loss: 838.9199,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3408/10000,\n",
      " train_loss: 838.9197,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3409/10000,\n",
      " train_loss: 838.9197,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3410/10000,\n",
      " train_loss: 838.9196,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3411/10000,\n",
      " train_loss: 838.9196,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 3412/10000,\n",
      " train_loss: 838.9195,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3413/10000,\n",
      " train_loss: 838.9194,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3414/10000,\n",
      " train_loss: 838.9194,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 3415/10000,\n",
      " train_loss: 838.9193,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3416/10000,\n",
      " train_loss: 838.9193,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 3417/10000,\n",
      " train_loss: 838.9193,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 3418/10000,\n",
      " train_loss: 838.9192,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 3419/10000,\n",
      " train_loss: 838.9192,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0074\n",
      "\n",
      "epoch: 3420/10000,\n",
      " train_loss: 838.9191,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0078\n",
      "\n",
      "epoch: 3421/10000,\n",
      " train_loss: 838.9191,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 3422/10000,\n",
      " train_loss: 838.9190,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 3423/10000,\n",
      " train_loss: 838.9189,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 3424/10000,\n",
      " train_loss: 838.9189,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3425/10000,\n",
      " train_loss: 838.9188,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 3426/10000,\n",
      " train_loss: 838.9188,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3427/10000,\n",
      " train_loss: 838.9187,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3428/10000,\n",
      " train_loss: 838.9186,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3429/10000,\n",
      " train_loss: 838.9186,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3430/10000,\n",
      " train_loss: 838.9186,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3431/10000,\n",
      " train_loss: 838.9185,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3432/10000,\n",
      " train_loss: 838.9185,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3433/10000,\n",
      " train_loss: 838.9184,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3434/10000,\n",
      " train_loss: 838.9184,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3435/10000,\n",
      " train_loss: 838.9183,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3436/10000,\n",
      " train_loss: 838.9183,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3437/10000,\n",
      " train_loss: 838.9182,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3438/10000,\n",
      " train_loss: 838.9182,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3439/10000,\n",
      " train_loss: 838.9182,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3440/10000,\n",
      " train_loss: 838.9181,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3441/10000,\n",
      " train_loss: 838.9180,\n",
      " train_mae: 25.3324,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3442/10000,\n",
      " train_loss: 838.9180,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3443/10000,\n",
      " train_loss: 838.9179,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3444/10000,\n",
      " train_loss: 838.9179,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3445/10000,\n",
      " train_loss: 838.9177,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3446/10000,\n",
      " train_loss: 838.9177,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3447/10000,\n",
      " train_loss: 838.9177,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3448/10000,\n",
      " train_loss: 838.9176,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3449/10000,\n",
      " train_loss: 838.9176,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3450/10000,\n",
      " train_loss: 838.9175,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 3451/10000,\n",
      " train_loss: 838.9174,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0125\n",
      "\n",
      "epoch: 3452/10000,\n",
      " train_loss: 838.9174,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 3453/10000,\n",
      " train_loss: 838.9174,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 3454/10000,\n",
      " train_loss: 838.9174,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0114\n",
      "\n",
      "epoch: 3455/10000,\n",
      " train_loss: 838.9174,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 3456/10000,\n",
      " train_loss: 838.9172,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 3457/10000,\n",
      " train_loss: 838.9172,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 3458/10000,\n",
      " train_loss: 838.9172,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 3459/10000,\n",
      " train_loss: 838.9171,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 3460/10000,\n",
      " train_loss: 838.9170,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3461/10000,\n",
      " train_loss: 838.9171,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3462/10000,\n",
      " train_loss: 838.9169,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3463/10000,\n",
      " train_loss: 838.9169,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3464/10000,\n",
      " train_loss: 838.9169,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3465/10000,\n",
      " train_loss: 838.9167,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3466/10000,\n",
      " train_loss: 838.9167,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3467/10000,\n",
      " train_loss: 838.9166,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3468/10000,\n",
      " train_loss: 838.9166,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3469/10000,\n",
      " train_loss: 838.9166,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 3470/10000,\n",
      " train_loss: 838.9166,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3471/10000,\n",
      " train_loss: 838.9166,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3472/10000,\n",
      " train_loss: 838.9164,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3473/10000,\n",
      " train_loss: 838.9164,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3474/10000,\n",
      " train_loss: 838.9164,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 3475/10000,\n",
      " train_loss: 838.9163,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 3476/10000,\n",
      " train_loss: 838.9163,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 3477/10000,\n",
      " train_loss: 838.9162,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3478/10000,\n",
      " train_loss: 838.9162,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0079\n",
      "\n",
      "epoch: 3479/10000,\n",
      " train_loss: 838.9161,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0121\n",
      "\n",
      "epoch: 3480/10000,\n",
      " train_loss: 838.9161,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3481/10000,\n",
      " train_loss: 838.9160,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3482/10000,\n",
      " train_loss: 838.9160,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3483/10000,\n",
      " train_loss: 838.9160,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3484/10000,\n",
      " train_loss: 838.9160,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3485/10000,\n",
      " train_loss: 838.9158,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3486/10000,\n",
      " train_loss: 838.9157,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3487/10000,\n",
      " train_loss: 838.9156,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3488/10000,\n",
      " train_loss: 838.9156,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3489/10000,\n",
      " train_loss: 838.9156,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3490/10000,\n",
      " train_loss: 838.9156,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 3491/10000,\n",
      " train_loss: 838.9155,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3492/10000,\n",
      " train_loss: 838.9155,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3493/10000,\n",
      " train_loss: 838.9154,\n",
      " train_mae: 25.3323,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3494/10000,\n",
      " train_loss: 838.9154,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0072\n",
      "\n",
      "epoch: 3495/10000,\n",
      " train_loss: 838.9153,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3496/10000,\n",
      " train_loss: 838.9153,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3497/10000,\n",
      " train_loss: 838.9153,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3498/10000,\n",
      " train_loss: 838.9152,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 3499/10000,\n",
      " train_loss: 838.9152,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3500/10000,\n",
      " train_loss: 838.9151,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3501/10000,\n",
      " train_loss: 838.9150,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 3502/10000,\n",
      " train_loss: 838.9150,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 3503/10000,\n",
      " train_loss: 838.9150,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3504/10000,\n",
      " train_loss: 838.9149,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3505/10000,\n",
      " train_loss: 838.9149,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3506/10000,\n",
      " train_loss: 838.9148,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3507/10000,\n",
      " train_loss: 838.9147,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3508/10000,\n",
      " train_loss: 838.9147,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3509/10000,\n",
      " train_loss: 838.9146,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3510/10000,\n",
      " train_loss: 838.9146,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3511/10000,\n",
      " train_loss: 838.9146,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3512/10000,\n",
      " train_loss: 838.9145,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3513/10000,\n",
      " train_loss: 838.9145,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3514/10000,\n",
      " train_loss: 838.9145,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3515/10000,\n",
      " train_loss: 838.9144,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3516/10000,\n",
      " train_loss: 838.9144,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 3517/10000,\n",
      " train_loss: 838.9142,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0085\n",
      "\n",
      "epoch: 3518/10000,\n",
      " train_loss: 838.9142,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0077\n",
      "\n",
      "epoch: 3519/10000,\n",
      " train_loss: 838.9142,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 3520/10000,\n",
      " train_loss: 838.9141,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 3521/10000,\n",
      " train_loss: 838.9141,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 3522/10000,\n",
      " train_loss: 838.9140,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 3523/10000,\n",
      " train_loss: 838.9141,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0073\n",
      "\n",
      "epoch: 3524/10000,\n",
      " train_loss: 838.9140,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3525/10000,\n",
      " train_loss: 838.9139,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3526/10000,\n",
      " train_loss: 838.9138,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3527/10000,\n",
      " train_loss: 838.9138,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3528/10000,\n",
      " train_loss: 838.9138,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3529/10000,\n",
      " train_loss: 838.9137,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3530/10000,\n",
      " train_loss: 838.9136,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3531/10000,\n",
      " train_loss: 838.9136,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3532/10000,\n",
      " train_loss: 838.9135,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3533/10000,\n",
      " train_loss: 838.9135,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3534/10000,\n",
      " train_loss: 838.9135,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3535/10000,\n",
      " train_loss: 838.9135,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3536/10000,\n",
      " train_loss: 838.9134,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3537/10000,\n",
      " train_loss: 838.9133,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3538/10000,\n",
      " train_loss: 838.9133,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3539/10000,\n",
      " train_loss: 838.9133,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3540/10000,\n",
      " train_loss: 838.9132,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3541/10000,\n",
      " train_loss: 838.9132,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3542/10000,\n",
      " train_loss: 838.9131,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3543/10000,\n",
      " train_loss: 838.9131,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 3544/10000,\n",
      " train_loss: 838.9131,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3545/10000,\n",
      " train_loss: 838.9130,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 3546/10000,\n",
      " train_loss: 838.9130,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3547/10000,\n",
      " train_loss: 838.9130,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3548/10000,\n",
      " train_loss: 838.9128,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3549/10000,\n",
      " train_loss: 838.9128,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3550/10000,\n",
      " train_loss: 838.9127,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3551/10000,\n",
      " train_loss: 838.9127,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 3552/10000,\n",
      " train_loss: 838.9127,\n",
      " train_mae: 25.3322,\n",
      " epoch_time_duration: 0.0157\n",
      "\n",
      "epoch: 3553/10000,\n",
      " train_loss: 838.9127,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 3554/10000,\n",
      " train_loss: 838.9127,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3555/10000,\n",
      " train_loss: 838.9125,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 3556/10000,\n",
      " train_loss: 838.9125,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3557/10000,\n",
      " train_loss: 838.9124,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3558/10000,\n",
      " train_loss: 838.9125,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3559/10000,\n",
      " train_loss: 838.9124,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3560/10000,\n",
      " train_loss: 838.9124,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3561/10000,\n",
      " train_loss: 838.9123,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3562/10000,\n",
      " train_loss: 838.9123,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3563/10000,\n",
      " train_loss: 838.9122,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3564/10000,\n",
      " train_loss: 838.9122,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3565/10000,\n",
      " train_loss: 838.9122,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3566/10000,\n",
      " train_loss: 838.9120,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3567/10000,\n",
      " train_loss: 838.9120,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3568/10000,\n",
      " train_loss: 838.9120,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3569/10000,\n",
      " train_loss: 838.9119,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3570/10000,\n",
      " train_loss: 838.9119,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3571/10000,\n",
      " train_loss: 838.9119,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3572/10000,\n",
      " train_loss: 838.9118,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3573/10000,\n",
      " train_loss: 838.9117,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3574/10000,\n",
      " train_loss: 838.9117,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3575/10000,\n",
      " train_loss: 838.9117,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3576/10000,\n",
      " train_loss: 838.9116,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3577/10000,\n",
      " train_loss: 838.9116,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3578/10000,\n",
      " train_loss: 838.9116,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0095\n",
      "\n",
      "epoch: 3579/10000,\n",
      " train_loss: 838.9115,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 3580/10000,\n",
      " train_loss: 838.9115,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 3581/10000,\n",
      " train_loss: 838.9114,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 3582/10000,\n",
      " train_loss: 838.9114,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 3583/10000,\n",
      " train_loss: 838.9113,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3584/10000,\n",
      " train_loss: 838.9113,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 3585/10000,\n",
      " train_loss: 838.9113,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 3586/10000,\n",
      " train_loss: 838.9113,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0112\n",
      "\n",
      "epoch: 3587/10000,\n",
      " train_loss: 838.9112,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 3588/10000,\n",
      " train_loss: 838.9111,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3589/10000,\n",
      " train_loss: 838.9111,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3590/10000,\n",
      " train_loss: 838.9111,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 3591/10000,\n",
      " train_loss: 838.9110,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3592/10000,\n",
      " train_loss: 838.9110,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3593/10000,\n",
      " train_loss: 838.9110,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3594/10000,\n",
      " train_loss: 838.9108,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3595/10000,\n",
      " train_loss: 838.9108,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3596/10000,\n",
      " train_loss: 838.9107,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3597/10000,\n",
      " train_loss: 838.9108,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3598/10000,\n",
      " train_loss: 838.9107,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3599/10000,\n",
      " train_loss: 838.9107,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3600/10000,\n",
      " train_loss: 838.9107,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3601/10000,\n",
      " train_loss: 838.9105,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 3602/10000,\n",
      " train_loss: 838.9106,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3603/10000,\n",
      " train_loss: 838.9105,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3604/10000,\n",
      " train_loss: 838.9105,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3605/10000,\n",
      " train_loss: 838.9104,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3606/10000,\n",
      " train_loss: 838.9103,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3607/10000,\n",
      " train_loss: 838.9103,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3608/10000,\n",
      " train_loss: 838.9103,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3609/10000,\n",
      " train_loss: 838.9102,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3610/10000,\n",
      " train_loss: 838.9102,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3611/10000,\n",
      " train_loss: 838.9102,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3612/10000,\n",
      " train_loss: 838.9101,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3613/10000,\n",
      " train_loss: 838.9101,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3614/10000,\n",
      " train_loss: 838.9101,\n",
      " train_mae: 25.3321,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3615/10000,\n",
      " train_loss: 838.9101,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3616/10000,\n",
      " train_loss: 838.9099,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3617/10000,\n",
      " train_loss: 838.9099,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3618/10000,\n",
      " train_loss: 838.9099,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3619/10000,\n",
      " train_loss: 838.9099,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3620/10000,\n",
      " train_loss: 838.9098,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3621/10000,\n",
      " train_loss: 838.9097,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3622/10000,\n",
      " train_loss: 838.9096,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3623/10000,\n",
      " train_loss: 838.9096,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0166\n",
      "\n",
      "epoch: 3624/10000,\n",
      " train_loss: 838.9096,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 3625/10000,\n",
      " train_loss: 838.9095,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0072\n",
      "\n",
      "epoch: 3626/10000,\n",
      " train_loss: 838.9095,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0084\n",
      "\n",
      "epoch: 3627/10000,\n",
      " train_loss: 838.9095,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0082\n",
      "\n",
      "epoch: 3628/10000,\n",
      " train_loss: 838.9094,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 3629/10000,\n",
      " train_loss: 838.9094,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0072\n",
      "\n",
      "epoch: 3630/10000,\n",
      " train_loss: 838.9094,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 3631/10000,\n",
      " train_loss: 838.9094,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 3632/10000,\n",
      " train_loss: 838.9093,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3633/10000,\n",
      " train_loss: 838.9093,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 3634/10000,\n",
      " train_loss: 838.9092,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3635/10000,\n",
      " train_loss: 838.9092,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3636/10000,\n",
      " train_loss: 838.9092,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 3637/10000,\n",
      " train_loss: 838.9091,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3638/10000,\n",
      " train_loss: 838.9092,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 3639/10000,\n",
      " train_loss: 838.9091,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3640/10000,\n",
      " train_loss: 838.9091,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3641/10000,\n",
      " train_loss: 838.9089,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3642/10000,\n",
      " train_loss: 838.9089,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3643/10000,\n",
      " train_loss: 838.9089,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3644/10000,\n",
      " train_loss: 838.9088,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3645/10000,\n",
      " train_loss: 838.9088,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3646/10000,\n",
      " train_loss: 838.9088,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 3647/10000,\n",
      " train_loss: 838.9086,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3648/10000,\n",
      " train_loss: 838.9086,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3649/10000,\n",
      " train_loss: 838.9086,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0079\n",
      "\n",
      "epoch: 3650/10000,\n",
      " train_loss: 838.9085,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0113\n",
      "\n",
      "epoch: 3651/10000,\n",
      " train_loss: 838.9085,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3652/10000,\n",
      " train_loss: 838.9085,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3653/10000,\n",
      " train_loss: 838.9085,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3654/10000,\n",
      " train_loss: 838.9084,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3655/10000,\n",
      " train_loss: 838.9084,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3656/10000,\n",
      " train_loss: 838.9084,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3657/10000,\n",
      " train_loss: 838.9083,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3658/10000,\n",
      " train_loss: 838.9083,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3659/10000,\n",
      " train_loss: 838.9082,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3660/10000,\n",
      " train_loss: 838.9082,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3661/10000,\n",
      " train_loss: 838.9081,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3662/10000,\n",
      " train_loss: 838.9081,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3663/10000,\n",
      " train_loss: 838.9081,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3664/10000,\n",
      " train_loss: 838.9080,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3665/10000,\n",
      " train_loss: 838.9080,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3666/10000,\n",
      " train_loss: 838.9080,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3667/10000,\n",
      " train_loss: 838.9078,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3668/10000,\n",
      " train_loss: 838.9078,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 3669/10000,\n",
      " train_loss: 838.9078,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 3670/10000,\n",
      " train_loss: 838.9077,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 3671/10000,\n",
      " train_loss: 838.9077,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 3672/10000,\n",
      " train_loss: 838.9077,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3673/10000,\n",
      " train_loss: 838.9077,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3674/10000,\n",
      " train_loss: 838.9076,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3675/10000,\n",
      " train_loss: 838.9076,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 3676/10000,\n",
      " train_loss: 838.9075,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3677/10000,\n",
      " train_loss: 838.9075,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3678/10000,\n",
      " train_loss: 838.9075,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3679/10000,\n",
      " train_loss: 838.9075,\n",
      " train_mae: 25.3320,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3680/10000,\n",
      " train_loss: 838.9074,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 3681/10000,\n",
      " train_loss: 838.9073,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0102\n",
      "\n",
      "epoch: 3682/10000,\n",
      " train_loss: 838.9073,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3683/10000,\n",
      " train_loss: 838.9072,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 3684/10000,\n",
      " train_loss: 838.9072,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3685/10000,\n",
      " train_loss: 838.9072,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3686/10000,\n",
      " train_loss: 838.9072,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3687/10000,\n",
      " train_loss: 838.9071,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 3688/10000,\n",
      " train_loss: 838.9071,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 3689/10000,\n",
      " train_loss: 838.9071,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3690/10000,\n",
      " train_loss: 838.9070,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 3691/10000,\n",
      " train_loss: 838.9069,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 3692/10000,\n",
      " train_loss: 838.9069,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3693/10000,\n",
      " train_loss: 838.9069,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3694/10000,\n",
      " train_loss: 838.9068,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3695/10000,\n",
      " train_loss: 838.9068,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3696/10000,\n",
      " train_loss: 838.9068,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3697/10000,\n",
      " train_loss: 838.9067,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3698/10000,\n",
      " train_loss: 838.9068,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3699/10000,\n",
      " train_loss: 838.9067,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3700/10000,\n",
      " train_loss: 838.9067,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3701/10000,\n",
      " train_loss: 838.9066,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3702/10000,\n",
      " train_loss: 838.9066,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3703/10000,\n",
      " train_loss: 838.9066,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3704/10000,\n",
      " train_loss: 838.9065,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3705/10000,\n",
      " train_loss: 838.9065,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3706/10000,\n",
      " train_loss: 838.9064,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3707/10000,\n",
      " train_loss: 838.9064,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3708/10000,\n",
      " train_loss: 838.9063,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3709/10000,\n",
      " train_loss: 838.9063,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 3710/10000,\n",
      " train_loss: 838.9062,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3711/10000,\n",
      " train_loss: 838.9062,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 3712/10000,\n",
      " train_loss: 838.9062,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3713/10000,\n",
      " train_loss: 838.9062,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 3714/10000,\n",
      " train_loss: 838.9062,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0105\n",
      "\n",
      "epoch: 3715/10000,\n",
      " train_loss: 838.9061,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0074\n",
      "\n",
      "epoch: 3716/10000,\n",
      " train_loss: 838.9060,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 3717/10000,\n",
      " train_loss: 838.9060,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 3718/10000,\n",
      " train_loss: 838.9060,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3719/10000,\n",
      " train_loss: 838.9060,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 3720/10000,\n",
      " train_loss: 838.9059,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 3721/10000,\n",
      " train_loss: 838.9059,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3722/10000,\n",
      " train_loss: 838.9058,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3723/10000,\n",
      " train_loss: 838.9057,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3724/10000,\n",
      " train_loss: 838.9057,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3725/10000,\n",
      " train_loss: 838.9057,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 3726/10000,\n",
      " train_loss: 838.9057,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3727/10000,\n",
      " train_loss: 838.9057,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3728/10000,\n",
      " train_loss: 838.9056,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3729/10000,\n",
      " train_loss: 838.9056,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3730/10000,\n",
      " train_loss: 838.9056,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3731/10000,\n",
      " train_loss: 838.9055,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3732/10000,\n",
      " train_loss: 838.9055,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3733/10000,\n",
      " train_loss: 838.9055,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3734/10000,\n",
      " train_loss: 838.9055,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3735/10000,\n",
      " train_loss: 838.9054,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3736/10000,\n",
      " train_loss: 838.9053,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 3737/10000,\n",
      " train_loss: 838.9054,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3738/10000,\n",
      " train_loss: 838.9053,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3739/10000,\n",
      " train_loss: 838.9053,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3740/10000,\n",
      " train_loss: 838.9052,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3741/10000,\n",
      " train_loss: 838.9052,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3742/10000,\n",
      " train_loss: 838.9052,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3743/10000,\n",
      " train_loss: 838.9052,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3744/10000,\n",
      " train_loss: 838.9050,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3745/10000,\n",
      " train_loss: 838.9050,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0090\n",
      "\n",
      "epoch: 3746/10000,\n",
      " train_loss: 838.9050,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 3747/10000,\n",
      " train_loss: 838.9050,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0090\n",
      "\n",
      "epoch: 3748/10000,\n",
      " train_loss: 838.9049,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0083\n",
      "\n",
      "epoch: 3749/10000,\n",
      " train_loss: 838.9049,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0078\n",
      "\n",
      "epoch: 3750/10000,\n",
      " train_loss: 838.9048,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3751/10000,\n",
      " train_loss: 838.9048,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 3752/10000,\n",
      " train_loss: 838.9048,\n",
      " train_mae: 25.3319,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 3753/10000,\n",
      " train_loss: 838.9047,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3754/10000,\n",
      " train_loss: 838.9047,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3755/10000,\n",
      " train_loss: 838.9047,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 3756/10000,\n",
      " train_loss: 838.9047,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3757/10000,\n",
      " train_loss: 838.9046,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3758/10000,\n",
      " train_loss: 838.9046,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3759/10000,\n",
      " train_loss: 838.9046,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3760/10000,\n",
      " train_loss: 838.9045,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3761/10000,\n",
      " train_loss: 838.9045,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3762/10000,\n",
      " train_loss: 838.9045,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3763/10000,\n",
      " train_loss: 838.9045,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3764/10000,\n",
      " train_loss: 838.9045,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3765/10000,\n",
      " train_loss: 838.9044,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3766/10000,\n",
      " train_loss: 838.9044,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3767/10000,\n",
      " train_loss: 838.9043,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3768/10000,\n",
      " train_loss: 838.9043,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3769/10000,\n",
      " train_loss: 838.9042,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3770/10000,\n",
      " train_loss: 838.9042,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3771/10000,\n",
      " train_loss: 838.9042,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3772/10000,\n",
      " train_loss: 838.9041,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3773/10000,\n",
      " train_loss: 838.9041,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3774/10000,\n",
      " train_loss: 838.9041,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3775/10000,\n",
      " train_loss: 838.9040,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3776/10000,\n",
      " train_loss: 838.9039,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3777/10000,\n",
      " train_loss: 838.9039,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3778/10000,\n",
      " train_loss: 838.9039,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0092\n",
      "\n",
      "epoch: 3779/10000,\n",
      " train_loss: 838.9039,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 3780/10000,\n",
      " train_loss: 838.9037,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3781/10000,\n",
      " train_loss: 838.9037,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3782/10000,\n",
      " train_loss: 838.9037,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3783/10000,\n",
      " train_loss: 838.9037,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3784/10000,\n",
      " train_loss: 838.9037,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3785/10000,\n",
      " train_loss: 838.9037,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3786/10000,\n",
      " train_loss: 838.9037,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3787/10000,\n",
      " train_loss: 838.9036,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3788/10000,\n",
      " train_loss: 838.9036,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3789/10000,\n",
      " train_loss: 838.9036,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3790/10000,\n",
      " train_loss: 838.9035,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3791/10000,\n",
      " train_loss: 838.9034,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3792/10000,\n",
      " train_loss: 838.9034,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3793/10000,\n",
      " train_loss: 838.9034,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3794/10000,\n",
      " train_loss: 838.9033,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3795/10000,\n",
      " train_loss: 838.9033,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3796/10000,\n",
      " train_loss: 838.9033,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 3797/10000,\n",
      " train_loss: 838.9033,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0091\n",
      "\n",
      "epoch: 3798/10000,\n",
      " train_loss: 838.9032,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 3799/10000,\n",
      " train_loss: 838.9032,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 3800/10000,\n",
      " train_loss: 838.9032,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0081\n",
      "\n",
      "epoch: 3801/10000,\n",
      " train_loss: 838.9031,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 3802/10000,\n",
      " train_loss: 838.9031,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0089\n",
      "\n",
      "epoch: 3803/10000,\n",
      " train_loss: 838.9030,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3804/10000,\n",
      " train_loss: 838.9030,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3805/10000,\n",
      " train_loss: 838.9030,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3806/10000,\n",
      " train_loss: 838.9029,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0078\n",
      "\n",
      "epoch: 3807/10000,\n",
      " train_loss: 838.9029,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 3808/10000,\n",
      " train_loss: 838.9029,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 3809/10000,\n",
      " train_loss: 838.9029,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 3810/10000,\n",
      " train_loss: 838.9029,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3811/10000,\n",
      " train_loss: 838.9028,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3812/10000,\n",
      " train_loss: 838.9028,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3813/10000,\n",
      " train_loss: 838.9026,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3814/10000,\n",
      " train_loss: 838.9027,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3815/10000,\n",
      " train_loss: 838.9026,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3816/10000,\n",
      " train_loss: 838.9026,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3817/10000,\n",
      " train_loss: 838.9026,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 3818/10000,\n",
      " train_loss: 838.9025,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3819/10000,\n",
      " train_loss: 838.9025,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 3820/10000,\n",
      " train_loss: 838.9025,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3821/10000,\n",
      " train_loss: 838.9025,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 3822/10000,\n",
      " train_loss: 838.9025,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3823/10000,\n",
      " train_loss: 838.9024,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 3824/10000,\n",
      " train_loss: 838.9023,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3825/10000,\n",
      " train_loss: 838.9023,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3826/10000,\n",
      " train_loss: 838.9023,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3827/10000,\n",
      " train_loss: 838.9023,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 3828/10000,\n",
      " train_loss: 838.9023,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3829/10000,\n",
      " train_loss: 838.9023,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3830/10000,\n",
      " train_loss: 838.9023,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3831/10000,\n",
      " train_loss: 838.9022,\n",
      " train_mae: 25.3318,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3832/10000,\n",
      " train_loss: 838.9021,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3833/10000,\n",
      " train_loss: 838.9020,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3834/10000,\n",
      " train_loss: 838.9020,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3835/10000,\n",
      " train_loss: 838.9020,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3836/10000,\n",
      " train_loss: 838.9020,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3837/10000,\n",
      " train_loss: 838.9019,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3838/10000,\n",
      " train_loss: 838.9020,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3839/10000,\n",
      " train_loss: 838.9018,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3840/10000,\n",
      " train_loss: 838.9018,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0088\n",
      "\n",
      "epoch: 3841/10000,\n",
      " train_loss: 838.9018,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3842/10000,\n",
      " train_loss: 838.9018,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3843/10000,\n",
      " train_loss: 838.9018,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3844/10000,\n",
      " train_loss: 838.9017,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3845/10000,\n",
      " train_loss: 838.9017,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 3846/10000,\n",
      " train_loss: 838.9017,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 3847/10000,\n",
      " train_loss: 838.9016,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0077\n",
      "\n",
      "epoch: 3848/10000,\n",
      " train_loss: 838.9016,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 3849/10000,\n",
      " train_loss: 838.9016,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 3850/10000,\n",
      " train_loss: 838.9016,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 3851/10000,\n",
      " train_loss: 838.9015,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0095\n",
      "\n",
      "epoch: 3852/10000,\n",
      " train_loss: 838.9015,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3853/10000,\n",
      " train_loss: 838.9014,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 3854/10000,\n",
      " train_loss: 838.9015,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 3855/10000,\n",
      " train_loss: 838.9014,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3856/10000,\n",
      " train_loss: 838.9014,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 3857/10000,\n",
      " train_loss: 838.9014,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 3858/10000,\n",
      " train_loss: 838.9013,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 3859/10000,\n",
      " train_loss: 838.9014,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3860/10000,\n",
      " train_loss: 838.9012,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3861/10000,\n",
      " train_loss: 838.9012,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3862/10000,\n",
      " train_loss: 838.9011,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3863/10000,\n",
      " train_loss: 838.9011,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3864/10000,\n",
      " train_loss: 838.9011,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3865/10000,\n",
      " train_loss: 838.9011,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0077\n",
      "\n",
      "epoch: 3866/10000,\n",
      " train_loss: 838.9009,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0087\n",
      "\n",
      "epoch: 3867/10000,\n",
      " train_loss: 838.9009,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 3868/10000,\n",
      " train_loss: 838.9009,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3869/10000,\n",
      " train_loss: 838.9009,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 3870/10000,\n",
      " train_loss: 838.9009,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3871/10000,\n",
      " train_loss: 838.9008,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3872/10000,\n",
      " train_loss: 838.9009,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3873/10000,\n",
      " train_loss: 838.9008,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3874/10000,\n",
      " train_loss: 838.9008,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3875/10000,\n",
      " train_loss: 838.9007,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3876/10000,\n",
      " train_loss: 838.9007,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3877/10000,\n",
      " train_loss: 838.9007,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 3878/10000,\n",
      " train_loss: 838.9006,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3879/10000,\n",
      " train_loss: 838.9006,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3880/10000,\n",
      " train_loss: 838.9006,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3881/10000,\n",
      " train_loss: 838.9006,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3882/10000,\n",
      " train_loss: 838.9006,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 3883/10000,\n",
      " train_loss: 838.9005,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3884/10000,\n",
      " train_loss: 838.9005,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0075\n",
      "\n",
      "epoch: 3885/10000,\n",
      " train_loss: 838.9005,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 3886/10000,\n",
      " train_loss: 838.9004,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 3887/10000,\n",
      " train_loss: 838.9003,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0077\n",
      "\n",
      "epoch: 3888/10000,\n",
      " train_loss: 838.9003,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 3889/10000,\n",
      " train_loss: 838.9003,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3890/10000,\n",
      " train_loss: 838.9003,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 3891/10000,\n",
      " train_loss: 838.9003,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0095\n",
      "\n",
      "epoch: 3892/10000,\n",
      " train_loss: 838.9003,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0134\n",
      "\n",
      "epoch: 3893/10000,\n",
      " train_loss: 838.9001,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 3894/10000,\n",
      " train_loss: 838.9001,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3895/10000,\n",
      " train_loss: 838.9001,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 3896/10000,\n",
      " train_loss: 838.9001,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0075\n",
      "\n",
      "epoch: 3897/10000,\n",
      " train_loss: 838.9000,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3898/10000,\n",
      " train_loss: 838.9000,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3899/10000,\n",
      " train_loss: 838.9000,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 3900/10000,\n",
      " train_loss: 838.8998,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 3901/10000,\n",
      " train_loss: 838.8998,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3902/10000,\n",
      " train_loss: 838.8998,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3903/10000,\n",
      " train_loss: 838.8998,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3904/10000,\n",
      " train_loss: 838.8998,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3905/10000,\n",
      " train_loss: 838.8998,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3906/10000,\n",
      " train_loss: 838.8998,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3907/10000,\n",
      " train_loss: 838.8998,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3908/10000,\n",
      " train_loss: 838.8997,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3909/10000,\n",
      " train_loss: 838.8997,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3910/10000,\n",
      " train_loss: 838.8997,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3911/10000,\n",
      " train_loss: 838.8997,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3912/10000,\n",
      " train_loss: 838.8995,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3913/10000,\n",
      " train_loss: 838.8996,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3914/10000,\n",
      " train_loss: 838.8995,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3915/10000,\n",
      " train_loss: 838.8995,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3916/10000,\n",
      " train_loss: 838.8995,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3917/10000,\n",
      " train_loss: 838.8995,\n",
      " train_mae: 25.3317,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3918/10000,\n",
      " train_loss: 838.8994,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 3919/10000,\n",
      " train_loss: 838.8994,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3920/10000,\n",
      " train_loss: 838.8994,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3921/10000,\n",
      " train_loss: 838.8994,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 3922/10000,\n",
      " train_loss: 838.8993,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 3923/10000,\n",
      " train_loss: 838.8993,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 3924/10000,\n",
      " train_loss: 838.8993,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3925/10000,\n",
      " train_loss: 838.8993,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3926/10000,\n",
      " train_loss: 838.8992,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3927/10000,\n",
      " train_loss: 838.8991,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3928/10000,\n",
      " train_loss: 838.8991,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3929/10000,\n",
      " train_loss: 838.8991,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3930/10000,\n",
      " train_loss: 838.8990,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 3931/10000,\n",
      " train_loss: 838.8990,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 3932/10000,\n",
      " train_loss: 838.8990,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0073\n",
      "\n",
      "epoch: 3933/10000,\n",
      " train_loss: 838.8990,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0079\n",
      "\n",
      "epoch: 3934/10000,\n",
      " train_loss: 838.8989,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 3935/10000,\n",
      " train_loss: 838.8990,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3936/10000,\n",
      " train_loss: 838.8989,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3937/10000,\n",
      " train_loss: 838.8989,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3938/10000,\n",
      " train_loss: 838.8989,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3939/10000,\n",
      " train_loss: 838.8989,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3940/10000,\n",
      " train_loss: 838.8989,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3941/10000,\n",
      " train_loss: 838.8987,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3942/10000,\n",
      " train_loss: 838.8987,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3943/10000,\n",
      " train_loss: 838.8987,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 3944/10000,\n",
      " train_loss: 838.8987,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3945/10000,\n",
      " train_loss: 838.8987,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 3946/10000,\n",
      " train_loss: 838.8986,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3947/10000,\n",
      " train_loss: 838.8986,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3948/10000,\n",
      " train_loss: 838.8986,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3949/10000,\n",
      " train_loss: 838.8985,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 3950/10000,\n",
      " train_loss: 838.8985,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 3951/10000,\n",
      " train_loss: 838.8985,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 3952/10000,\n",
      " train_loss: 838.8985,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3953/10000,\n",
      " train_loss: 838.8984,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 3954/10000,\n",
      " train_loss: 838.8984,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3955/10000,\n",
      " train_loss: 838.8984,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0083\n",
      "\n",
      "epoch: 3956/10000,\n",
      " train_loss: 838.8984,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0097\n",
      "\n",
      "epoch: 3957/10000,\n",
      " train_loss: 838.8984,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 3958/10000,\n",
      " train_loss: 838.8984,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3959/10000,\n",
      " train_loss: 838.8982,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 3960/10000,\n",
      " train_loss: 838.8982,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3961/10000,\n",
      " train_loss: 838.8982,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3962/10000,\n",
      " train_loss: 838.8982,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 3963/10000,\n",
      " train_loss: 838.8981,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 3964/10000,\n",
      " train_loss: 838.8981,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3965/10000,\n",
      " train_loss: 838.8981,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3966/10000,\n",
      " train_loss: 838.8981,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3967/10000,\n",
      " train_loss: 838.8979,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3968/10000,\n",
      " train_loss: 838.8980,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3969/10000,\n",
      " train_loss: 838.8979,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3970/10000,\n",
      " train_loss: 838.8979,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 3971/10000,\n",
      " train_loss: 838.8979,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 3972/10000,\n",
      " train_loss: 838.8978,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 3973/10000,\n",
      " train_loss: 838.8978,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 3974/10000,\n",
      " train_loss: 838.8978,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 3975/10000,\n",
      " train_loss: 838.8978,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 3976/10000,\n",
      " train_loss: 838.8978,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 3977/10000,\n",
      " train_loss: 838.8978,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3978/10000,\n",
      " train_loss: 838.8978,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 3979/10000,\n",
      " train_loss: 838.8977,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 3980/10000,\n",
      " train_loss: 838.8977,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 3981/10000,\n",
      " train_loss: 838.8976,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3982/10000,\n",
      " train_loss: 838.8976,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 3983/10000,\n",
      " train_loss: 838.8976,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 3984/10000,\n",
      " train_loss: 838.8976,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 3985/10000,\n",
      " train_loss: 838.8976,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 3986/10000,\n",
      " train_loss: 838.8975,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 3987/10000,\n",
      " train_loss: 838.8975,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 3988/10000,\n",
      " train_loss: 838.8975,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 3989/10000,\n",
      " train_loss: 838.8975,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0083\n",
      "\n",
      "epoch: 3990/10000,\n",
      " train_loss: 838.8975,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 3991/10000,\n",
      " train_loss: 838.8975,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3992/10000,\n",
      " train_loss: 838.8974,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 3993/10000,\n",
      " train_loss: 838.8973,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 3994/10000,\n",
      " train_loss: 838.8974,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 3995/10000,\n",
      " train_loss: 838.8972,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 3996/10000,\n",
      " train_loss: 838.8973,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 3997/10000,\n",
      " train_loss: 838.8972,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3998/10000,\n",
      " train_loss: 838.8972,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 3999/10000,\n",
      " train_loss: 838.8972,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4000/10000,\n",
      " train_loss: 838.8972,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4001/10000,\n",
      " train_loss: 838.8972,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4002/10000,\n",
      " train_loss: 838.8972,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4003/10000,\n",
      " train_loss: 838.8971,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4004/10000,\n",
      " train_loss: 838.8970,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4005/10000,\n",
      " train_loss: 838.8970,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4006/10000,\n",
      " train_loss: 838.8970,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 4007/10000,\n",
      " train_loss: 838.8969,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4008/10000,\n",
      " train_loss: 838.8969,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4009/10000,\n",
      " train_loss: 838.8969,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4010/10000,\n",
      " train_loss: 838.8969,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4011/10000,\n",
      " train_loss: 838.8969,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4012/10000,\n",
      " train_loss: 838.8969,\n",
      " train_mae: 25.3316,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4013/10000,\n",
      " train_loss: 838.8968,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 4014/10000,\n",
      " train_loss: 838.8968,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4015/10000,\n",
      " train_loss: 838.8968,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4016/10000,\n",
      " train_loss: 838.8967,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4017/10000,\n",
      " train_loss: 838.8967,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 4018/10000,\n",
      " train_loss: 838.8967,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4019/10000,\n",
      " train_loss: 838.8967,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4020/10000,\n",
      " train_loss: 838.8967,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4021/10000,\n",
      " train_loss: 838.8966,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4022/10000,\n",
      " train_loss: 838.8965,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 4023/10000,\n",
      " train_loss: 838.8965,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4024/10000,\n",
      " train_loss: 838.8965,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4025/10000,\n",
      " train_loss: 838.8965,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4026/10000,\n",
      " train_loss: 838.8965,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0091\n",
      "\n",
      "epoch: 4027/10000,\n",
      " train_loss: 838.8965,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0073\n",
      "\n",
      "epoch: 4028/10000,\n",
      " train_loss: 838.8965,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 4029/10000,\n",
      " train_loss: 838.8964,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4030/10000,\n",
      " train_loss: 838.8964,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4031/10000,\n",
      " train_loss: 838.8964,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4032/10000,\n",
      " train_loss: 838.8962,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4033/10000,\n",
      " train_loss: 838.8962,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4034/10000,\n",
      " train_loss: 838.8962,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4035/10000,\n",
      " train_loss: 838.8962,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4036/10000,\n",
      " train_loss: 838.8961,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4037/10000,\n",
      " train_loss: 838.8962,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4038/10000,\n",
      " train_loss: 838.8961,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4039/10000,\n",
      " train_loss: 838.8961,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4040/10000,\n",
      " train_loss: 838.8961,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4041/10000,\n",
      " train_loss: 838.8961,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4042/10000,\n",
      " train_loss: 838.8961,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4043/10000,\n",
      " train_loss: 838.8959,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4044/10000,\n",
      " train_loss: 838.8959,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4045/10000,\n",
      " train_loss: 838.8959,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4046/10000,\n",
      " train_loss: 838.8959,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 4047/10000,\n",
      " train_loss: 838.8959,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4048/10000,\n",
      " train_loss: 838.8959,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 4049/10000,\n",
      " train_loss: 838.8959,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 4050/10000,\n",
      " train_loss: 838.8958,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4051/10000,\n",
      " train_loss: 838.8958,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 4052/10000,\n",
      " train_loss: 838.8958,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 4053/10000,\n",
      " train_loss: 838.8958,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4054/10000,\n",
      " train_loss: 838.8958,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4055/10000,\n",
      " train_loss: 838.8957,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 4056/10000,\n",
      " train_loss: 838.8958,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4057/10000,\n",
      " train_loss: 838.8956,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0084\n",
      "\n",
      "epoch: 4058/10000,\n",
      " train_loss: 838.8956,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0138\n",
      "\n",
      "epoch: 4059/10000,\n",
      " train_loss: 838.8956,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0073\n",
      "\n",
      "epoch: 4060/10000,\n",
      " train_loss: 838.8956,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4061/10000,\n",
      " train_loss: 838.8956,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4062/10000,\n",
      " train_loss: 838.8955,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4063/10000,\n",
      " train_loss: 838.8955,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 4064/10000,\n",
      " train_loss: 838.8955,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 4065/10000,\n",
      " train_loss: 838.8955,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4066/10000,\n",
      " train_loss: 838.8954,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4067/10000,\n",
      " train_loss: 838.8954,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4068/10000,\n",
      " train_loss: 838.8954,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4069/10000,\n",
      " train_loss: 838.8954,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4070/10000,\n",
      " train_loss: 838.8954,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4071/10000,\n",
      " train_loss: 838.8953,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4072/10000,\n",
      " train_loss: 838.8952,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4073/10000,\n",
      " train_loss: 838.8952,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4074/10000,\n",
      " train_loss: 838.8952,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 4075/10000,\n",
      " train_loss: 838.8952,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4076/10000,\n",
      " train_loss: 838.8952,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4077/10000,\n",
      " train_loss: 838.8951,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4078/10000,\n",
      " train_loss: 838.8951,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4079/10000,\n",
      " train_loss: 838.8951,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4080/10000,\n",
      " train_loss: 838.8951,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4081/10000,\n",
      " train_loss: 838.8951,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4082/10000,\n",
      " train_loss: 838.8951,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4083/10000,\n",
      " train_loss: 838.8950,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4084/10000,\n",
      " train_loss: 838.8950,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4085/10000,\n",
      " train_loss: 838.8950,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4086/10000,\n",
      " train_loss: 838.8950,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4087/10000,\n",
      " train_loss: 838.8949,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4088/10000,\n",
      " train_loss: 838.8948,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4089/10000,\n",
      " train_loss: 838.8948,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0081\n",
      "\n",
      "epoch: 4090/10000,\n",
      " train_loss: 838.8948,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0082\n",
      "\n",
      "epoch: 4091/10000,\n",
      " train_loss: 838.8948,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4092/10000,\n",
      " train_loss: 838.8948,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4093/10000,\n",
      " train_loss: 838.8947,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4094/10000,\n",
      " train_loss: 838.8947,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0119\n",
      "\n",
      "epoch: 4095/10000,\n",
      " train_loss: 838.8947,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 4096/10000,\n",
      " train_loss: 838.8947,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4097/10000,\n",
      " train_loss: 838.8947,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 4098/10000,\n",
      " train_loss: 838.8947,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4099/10000,\n",
      " train_loss: 838.8946,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4100/10000,\n",
      " train_loss: 838.8946,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4101/10000,\n",
      " train_loss: 838.8946,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4102/10000,\n",
      " train_loss: 838.8946,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4103/10000,\n",
      " train_loss: 838.8945,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4104/10000,\n",
      " train_loss: 838.8945,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 4105/10000,\n",
      " train_loss: 838.8945,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4106/10000,\n",
      " train_loss: 838.8945,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4107/10000,\n",
      " train_loss: 838.8945,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4108/10000,\n",
      " train_loss: 838.8943,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4109/10000,\n",
      " train_loss: 838.8943,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4110/10000,\n",
      " train_loss: 838.8943,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4111/10000,\n",
      " train_loss: 838.8943,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4112/10000,\n",
      " train_loss: 838.8943,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4113/10000,\n",
      " train_loss: 838.8943,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4114/10000,\n",
      " train_loss: 838.8942,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4115/10000,\n",
      " train_loss: 838.8942,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4116/10000,\n",
      " train_loss: 838.8942,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 4117/10000,\n",
      " train_loss: 838.8942,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0074\n",
      "\n",
      "epoch: 4118/10000,\n",
      " train_loss: 838.8942,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0088\n",
      "\n",
      "epoch: 4119/10000,\n",
      " train_loss: 838.8942,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4120/10000,\n",
      " train_loss: 838.8941,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4121/10000,\n",
      " train_loss: 838.8941,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4122/10000,\n",
      " train_loss: 838.8940,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4123/10000,\n",
      " train_loss: 838.8940,\n",
      " train_mae: 25.3315,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4124/10000,\n",
      " train_loss: 838.8940,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4125/10000,\n",
      " train_loss: 838.8940,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4126/10000,\n",
      " train_loss: 838.8940,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 4127/10000,\n",
      " train_loss: 838.8939,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4128/10000,\n",
      " train_loss: 838.8939,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4129/10000,\n",
      " train_loss: 838.8939,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4130/10000,\n",
      " train_loss: 838.8939,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4131/10000,\n",
      " train_loss: 838.8939,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 4132/10000,\n",
      " train_loss: 838.8939,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 4133/10000,\n",
      " train_loss: 838.8938,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 4134/10000,\n",
      " train_loss: 838.8938,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4135/10000,\n",
      " train_loss: 838.8937,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4136/10000,\n",
      " train_loss: 838.8938,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 4137/10000,\n",
      " train_loss: 838.8938,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4138/10000,\n",
      " train_loss: 838.8937,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4139/10000,\n",
      " train_loss: 838.8937,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4140/10000,\n",
      " train_loss: 838.8937,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4141/10000,\n",
      " train_loss: 838.8937,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4142/10000,\n",
      " train_loss: 838.8937,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4143/10000,\n",
      " train_loss: 838.8936,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4144/10000,\n",
      " train_loss: 838.8936,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4145/10000,\n",
      " train_loss: 838.8936,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4146/10000,\n",
      " train_loss: 838.8936,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4147/10000,\n",
      " train_loss: 838.8935,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4148/10000,\n",
      " train_loss: 838.8936,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4149/10000,\n",
      " train_loss: 838.8936,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0075\n",
      "\n",
      "epoch: 4150/10000,\n",
      " train_loss: 838.8935,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0106\n",
      "\n",
      "epoch: 4151/10000,\n",
      " train_loss: 838.8934,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4152/10000,\n",
      " train_loss: 838.8934,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 4153/10000,\n",
      " train_loss: 838.8934,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4154/10000,\n",
      " train_loss: 838.8933,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4155/10000,\n",
      " train_loss: 838.8934,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 4156/10000,\n",
      " train_loss: 838.8932,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 4157/10000,\n",
      " train_loss: 838.8932,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 4158/10000,\n",
      " train_loss: 838.8932,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4159/10000,\n",
      " train_loss: 838.8932,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4160/10000,\n",
      " train_loss: 838.8932,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4161/10000,\n",
      " train_loss: 838.8931,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4162/10000,\n",
      " train_loss: 838.8931,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4163/10000,\n",
      " train_loss: 838.8931,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4164/10000,\n",
      " train_loss: 838.8931,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 4165/10000,\n",
      " train_loss: 838.8931,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4166/10000,\n",
      " train_loss: 838.8931,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4167/10000,\n",
      " train_loss: 838.8930,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4168/10000,\n",
      " train_loss: 838.8930,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 4169/10000,\n",
      " train_loss: 838.8930,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 4170/10000,\n",
      " train_loss: 838.8930,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 4171/10000,\n",
      " train_loss: 838.8929,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 4172/10000,\n",
      " train_loss: 838.8929,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4173/10000,\n",
      " train_loss: 838.8929,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4174/10000,\n",
      " train_loss: 838.8929,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4175/10000,\n",
      " train_loss: 838.8929,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 4176/10000,\n",
      " train_loss: 838.8928,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4177/10000,\n",
      " train_loss: 838.8928,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0100\n",
      "\n",
      "epoch: 4178/10000,\n",
      " train_loss: 838.8928,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 4179/10000,\n",
      " train_loss: 838.8928,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 4180/10000,\n",
      " train_loss: 838.8928,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 4181/10000,\n",
      " train_loss: 838.8928,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 4182/10000,\n",
      " train_loss: 838.8928,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4183/10000,\n",
      " train_loss: 838.8926,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4184/10000,\n",
      " train_loss: 838.8926,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 4185/10000,\n",
      " train_loss: 838.8926,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 4186/10000,\n",
      " train_loss: 838.8926,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 4187/10000,\n",
      " train_loss: 838.8926,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4188/10000,\n",
      " train_loss: 838.8926,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4189/10000,\n",
      " train_loss: 838.8926,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4190/10000,\n",
      " train_loss: 838.8925,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4191/10000,\n",
      " train_loss: 838.8925,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4192/10000,\n",
      " train_loss: 838.8925,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4193/10000,\n",
      " train_loss: 838.8925,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4194/10000,\n",
      " train_loss: 838.8925,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4195/10000,\n",
      " train_loss: 838.8925,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4196/10000,\n",
      " train_loss: 838.8925,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4197/10000,\n",
      " train_loss: 838.8923,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4198/10000,\n",
      " train_loss: 838.8923,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4199/10000,\n",
      " train_loss: 838.8923,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4200/10000,\n",
      " train_loss: 838.8923,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4201/10000,\n",
      " train_loss: 838.8922,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4202/10000,\n",
      " train_loss: 838.8922,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4203/10000,\n",
      " train_loss: 838.8922,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4204/10000,\n",
      " train_loss: 838.8922,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0096\n",
      "\n",
      "epoch: 4205/10000,\n",
      " train_loss: 838.8922,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4206/10000,\n",
      " train_loss: 838.8922,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0077\n",
      "\n",
      "epoch: 4207/10000,\n",
      " train_loss: 838.8920,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 4208/10000,\n",
      " train_loss: 838.8920,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0075\n",
      "\n",
      "epoch: 4209/10000,\n",
      " train_loss: 838.8920,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 4210/10000,\n",
      " train_loss: 838.8920,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0077\n",
      "\n",
      "epoch: 4211/10000,\n",
      " train_loss: 838.8920,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 4212/10000,\n",
      " train_loss: 838.8920,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0078\n",
      "\n",
      "epoch: 4213/10000,\n",
      " train_loss: 838.8920,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0073\n",
      "\n",
      "epoch: 4214/10000,\n",
      " train_loss: 838.8920,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4215/10000,\n",
      " train_loss: 838.8920,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4216/10000,\n",
      " train_loss: 838.8920,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4217/10000,\n",
      " train_loss: 838.8920,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4218/10000,\n",
      " train_loss: 838.8920,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4219/10000,\n",
      " train_loss: 838.8919,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4220/10000,\n",
      " train_loss: 838.8918,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4221/10000,\n",
      " train_loss: 838.8918,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4222/10000,\n",
      " train_loss: 838.8918,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4223/10000,\n",
      " train_loss: 838.8918,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4224/10000,\n",
      " train_loss: 838.8918,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 4225/10000,\n",
      " train_loss: 838.8917,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4226/10000,\n",
      " train_loss: 838.8917,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4227/10000,\n",
      " train_loss: 838.8917,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4228/10000,\n",
      " train_loss: 838.8917,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 4229/10000,\n",
      " train_loss: 838.8917,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4230/10000,\n",
      " train_loss: 838.8917,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4231/10000,\n",
      " train_loss: 838.8916,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4232/10000,\n",
      " train_loss: 838.8916,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4233/10000,\n",
      " train_loss: 838.8916,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 4234/10000,\n",
      " train_loss: 838.8916,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 4235/10000,\n",
      " train_loss: 838.8915,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 4236/10000,\n",
      " train_loss: 838.8916,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4237/10000,\n",
      " train_loss: 838.8915,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 4238/10000,\n",
      " train_loss: 838.8915,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 4239/10000,\n",
      " train_loss: 838.8915,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4240/10000,\n",
      " train_loss: 838.8915,\n",
      " train_mae: 25.3314,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4241/10000,\n",
      " train_loss: 838.8915,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 4242/10000,\n",
      " train_loss: 838.8914,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 4243/10000,\n",
      " train_loss: 838.8915,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4244/10000,\n",
      " train_loss: 838.8914,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4245/10000,\n",
      " train_loss: 838.8913,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 4246/10000,\n",
      " train_loss: 838.8913,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0137\n",
      "\n",
      "epoch: 4247/10000,\n",
      " train_loss: 838.8913,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 4248/10000,\n",
      " train_loss: 838.8913,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 4249/10000,\n",
      " train_loss: 838.8912,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 4250/10000,\n",
      " train_loss: 838.8912,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 4251/10000,\n",
      " train_loss: 838.8912,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4252/10000,\n",
      " train_loss: 838.8912,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4253/10000,\n",
      " train_loss: 838.8912,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4254/10000,\n",
      " train_loss: 838.8912,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 4255/10000,\n",
      " train_loss: 838.8912,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 4256/10000,\n",
      " train_loss: 838.8912,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4257/10000,\n",
      " train_loss: 838.8912,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 4258/10000,\n",
      " train_loss: 838.8911,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4259/10000,\n",
      " train_loss: 838.8911,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 4260/10000,\n",
      " train_loss: 838.8911,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 4261/10000,\n",
      " train_loss: 838.8911,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 4262/10000,\n",
      " train_loss: 838.8910,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 4263/10000,\n",
      " train_loss: 838.8910,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 4264/10000,\n",
      " train_loss: 838.8909,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4265/10000,\n",
      " train_loss: 838.8909,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4266/10000,\n",
      " train_loss: 838.8909,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4267/10000,\n",
      " train_loss: 838.8909,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4268/10000,\n",
      " train_loss: 838.8909,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 4269/10000,\n",
      " train_loss: 838.8909,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0144\n",
      "\n",
      "epoch: 4270/10000,\n",
      " train_loss: 838.8908,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4271/10000,\n",
      " train_loss: 838.8908,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 4272/10000,\n",
      " train_loss: 838.8909,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4273/10000,\n",
      " train_loss: 838.8908,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4274/10000,\n",
      " train_loss: 838.8907,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4275/10000,\n",
      " train_loss: 838.8907,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4276/10000,\n",
      " train_loss: 838.8907,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4277/10000,\n",
      " train_loss: 838.8907,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4278/10000,\n",
      " train_loss: 838.8907,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4279/10000,\n",
      " train_loss: 838.8907,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4280/10000,\n",
      " train_loss: 838.8907,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0075\n",
      "\n",
      "epoch: 4281/10000,\n",
      " train_loss: 838.8907,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 4282/10000,\n",
      " train_loss: 838.8906,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0072\n",
      "\n",
      "epoch: 4283/10000,\n",
      " train_loss: 838.8906,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4284/10000,\n",
      " train_loss: 838.8906,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 4285/10000,\n",
      " train_loss: 838.8906,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4286/10000,\n",
      " train_loss: 838.8906,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4287/10000,\n",
      " train_loss: 838.8906,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 4288/10000,\n",
      " train_loss: 838.8906,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4289/10000,\n",
      " train_loss: 838.8904,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4290/10000,\n",
      " train_loss: 838.8904,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4291/10000,\n",
      " train_loss: 838.8904,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4292/10000,\n",
      " train_loss: 838.8904,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4293/10000,\n",
      " train_loss: 838.8903,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4294/10000,\n",
      " train_loss: 838.8903,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4295/10000,\n",
      " train_loss: 838.8903,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4296/10000,\n",
      " train_loss: 838.8903,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4297/10000,\n",
      " train_loss: 838.8903,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4298/10000,\n",
      " train_loss: 838.8903,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4299/10000,\n",
      " train_loss: 838.8902,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4300/10000,\n",
      " train_loss: 838.8902,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0080\n",
      "\n",
      "epoch: 4301/10000,\n",
      " train_loss: 838.8902,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0115\n",
      "\n",
      "epoch: 4302/10000,\n",
      " train_loss: 838.8901,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 4303/10000,\n",
      " train_loss: 838.8901,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 4304/10000,\n",
      " train_loss: 838.8901,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4305/10000,\n",
      " train_loss: 838.8901,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 4306/10000,\n",
      " train_loss: 838.8901,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0072\n",
      "\n",
      "epoch: 4307/10000,\n",
      " train_loss: 838.8901,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 4308/10000,\n",
      " train_loss: 838.8900,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 4309/10000,\n",
      " train_loss: 838.8900,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0072\n",
      "\n",
      "epoch: 4310/10000,\n",
      " train_loss: 838.8900,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4311/10000,\n",
      " train_loss: 838.8900,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4312/10000,\n",
      " train_loss: 838.8900,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4313/10000,\n",
      " train_loss: 838.8900,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4314/10000,\n",
      " train_loss: 838.8899,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4315/10000,\n",
      " train_loss: 838.8899,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4316/10000,\n",
      " train_loss: 838.8899,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4317/10000,\n",
      " train_loss: 838.8899,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4318/10000,\n",
      " train_loss: 838.8899,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4319/10000,\n",
      " train_loss: 838.8898,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4320/10000,\n",
      " train_loss: 838.8898,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4321/10000,\n",
      " train_loss: 838.8898,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4322/10000,\n",
      " train_loss: 838.8898,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4323/10000,\n",
      " train_loss: 838.8898,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4324/10000,\n",
      " train_loss: 838.8897,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4325/10000,\n",
      " train_loss: 838.8897,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 4326/10000,\n",
      " train_loss: 838.8897,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0089\n",
      "\n",
      "epoch: 4327/10000,\n",
      " train_loss: 838.8897,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 4328/10000,\n",
      " train_loss: 838.8896,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 4329/10000,\n",
      " train_loss: 838.8896,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4330/10000,\n",
      " train_loss: 838.8896,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4331/10000,\n",
      " train_loss: 838.8896,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4332/10000,\n",
      " train_loss: 838.8896,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4333/10000,\n",
      " train_loss: 838.8896,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4334/10000,\n",
      " train_loss: 838.8895,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0079\n",
      "\n",
      "epoch: 4335/10000,\n",
      " train_loss: 838.8895,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0123\n",
      "\n",
      "epoch: 4336/10000,\n",
      " train_loss: 838.8895,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4337/10000,\n",
      " train_loss: 838.8895,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4338/10000,\n",
      " train_loss: 838.8893,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4339/10000,\n",
      " train_loss: 838.8893,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4340/10000,\n",
      " train_loss: 838.8893,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4341/10000,\n",
      " train_loss: 838.8893,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4342/10000,\n",
      " train_loss: 838.8893,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4343/10000,\n",
      " train_loss: 838.8893,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4344/10000,\n",
      " train_loss: 838.8893,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 4345/10000,\n",
      " train_loss: 838.8893,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 4346/10000,\n",
      " train_loss: 838.8893,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4347/10000,\n",
      " train_loss: 838.8893,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4348/10000,\n",
      " train_loss: 838.8893,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4349/10000,\n",
      " train_loss: 838.8892,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4350/10000,\n",
      " train_loss: 838.8892,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4351/10000,\n",
      " train_loss: 838.8892,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4352/10000,\n",
      " train_loss: 838.8892,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4353/10000,\n",
      " train_loss: 838.8892,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 4354/10000,\n",
      " train_loss: 838.8891,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4355/10000,\n",
      " train_loss: 838.8891,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0073\n",
      "\n",
      "epoch: 4356/10000,\n",
      " train_loss: 838.8891,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0081\n",
      "\n",
      "epoch: 4357/10000,\n",
      " train_loss: 838.8891,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 4358/10000,\n",
      " train_loss: 838.8890,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 4359/10000,\n",
      " train_loss: 838.8890,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 4360/10000,\n",
      " train_loss: 838.8890,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 4361/10000,\n",
      " train_loss: 838.8890,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 4362/10000,\n",
      " train_loss: 838.8890,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4363/10000,\n",
      " train_loss: 838.8890,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 4364/10000,\n",
      " train_loss: 838.8890,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 4365/10000,\n",
      " train_loss: 838.8890,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0074\n",
      "\n",
      "epoch: 4366/10000,\n",
      " train_loss: 838.8889,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0124\n",
      "\n",
      "epoch: 4367/10000,\n",
      " train_loss: 838.8889,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4368/10000,\n",
      " train_loss: 838.8889,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4369/10000,\n",
      " train_loss: 838.8889,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4370/10000,\n",
      " train_loss: 838.8889,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4371/10000,\n",
      " train_loss: 838.8889,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4372/10000,\n",
      " train_loss: 838.8889,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4373/10000,\n",
      " train_loss: 838.8889,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4374/10000,\n",
      " train_loss: 838.8889,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4375/10000,\n",
      " train_loss: 838.8888,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4376/10000,\n",
      " train_loss: 838.8887,\n",
      " train_mae: 25.3313,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4377/10000,\n",
      " train_loss: 838.8887,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4378/10000,\n",
      " train_loss: 838.8887,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4379/10000,\n",
      " train_loss: 838.8887,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4380/10000,\n",
      " train_loss: 838.8887,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4381/10000,\n",
      " train_loss: 838.8887,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4382/10000,\n",
      " train_loss: 838.8887,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4383/10000,\n",
      " train_loss: 838.8887,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4384/10000,\n",
      " train_loss: 838.8886,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4385/10000,\n",
      " train_loss: 838.8886,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4386/10000,\n",
      " train_loss: 838.8886,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4387/10000,\n",
      " train_loss: 838.8886,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 4388/10000,\n",
      " train_loss: 838.8885,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 4389/10000,\n",
      " train_loss: 838.8886,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 4390/10000,\n",
      " train_loss: 838.8885,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4391/10000,\n",
      " train_loss: 838.8884,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4392/10000,\n",
      " train_loss: 838.8884,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4393/10000,\n",
      " train_loss: 838.8885,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4394/10000,\n",
      " train_loss: 838.8884,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 4395/10000,\n",
      " train_loss: 838.8884,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4396/10000,\n",
      " train_loss: 838.8884,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4397/10000,\n",
      " train_loss: 838.8884,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4398/10000,\n",
      " train_loss: 838.8883,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0133\n",
      "\n",
      "epoch: 4399/10000,\n",
      " train_loss: 838.8884,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0134\n",
      "\n",
      "epoch: 4400/10000,\n",
      " train_loss: 838.8883,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 4401/10000,\n",
      " train_loss: 838.8883,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0082\n",
      "\n",
      "epoch: 4402/10000,\n",
      " train_loss: 838.8883,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0113\n",
      "\n",
      "epoch: 4403/10000,\n",
      " train_loss: 838.8883,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0086\n",
      "\n",
      "epoch: 4404/10000,\n",
      " train_loss: 838.8883,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 4405/10000,\n",
      " train_loss: 838.8881,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0095\n",
      "\n",
      "epoch: 4406/10000,\n",
      " train_loss: 838.8883,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 4407/10000,\n",
      " train_loss: 838.8881,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 4408/10000,\n",
      " train_loss: 838.8881,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 4409/10000,\n",
      " train_loss: 838.8881,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4410/10000,\n",
      " train_loss: 838.8881,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 4411/10000,\n",
      " train_loss: 838.8881,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4412/10000,\n",
      " train_loss: 838.8881,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4413/10000,\n",
      " train_loss: 838.8881,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4414/10000,\n",
      " train_loss: 838.8881,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4415/10000,\n",
      " train_loss: 838.8881,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 4416/10000,\n",
      " train_loss: 838.8881,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4417/10000,\n",
      " train_loss: 838.8881,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4418/10000,\n",
      " train_loss: 838.8880,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4419/10000,\n",
      " train_loss: 838.8880,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 4420/10000,\n",
      " train_loss: 838.8879,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4421/10000,\n",
      " train_loss: 838.8879,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4422/10000,\n",
      " train_loss: 838.8879,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0115\n",
      "\n",
      "epoch: 4423/10000,\n",
      " train_loss: 838.8879,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0094\n",
      "\n",
      "epoch: 4424/10000,\n",
      " train_loss: 838.8879,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0078\n",
      "\n",
      "epoch: 4425/10000,\n",
      " train_loss: 838.8879,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 4426/10000,\n",
      " train_loss: 838.8879,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4427/10000,\n",
      " train_loss: 838.8878,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4428/10000,\n",
      " train_loss: 838.8878,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4429/10000,\n",
      " train_loss: 838.8878,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4430/10000,\n",
      " train_loss: 838.8878,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4431/10000,\n",
      " train_loss: 838.8878,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4432/10000,\n",
      " train_loss: 838.8878,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4433/10000,\n",
      " train_loss: 838.8878,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4434/10000,\n",
      " train_loss: 838.8878,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 4435/10000,\n",
      " train_loss: 838.8877,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4436/10000,\n",
      " train_loss: 838.8877,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4437/10000,\n",
      " train_loss: 838.8877,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4438/10000,\n",
      " train_loss: 838.8877,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4439/10000,\n",
      " train_loss: 838.8877,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4440/10000,\n",
      " train_loss: 838.8877,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4441/10000,\n",
      " train_loss: 838.8876,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4442/10000,\n",
      " train_loss: 838.8876,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4443/10000,\n",
      " train_loss: 838.8876,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4444/10000,\n",
      " train_loss: 838.8876,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4445/10000,\n",
      " train_loss: 838.8876,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4446/10000,\n",
      " train_loss: 838.8875,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4447/10000,\n",
      " train_loss: 838.8875,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 4448/10000,\n",
      " train_loss: 838.8876,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 4449/10000,\n",
      " train_loss: 838.8875,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0102\n",
      "\n",
      "epoch: 4450/10000,\n",
      " train_loss: 838.8874,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4451/10000,\n",
      " train_loss: 838.8874,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 4452/10000,\n",
      " train_loss: 838.8874,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0087\n",
      "\n",
      "epoch: 4453/10000,\n",
      " train_loss: 838.8874,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0080\n",
      "\n",
      "epoch: 4454/10000,\n",
      " train_loss: 838.8874,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0074\n",
      "\n",
      "epoch: 4455/10000,\n",
      " train_loss: 838.8874,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4456/10000,\n",
      " train_loss: 838.8873,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 4457/10000,\n",
      " train_loss: 838.8873,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0078\n",
      "\n",
      "epoch: 4458/10000,\n",
      " train_loss: 838.8873,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 4459/10000,\n",
      " train_loss: 838.8873,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 4460/10000,\n",
      " train_loss: 838.8873,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4461/10000,\n",
      " train_loss: 838.8873,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4462/10000,\n",
      " train_loss: 838.8873,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4463/10000,\n",
      " train_loss: 838.8873,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4464/10000,\n",
      " train_loss: 838.8873,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4465/10000,\n",
      " train_loss: 838.8873,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4466/10000,\n",
      " train_loss: 838.8873,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4467/10000,\n",
      " train_loss: 838.8871,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4468/10000,\n",
      " train_loss: 838.8871,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4469/10000,\n",
      " train_loss: 838.8871,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4470/10000,\n",
      " train_loss: 838.8871,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4471/10000,\n",
      " train_loss: 838.8871,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4472/10000,\n",
      " train_loss: 838.8870,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4473/10000,\n",
      " train_loss: 838.8871,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4474/10000,\n",
      " train_loss: 838.8870,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4475/10000,\n",
      " train_loss: 838.8870,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4476/10000,\n",
      " train_loss: 838.8870,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4477/10000,\n",
      " train_loss: 838.8870,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4478/10000,\n",
      " train_loss: 838.8870,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4479/10000,\n",
      " train_loss: 838.8870,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4480/10000,\n",
      " train_loss: 838.8870,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4481/10000,\n",
      " train_loss: 838.8869,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4482/10000,\n",
      " train_loss: 838.8869,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4483/10000,\n",
      " train_loss: 838.8869,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4484/10000,\n",
      " train_loss: 838.8869,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4485/10000,\n",
      " train_loss: 838.8869,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 4486/10000,\n",
      " train_loss: 838.8868,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0118\n",
      "\n",
      "epoch: 4487/10000,\n",
      " train_loss: 838.8869,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0149\n",
      "\n",
      "epoch: 4488/10000,\n",
      " train_loss: 838.8868,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 4489/10000,\n",
      " train_loss: 838.8868,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 4490/10000,\n",
      " train_loss: 838.8868,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 4491/10000,\n",
      " train_loss: 838.8868,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4492/10000,\n",
      " train_loss: 838.8868,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4493/10000,\n",
      " train_loss: 838.8868,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4494/10000,\n",
      " train_loss: 838.8867,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4495/10000,\n",
      " train_loss: 838.8867,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4496/10000,\n",
      " train_loss: 838.8867,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4497/10000,\n",
      " train_loss: 838.8867,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4498/10000,\n",
      " train_loss: 838.8867,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4499/10000,\n",
      " train_loss: 838.8867,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4500/10000,\n",
      " train_loss: 838.8867,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4501/10000,\n",
      " train_loss: 838.8867,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4502/10000,\n",
      " train_loss: 838.8867,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4503/10000,\n",
      " train_loss: 838.8867,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4504/10000,\n",
      " train_loss: 838.8866,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4505/10000,\n",
      " train_loss: 838.8865,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4506/10000,\n",
      " train_loss: 838.8865,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4507/10000,\n",
      " train_loss: 838.8865,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4508/10000,\n",
      " train_loss: 838.8865,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4509/10000,\n",
      " train_loss: 838.8865,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4510/10000,\n",
      " train_loss: 838.8865,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4511/10000,\n",
      " train_loss: 838.8864,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4512/10000,\n",
      " train_loss: 838.8864,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4513/10000,\n",
      " train_loss: 838.8864,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4514/10000,\n",
      " train_loss: 838.8864,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4515/10000,\n",
      " train_loss: 838.8864,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4516/10000,\n",
      " train_loss: 838.8864,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4517/10000,\n",
      " train_loss: 838.8864,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0102\n",
      "\n",
      "epoch: 4518/10000,\n",
      " train_loss: 838.8864,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0120\n",
      "\n",
      "epoch: 4519/10000,\n",
      " train_loss: 838.8863,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 4520/10000,\n",
      " train_loss: 838.8862,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 4521/10000,\n",
      " train_loss: 838.8863,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4522/10000,\n",
      " train_loss: 838.8863,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4523/10000,\n",
      " train_loss: 838.8862,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 4524/10000,\n",
      " train_loss: 838.8862,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4525/10000,\n",
      " train_loss: 838.8862,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4526/10000,\n",
      " train_loss: 838.8862,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4527/10000,\n",
      " train_loss: 838.8862,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4528/10000,\n",
      " train_loss: 838.8861,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4529/10000,\n",
      " train_loss: 838.8862,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4530/10000,\n",
      " train_loss: 838.8861,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4531/10000,\n",
      " train_loss: 838.8861,\n",
      " train_mae: 25.3312,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 4532/10000,\n",
      " train_loss: 838.8861,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4533/10000,\n",
      " train_loss: 838.8861,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 4534/10000,\n",
      " train_loss: 838.8861,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4535/10000,\n",
      " train_loss: 838.8860,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4536/10000,\n",
      " train_loss: 838.8860,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4537/10000,\n",
      " train_loss: 838.8860,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4538/10000,\n",
      " train_loss: 838.8860,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4539/10000,\n",
      " train_loss: 838.8860,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 4540/10000,\n",
      " train_loss: 838.8860,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 4541/10000,\n",
      " train_loss: 838.8859,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4542/10000,\n",
      " train_loss: 838.8859,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4543/10000,\n",
      " train_loss: 838.8860,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 4544/10000,\n",
      " train_loss: 838.8859,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 4545/10000,\n",
      " train_loss: 838.8859,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4546/10000,\n",
      " train_loss: 838.8859,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0074\n",
      "\n",
      "epoch: 4547/10000,\n",
      " train_loss: 838.8859,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0103\n",
      "\n",
      "epoch: 4548/10000,\n",
      " train_loss: 838.8859,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4549/10000,\n",
      " train_loss: 838.8858,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 4550/10000,\n",
      " train_loss: 838.8859,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4551/10000,\n",
      " train_loss: 838.8859,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4552/10000,\n",
      " train_loss: 838.8858,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4553/10000,\n",
      " train_loss: 838.8858,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4554/10000,\n",
      " train_loss: 838.8857,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 4555/10000,\n",
      " train_loss: 838.8857,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 4556/10000,\n",
      " train_loss: 838.8858,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4557/10000,\n",
      " train_loss: 838.8857,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 4558/10000,\n",
      " train_loss: 838.8857,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 4559/10000,\n",
      " train_loss: 838.8857,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4560/10000,\n",
      " train_loss: 838.8857,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 4561/10000,\n",
      " train_loss: 838.8857,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4562/10000,\n",
      " train_loss: 838.8856,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4563/10000,\n",
      " train_loss: 838.8856,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4564/10000,\n",
      " train_loss: 838.8857,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4565/10000,\n",
      " train_loss: 838.8856,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4566/10000,\n",
      " train_loss: 838.8856,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4567/10000,\n",
      " train_loss: 838.8855,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4568/10000,\n",
      " train_loss: 838.8856,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4569/10000,\n",
      " train_loss: 838.8855,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4570/10000,\n",
      " train_loss: 838.8854,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4571/10000,\n",
      " train_loss: 838.8855,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0116\n",
      "\n",
      "epoch: 4572/10000,\n",
      " train_loss: 838.8854,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4573/10000,\n",
      " train_loss: 838.8854,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4574/10000,\n",
      " train_loss: 838.8854,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4575/10000,\n",
      " train_loss: 838.8854,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4576/10000,\n",
      " train_loss: 838.8854,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4577/10000,\n",
      " train_loss: 838.8854,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 4578/10000,\n",
      " train_loss: 838.8854,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4579/10000,\n",
      " train_loss: 838.8853,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4580/10000,\n",
      " train_loss: 838.8853,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4581/10000,\n",
      " train_loss: 838.8853,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4582/10000,\n",
      " train_loss: 838.8853,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4583/10000,\n",
      " train_loss: 838.8853,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 4584/10000,\n",
      " train_loss: 838.8853,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4585/10000,\n",
      " train_loss: 838.8853,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 4586/10000,\n",
      " train_loss: 838.8853,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4587/10000,\n",
      " train_loss: 838.8852,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4588/10000,\n",
      " train_loss: 838.8853,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 4589/10000,\n",
      " train_loss: 838.8852,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4590/10000,\n",
      " train_loss: 838.8852,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4591/10000,\n",
      " train_loss: 838.8852,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4592/10000,\n",
      " train_loss: 838.8852,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4593/10000,\n",
      " train_loss: 838.8851,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4594/10000,\n",
      " train_loss: 838.8851,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4595/10000,\n",
      " train_loss: 838.8851,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4596/10000,\n",
      " train_loss: 838.8851,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4597/10000,\n",
      " train_loss: 838.8851,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4598/10000,\n",
      " train_loss: 838.8851,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 4599/10000,\n",
      " train_loss: 838.8851,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4600/10000,\n",
      " train_loss: 838.8851,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4601/10000,\n",
      " train_loss: 838.8850,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4602/10000,\n",
      " train_loss: 838.8850,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4603/10000,\n",
      " train_loss: 838.8849,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 4604/10000,\n",
      " train_loss: 838.8849,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0085\n",
      "\n",
      "epoch: 4605/10000,\n",
      " train_loss: 838.8849,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 4606/10000,\n",
      " train_loss: 838.8849,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4607/10000,\n",
      " train_loss: 838.8849,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 4608/10000,\n",
      " train_loss: 838.8849,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4609/10000,\n",
      " train_loss: 838.8849,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4610/10000,\n",
      " train_loss: 838.8849,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4611/10000,\n",
      " train_loss: 838.8849,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4612/10000,\n",
      " train_loss: 838.8848,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4613/10000,\n",
      " train_loss: 838.8848,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4614/10000,\n",
      " train_loss: 838.8848,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4615/10000,\n",
      " train_loss: 838.8848,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4616/10000,\n",
      " train_loss: 838.8848,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4617/10000,\n",
      " train_loss: 838.8848,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4618/10000,\n",
      " train_loss: 838.8848,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4619/10000,\n",
      " train_loss: 838.8848,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4620/10000,\n",
      " train_loss: 838.8848,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4621/10000,\n",
      " train_loss: 838.8847,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4622/10000,\n",
      " train_loss: 838.8847,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4623/10000,\n",
      " train_loss: 838.8847,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4624/10000,\n",
      " train_loss: 838.8847,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4625/10000,\n",
      " train_loss: 838.8847,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4626/10000,\n",
      " train_loss: 838.8847,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4627/10000,\n",
      " train_loss: 838.8846,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4628/10000,\n",
      " train_loss: 838.8846,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4629/10000,\n",
      " train_loss: 838.8846,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4630/10000,\n",
      " train_loss: 838.8845,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4631/10000,\n",
      " train_loss: 838.8846,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4632/10000,\n",
      " train_loss: 838.8845,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 4633/10000,\n",
      " train_loss: 838.8845,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4634/10000,\n",
      " train_loss: 838.8845,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4635/10000,\n",
      " train_loss: 838.8845,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0096\n",
      "\n",
      "epoch: 4636/10000,\n",
      " train_loss: 838.8845,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 4637/10000,\n",
      " train_loss: 838.8845,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0081\n",
      "\n",
      "epoch: 4638/10000,\n",
      " train_loss: 838.8845,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 4639/10000,\n",
      " train_loss: 838.8845,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4640/10000,\n",
      " train_loss: 838.8844,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 4641/10000,\n",
      " train_loss: 838.8844,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0076\n",
      "\n",
      "epoch: 4642/10000,\n",
      " train_loss: 838.8844,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0074\n",
      "\n",
      "epoch: 4643/10000,\n",
      " train_loss: 838.8844,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0088\n",
      "\n",
      "epoch: 4644/10000,\n",
      " train_loss: 838.8844,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4645/10000,\n",
      " train_loss: 838.8843,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4646/10000,\n",
      " train_loss: 838.8843,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4647/10000,\n",
      " train_loss: 838.8844,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 4648/10000,\n",
      " train_loss: 838.8842,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4649/10000,\n",
      " train_loss: 838.8842,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4650/10000,\n",
      " train_loss: 838.8842,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4651/10000,\n",
      " train_loss: 838.8842,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4652/10000,\n",
      " train_loss: 838.8842,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4653/10000,\n",
      " train_loss: 838.8842,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4654/10000,\n",
      " train_loss: 838.8842,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4655/10000,\n",
      " train_loss: 838.8842,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4656/10000,\n",
      " train_loss: 838.8842,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4657/10000,\n",
      " train_loss: 838.8842,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4658/10000,\n",
      " train_loss: 838.8842,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4659/10000,\n",
      " train_loss: 838.8842,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4660/10000,\n",
      " train_loss: 838.8842,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4661/10000,\n",
      " train_loss: 838.8842,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4662/10000,\n",
      " train_loss: 838.8841,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4663/10000,\n",
      " train_loss: 838.8842,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0079\n",
      "\n",
      "epoch: 4664/10000,\n",
      " train_loss: 838.8842,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 4665/10000,\n",
      " train_loss: 838.8841,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0095\n",
      "\n",
      "epoch: 4666/10000,\n",
      " train_loss: 838.8840,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4667/10000,\n",
      " train_loss: 838.8841,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4668/10000,\n",
      " train_loss: 838.8840,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4669/10000,\n",
      " train_loss: 838.8840,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4670/10000,\n",
      " train_loss: 838.8840,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 4671/10000,\n",
      " train_loss: 838.8840,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4672/10000,\n",
      " train_loss: 838.8840,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4673/10000,\n",
      " train_loss: 838.8840,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4674/10000,\n",
      " train_loss: 838.8840,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4675/10000,\n",
      " train_loss: 838.8840,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 4676/10000,\n",
      " train_loss: 838.8840,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4677/10000,\n",
      " train_loss: 838.8840,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4678/10000,\n",
      " train_loss: 838.8839,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4679/10000,\n",
      " train_loss: 838.8839,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4680/10000,\n",
      " train_loss: 838.8839,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4681/10000,\n",
      " train_loss: 838.8839,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4682/10000,\n",
      " train_loss: 838.8839,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4683/10000,\n",
      " train_loss: 838.8839,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4684/10000,\n",
      " train_loss: 838.8839,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4685/10000,\n",
      " train_loss: 838.8839,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4686/10000,\n",
      " train_loss: 838.8838,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4687/10000,\n",
      " train_loss: 838.8838,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 4688/10000,\n",
      " train_loss: 838.8838,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4689/10000,\n",
      " train_loss: 838.8838,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4690/10000,\n",
      " train_loss: 838.8838,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4691/10000,\n",
      " train_loss: 838.8837,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0090\n",
      "\n",
      "epoch: 4692/10000,\n",
      " train_loss: 838.8837,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4693/10000,\n",
      " train_loss: 838.8837,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4694/10000,\n",
      " train_loss: 838.8837,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4695/10000,\n",
      " train_loss: 838.8837,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4696/10000,\n",
      " train_loss: 838.8837,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4697/10000,\n",
      " train_loss: 838.8837,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4698/10000,\n",
      " train_loss: 838.8837,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4699/10000,\n",
      " train_loss: 838.8835,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4700/10000,\n",
      " train_loss: 838.8835,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4701/10000,\n",
      " train_loss: 838.8835,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4702/10000,\n",
      " train_loss: 838.8835,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4703/10000,\n",
      " train_loss: 838.8835,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4704/10000,\n",
      " train_loss: 838.8835,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 4705/10000,\n",
      " train_loss: 838.8835,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0076\n",
      "\n",
      "epoch: 4706/10000,\n",
      " train_loss: 838.8835,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 4707/10000,\n",
      " train_loss: 838.8835,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 4708/10000,\n",
      " train_loss: 838.8834,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 4709/10000,\n",
      " train_loss: 838.8835,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4710/10000,\n",
      " train_loss: 838.8835,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4711/10000,\n",
      " train_loss: 838.8834,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4712/10000,\n",
      " train_loss: 838.8834,\n",
      " train_mae: 25.3311,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4713/10000,\n",
      " train_loss: 838.8834,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 4714/10000,\n",
      " train_loss: 838.8834,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4715/10000,\n",
      " train_loss: 838.8834,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4716/10000,\n",
      " train_loss: 838.8834,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4717/10000,\n",
      " train_loss: 838.8834,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4718/10000,\n",
      " train_loss: 838.8834,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 4719/10000,\n",
      " train_loss: 838.8834,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4720/10000,\n",
      " train_loss: 838.8834,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4721/10000,\n",
      " train_loss: 838.8832,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4722/10000,\n",
      " train_loss: 838.8832,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0078\n",
      "\n",
      "epoch: 4723/10000,\n",
      " train_loss: 838.8832,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 4724/10000,\n",
      " train_loss: 838.8832,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 4725/10000,\n",
      " train_loss: 838.8832,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 4726/10000,\n",
      " train_loss: 838.8832,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 4727/10000,\n",
      " train_loss: 838.8832,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 4728/10000,\n",
      " train_loss: 838.8832,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4729/10000,\n",
      " train_loss: 838.8831,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0074\n",
      "\n",
      "epoch: 4730/10000,\n",
      " train_loss: 838.8832,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4731/10000,\n",
      " train_loss: 838.8832,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 4732/10000,\n",
      " train_loss: 838.8831,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 4733/10000,\n",
      " train_loss: 838.8831,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0075\n",
      "\n",
      "epoch: 4734/10000,\n",
      " train_loss: 838.8831,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0081\n",
      "\n",
      "epoch: 4735/10000,\n",
      " train_loss: 838.8831,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4736/10000,\n",
      " train_loss: 838.8831,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4737/10000,\n",
      " train_loss: 838.8831,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4738/10000,\n",
      " train_loss: 838.8831,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4739/10000,\n",
      " train_loss: 838.8830,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 4740/10000,\n",
      " train_loss: 838.8831,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4741/10000,\n",
      " train_loss: 838.8830,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4742/10000,\n",
      " train_loss: 838.8831,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4743/10000,\n",
      " train_loss: 838.8830,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4744/10000,\n",
      " train_loss: 838.8830,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4745/10000,\n",
      " train_loss: 838.8830,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4746/10000,\n",
      " train_loss: 838.8830,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4747/10000,\n",
      " train_loss: 838.8829,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4748/10000,\n",
      " train_loss: 838.8829,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4749/10000,\n",
      " train_loss: 838.8829,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0083\n",
      "\n",
      "epoch: 4750/10000,\n",
      " train_loss: 838.8829,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0078\n",
      "\n",
      "epoch: 4751/10000,\n",
      " train_loss: 838.8829,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 4752/10000,\n",
      " train_loss: 838.8829,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4753/10000,\n",
      " train_loss: 838.8829,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4754/10000,\n",
      " train_loss: 838.8829,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4755/10000,\n",
      " train_loss: 838.8828,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4756/10000,\n",
      " train_loss: 838.8829,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4757/10000,\n",
      " train_loss: 838.8828,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4758/10000,\n",
      " train_loss: 838.8828,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4759/10000,\n",
      " train_loss: 838.8828,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4760/10000,\n",
      " train_loss: 838.8828,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4761/10000,\n",
      " train_loss: 838.8828,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4762/10000,\n",
      " train_loss: 838.8828,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4763/10000,\n",
      " train_loss: 838.8828,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4764/10000,\n",
      " train_loss: 838.8828,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4765/10000,\n",
      " train_loss: 838.8828,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4766/10000,\n",
      " train_loss: 838.8828,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4767/10000,\n",
      " train_loss: 838.8828,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 4768/10000,\n",
      " train_loss: 838.8828,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4769/10000,\n",
      " train_loss: 838.8826,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4770/10000,\n",
      " train_loss: 838.8826,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4771/10000,\n",
      " train_loss: 838.8826,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4772/10000,\n",
      " train_loss: 838.8826,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4773/10000,\n",
      " train_loss: 838.8826,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 4774/10000,\n",
      " train_loss: 838.8826,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4775/10000,\n",
      " train_loss: 838.8826,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4776/10000,\n",
      " train_loss: 838.8825,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4777/10000,\n",
      " train_loss: 838.8825,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4778/10000,\n",
      " train_loss: 838.8825,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4779/10000,\n",
      " train_loss: 838.8824,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4780/10000,\n",
      " train_loss: 838.8824,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4781/10000,\n",
      " train_loss: 838.8824,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0078\n",
      "\n",
      "epoch: 4782/10000,\n",
      " train_loss: 838.8824,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0183\n",
      "\n",
      "epoch: 4783/10000,\n",
      " train_loss: 838.8824,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 4784/10000,\n",
      " train_loss: 838.8824,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 4785/10000,\n",
      " train_loss: 838.8824,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 4786/10000,\n",
      " train_loss: 838.8824,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0077\n",
      "\n",
      "epoch: 4787/10000,\n",
      " train_loss: 838.8823,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0073\n",
      "\n",
      "epoch: 4788/10000,\n",
      " train_loss: 838.8823,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 4789/10000,\n",
      " train_loss: 838.8823,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4790/10000,\n",
      " train_loss: 838.8823,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 4791/10000,\n",
      " train_loss: 838.8823,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0082\n",
      "\n",
      "epoch: 4792/10000,\n",
      " train_loss: 838.8823,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 4793/10000,\n",
      " train_loss: 838.8823,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 4794/10000,\n",
      " train_loss: 838.8823,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4795/10000,\n",
      " train_loss: 838.8823,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4796/10000,\n",
      " train_loss: 838.8823,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4797/10000,\n",
      " train_loss: 838.8823,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4798/10000,\n",
      " train_loss: 838.8823,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4799/10000,\n",
      " train_loss: 838.8822,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4800/10000,\n",
      " train_loss: 838.8822,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4801/10000,\n",
      " train_loss: 838.8822,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4802/10000,\n",
      " train_loss: 838.8822,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4803/10000,\n",
      " train_loss: 838.8822,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4804/10000,\n",
      " train_loss: 838.8821,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4805/10000,\n",
      " train_loss: 838.8821,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4806/10000,\n",
      " train_loss: 838.8821,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4807/10000,\n",
      " train_loss: 838.8822,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4808/10000,\n",
      " train_loss: 838.8821,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0088\n",
      "\n",
      "epoch: 4809/10000,\n",
      " train_loss: 838.8821,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0107\n",
      "\n",
      "epoch: 4810/10000,\n",
      " train_loss: 838.8821,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 4811/10000,\n",
      " train_loss: 838.8821,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0085\n",
      "\n",
      "epoch: 4812/10000,\n",
      " train_loss: 838.8821,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0092\n",
      "\n",
      "epoch: 4813/10000,\n",
      " train_loss: 838.8821,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0084\n",
      "\n",
      "epoch: 4814/10000,\n",
      " train_loss: 838.8821,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0216\n",
      "\n",
      "epoch: 4815/10000,\n",
      " train_loss: 838.8821,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0117\n",
      "\n",
      "epoch: 4816/10000,\n",
      " train_loss: 838.8821,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 4817/10000,\n",
      " train_loss: 838.8820,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 4818/10000,\n",
      " train_loss: 838.8820,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0088\n",
      "\n",
      "epoch: 4819/10000,\n",
      " train_loss: 838.8820,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0090\n",
      "\n",
      "epoch: 4820/10000,\n",
      " train_loss: 838.8820,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4821/10000,\n",
      " train_loss: 838.8820,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4822/10000,\n",
      " train_loss: 838.8820,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4823/10000,\n",
      " train_loss: 838.8820,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4824/10000,\n",
      " train_loss: 838.8820,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 4825/10000,\n",
      " train_loss: 838.8820,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0072\n",
      "\n",
      "epoch: 4826/10000,\n",
      " train_loss: 838.8820,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4827/10000,\n",
      " train_loss: 838.8820,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 4828/10000,\n",
      " train_loss: 838.8818,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0091\n",
      "\n",
      "epoch: 4829/10000,\n",
      " train_loss: 838.8819,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 4830/10000,\n",
      " train_loss: 838.8819,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 4831/10000,\n",
      " train_loss: 838.8820,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 4832/10000,\n",
      " train_loss: 838.8818,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4833/10000,\n",
      " train_loss: 838.8818,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4834/10000,\n",
      " train_loss: 838.8818,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4835/10000,\n",
      " train_loss: 838.8818,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4836/10000,\n",
      " train_loss: 838.8818,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4837/10000,\n",
      " train_loss: 838.8818,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4838/10000,\n",
      " train_loss: 838.8818,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 4839/10000,\n",
      " train_loss: 838.8818,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0076\n",
      "\n",
      "epoch: 4840/10000,\n",
      " train_loss: 838.8818,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 4841/10000,\n",
      " train_loss: 838.8817,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0072\n",
      "\n",
      "epoch: 4842/10000,\n",
      " train_loss: 838.8817,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 4843/10000,\n",
      " train_loss: 838.8817,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0080\n",
      "\n",
      "epoch: 4844/10000,\n",
      " train_loss: 838.8817,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4845/10000,\n",
      " train_loss: 838.8817,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 4846/10000,\n",
      " train_loss: 838.8817,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 4847/10000,\n",
      " train_loss: 838.8817,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 4848/10000,\n",
      " train_loss: 838.8817,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0074\n",
      "\n",
      "epoch: 4849/10000,\n",
      " train_loss: 838.8815,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 4850/10000,\n",
      " train_loss: 838.8816,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 4851/10000,\n",
      " train_loss: 838.8815,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0077\n",
      "\n",
      "epoch: 4852/10000,\n",
      " train_loss: 838.8815,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4853/10000,\n",
      " train_loss: 838.8815,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4854/10000,\n",
      " train_loss: 838.8815,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 4855/10000,\n",
      " train_loss: 838.8815,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4856/10000,\n",
      " train_loss: 838.8815,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 4857/10000,\n",
      " train_loss: 838.8815,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0147\n",
      "\n",
      "epoch: 4858/10000,\n",
      " train_loss: 838.8815,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4859/10000,\n",
      " train_loss: 838.8815,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 4860/10000,\n",
      " train_loss: 838.8815,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 4861/10000,\n",
      " train_loss: 838.8815,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4862/10000,\n",
      " train_loss: 838.8814,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4863/10000,\n",
      " train_loss: 838.8814,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4864/10000,\n",
      " train_loss: 838.8814,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4865/10000,\n",
      " train_loss: 838.8814,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4866/10000,\n",
      " train_loss: 838.8814,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4867/10000,\n",
      " train_loss: 838.8814,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4868/10000,\n",
      " train_loss: 838.8814,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4869/10000,\n",
      " train_loss: 838.8814,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4870/10000,\n",
      " train_loss: 838.8814,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4871/10000,\n",
      " train_loss: 838.8813,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4872/10000,\n",
      " train_loss: 838.8813,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4873/10000,\n",
      " train_loss: 838.8814,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4874/10000,\n",
      " train_loss: 838.8813,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4875/10000,\n",
      " train_loss: 838.8813,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4876/10000,\n",
      " train_loss: 838.8813,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4877/10000,\n",
      " train_loss: 838.8813,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4878/10000,\n",
      " train_loss: 838.8813,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4879/10000,\n",
      " train_loss: 838.8813,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4880/10000,\n",
      " train_loss: 838.8813,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4881/10000,\n",
      " train_loss: 838.8813,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 4882/10000,\n",
      " train_loss: 838.8812,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0096\n",
      "\n",
      "epoch: 4883/10000,\n",
      " train_loss: 838.8812,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0101\n",
      "\n",
      "epoch: 4884/10000,\n",
      " train_loss: 838.8812,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0073\n",
      "\n",
      "epoch: 4885/10000,\n",
      " train_loss: 838.8812,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4886/10000,\n",
      " train_loss: 838.8812,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4887/10000,\n",
      " train_loss: 838.8812,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4888/10000,\n",
      " train_loss: 838.8812,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4889/10000,\n",
      " train_loss: 838.8812,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4890/10000,\n",
      " train_loss: 838.8811,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4891/10000,\n",
      " train_loss: 838.8811,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4892/10000,\n",
      " train_loss: 838.8811,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4893/10000,\n",
      " train_loss: 838.8811,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4894/10000,\n",
      " train_loss: 838.8811,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4895/10000,\n",
      " train_loss: 838.8810,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4896/10000,\n",
      " train_loss: 838.8810,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4897/10000,\n",
      " train_loss: 838.8811,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4898/10000,\n",
      " train_loss: 838.8810,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4899/10000,\n",
      " train_loss: 838.8810,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4900/10000,\n",
      " train_loss: 838.8810,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 4901/10000,\n",
      " train_loss: 838.8810,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4902/10000,\n",
      " train_loss: 838.8810,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4903/10000,\n",
      " train_loss: 838.8810,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4904/10000,\n",
      " train_loss: 838.8809,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4905/10000,\n",
      " train_loss: 838.8810,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4906/10000,\n",
      " train_loss: 838.8810,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4907/10000,\n",
      " train_loss: 838.8809,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4908/10000,\n",
      " train_loss: 838.8809,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4909/10000,\n",
      " train_loss: 838.8809,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4910/10000,\n",
      " train_loss: 838.8809,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4911/10000,\n",
      " train_loss: 838.8809,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 4912/10000,\n",
      " train_loss: 838.8809,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4913/10000,\n",
      " train_loss: 838.8809,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 4914/10000,\n",
      " train_loss: 838.8809,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 4915/10000,\n",
      " train_loss: 838.8808,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0072\n",
      "\n",
      "epoch: 4916/10000,\n",
      " train_loss: 838.8808,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4917/10000,\n",
      " train_loss: 838.8808,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 4918/10000,\n",
      " train_loss: 838.8808,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4919/10000,\n",
      " train_loss: 838.8808,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4920/10000,\n",
      " train_loss: 838.8808,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 4921/10000,\n",
      " train_loss: 838.8808,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4922/10000,\n",
      " train_loss: 838.8808,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4923/10000,\n",
      " train_loss: 838.8808,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4924/10000,\n",
      " train_loss: 838.8807,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4925/10000,\n",
      " train_loss: 838.8808,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4926/10000,\n",
      " train_loss: 838.8807,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 4927/10000,\n",
      " train_loss: 838.8806,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 4928/10000,\n",
      " train_loss: 838.8807,\n",
      " train_mae: 25.3310,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 4929/10000,\n",
      " train_loss: 838.8807,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4930/10000,\n",
      " train_loss: 838.8807,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4931/10000,\n",
      " train_loss: 838.8806,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 4932/10000,\n",
      " train_loss: 838.8806,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 4933/10000,\n",
      " train_loss: 838.8806,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 4934/10000,\n",
      " train_loss: 838.8806,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4935/10000,\n",
      " train_loss: 838.8806,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 4936/10000,\n",
      " train_loss: 838.8806,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 4937/10000,\n",
      " train_loss: 838.8806,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 4938/10000,\n",
      " train_loss: 838.8806,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4939/10000,\n",
      " train_loss: 838.8806,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 4940/10000,\n",
      " train_loss: 838.8805,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0117\n",
      "\n",
      "epoch: 4941/10000,\n",
      " train_loss: 838.8806,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0142\n",
      "\n",
      "epoch: 4942/10000,\n",
      " train_loss: 838.8806,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0093\n",
      "\n",
      "epoch: 4943/10000,\n",
      " train_loss: 838.8805,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0076\n",
      "\n",
      "epoch: 4944/10000,\n",
      " train_loss: 838.8805,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0092\n",
      "\n",
      "epoch: 4945/10000,\n",
      " train_loss: 838.8805,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0115\n",
      "\n",
      "epoch: 4946/10000,\n",
      " train_loss: 838.8805,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0098\n",
      "\n",
      "epoch: 4947/10000,\n",
      " train_loss: 838.8805,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0127\n",
      "\n",
      "epoch: 4948/10000,\n",
      " train_loss: 838.8804,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0084\n",
      "\n",
      "epoch: 4949/10000,\n",
      " train_loss: 838.8803,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0074\n",
      "\n",
      "epoch: 4950/10000,\n",
      " train_loss: 838.8804,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4951/10000,\n",
      " train_loss: 838.8803,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4952/10000,\n",
      " train_loss: 838.8804,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 4953/10000,\n",
      " train_loss: 838.8803,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4954/10000,\n",
      " train_loss: 838.8803,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 4955/10000,\n",
      " train_loss: 838.8803,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4956/10000,\n",
      " train_loss: 838.8803,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4957/10000,\n",
      " train_loss: 838.8803,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 4958/10000,\n",
      " train_loss: 838.8803,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4959/10000,\n",
      " train_loss: 838.8803,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 4960/10000,\n",
      " train_loss: 838.8803,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 4961/10000,\n",
      " train_loss: 838.8803,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4962/10000,\n",
      " train_loss: 838.8803,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4963/10000,\n",
      " train_loss: 838.8802,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4964/10000,\n",
      " train_loss: 838.8802,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4965/10000,\n",
      " train_loss: 838.8802,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4966/10000,\n",
      " train_loss: 838.8802,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4967/10000,\n",
      " train_loss: 838.8802,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 4968/10000,\n",
      " train_loss: 838.8802,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 4969/10000,\n",
      " train_loss: 838.8802,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 4970/10000,\n",
      " train_loss: 838.8802,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 4971/10000,\n",
      " train_loss: 838.8802,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 4972/10000,\n",
      " train_loss: 838.8802,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4973/10000,\n",
      " train_loss: 838.8802,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 4974/10000,\n",
      " train_loss: 838.8801,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0080\n",
      "\n",
      "epoch: 4975/10000,\n",
      " train_loss: 838.8802,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 4976/10000,\n",
      " train_loss: 838.8801,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 4977/10000,\n",
      " train_loss: 838.8801,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4978/10000,\n",
      " train_loss: 838.8801,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 4979/10000,\n",
      " train_loss: 838.8801,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 4980/10000,\n",
      " train_loss: 838.8801,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 4981/10000,\n",
      " train_loss: 838.8801,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 4982/10000,\n",
      " train_loss: 838.8801,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 4983/10000,\n",
      " train_loss: 838.8801,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 4984/10000,\n",
      " train_loss: 838.8801,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4985/10000,\n",
      " train_loss: 838.8800,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 4986/10000,\n",
      " train_loss: 838.8801,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 4987/10000,\n",
      " train_loss: 838.8801,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 4988/10000,\n",
      " train_loss: 838.8801,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 4989/10000,\n",
      " train_loss: 838.8800,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 4990/10000,\n",
      " train_loss: 838.8800,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4991/10000,\n",
      " train_loss: 838.8800,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4992/10000,\n",
      " train_loss: 838.8800,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 4993/10000,\n",
      " train_loss: 838.8800,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 4994/10000,\n",
      " train_loss: 838.8800,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4995/10000,\n",
      " train_loss: 838.8799,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 4996/10000,\n",
      " train_loss: 838.8799,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4997/10000,\n",
      " train_loss: 838.8799,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 4998/10000,\n",
      " train_loss: 838.8799,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 4999/10000,\n",
      " train_loss: 838.8799,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5000/10000,\n",
      " train_loss: 838.8799,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5001/10000,\n",
      " train_loss: 838.8799,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5002/10000,\n",
      " train_loss: 838.8799,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 5003/10000,\n",
      " train_loss: 838.8799,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0098\n",
      "\n",
      "epoch: 5004/10000,\n",
      " train_loss: 838.8799,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5005/10000,\n",
      " train_loss: 838.8799,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5006/10000,\n",
      " train_loss: 838.8799,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5007/10000,\n",
      " train_loss: 838.8799,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5008/10000,\n",
      " train_loss: 838.8798,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5009/10000,\n",
      " train_loss: 838.8799,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5010/10000,\n",
      " train_loss: 838.8798,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5011/10000,\n",
      " train_loss: 838.8798,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0104\n",
      "\n",
      "epoch: 5012/10000,\n",
      " train_loss: 838.8798,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 5013/10000,\n",
      " train_loss: 838.8798,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 5014/10000,\n",
      " train_loss: 838.8798,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 5015/10000,\n",
      " train_loss: 838.8798,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 5016/10000,\n",
      " train_loss: 838.8798,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5017/10000,\n",
      " train_loss: 838.8798,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 5018/10000,\n",
      " train_loss: 838.8796,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5019/10000,\n",
      " train_loss: 838.8798,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5020/10000,\n",
      " train_loss: 838.8796,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5021/10000,\n",
      " train_loss: 838.8796,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5022/10000,\n",
      " train_loss: 838.8796,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5023/10000,\n",
      " train_loss: 838.8796,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5024/10000,\n",
      " train_loss: 838.8796,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5025/10000,\n",
      " train_loss: 838.8796,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5026/10000,\n",
      " train_loss: 838.8796,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5027/10000,\n",
      " train_loss: 838.8796,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5028/10000,\n",
      " train_loss: 838.8796,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5029/10000,\n",
      " train_loss: 838.8796,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5030/10000,\n",
      " train_loss: 838.8796,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5031/10000,\n",
      " train_loss: 838.8796,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5032/10000,\n",
      " train_loss: 838.8795,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5033/10000,\n",
      " train_loss: 838.8795,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5034/10000,\n",
      " train_loss: 838.8795,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 5035/10000,\n",
      " train_loss: 838.8795,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0127\n",
      "\n",
      "epoch: 5036/10000,\n",
      " train_loss: 838.8795,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5037/10000,\n",
      " train_loss: 838.8795,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5038/10000,\n",
      " train_loss: 838.8795,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 5039/10000,\n",
      " train_loss: 838.8795,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 5040/10000,\n",
      " train_loss: 838.8795,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 5041/10000,\n",
      " train_loss: 838.8795,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 5042/10000,\n",
      " train_loss: 838.8795,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5043/10000,\n",
      " train_loss: 838.8795,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 5044/10000,\n",
      " train_loss: 838.8795,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 5045/10000,\n",
      " train_loss: 838.8793,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 5046/10000,\n",
      " train_loss: 838.8795,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 5047/10000,\n",
      " train_loss: 838.8793,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 5048/10000,\n",
      " train_loss: 838.8793,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5049/10000,\n",
      " train_loss: 838.8793,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5050/10000,\n",
      " train_loss: 838.8793,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5051/10000,\n",
      " train_loss: 838.8793,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5052/10000,\n",
      " train_loss: 838.8793,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5053/10000,\n",
      " train_loss: 838.8793,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5054/10000,\n",
      " train_loss: 838.8793,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5055/10000,\n",
      " train_loss: 838.8793,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5056/10000,\n",
      " train_loss: 838.8793,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5057/10000,\n",
      " train_loss: 838.8793,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5058/10000,\n",
      " train_loss: 838.8793,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5059/10000,\n",
      " train_loss: 838.8793,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5060/10000,\n",
      " train_loss: 838.8792,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5061/10000,\n",
      " train_loss: 838.8793,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0110\n",
      "\n",
      "epoch: 5062/10000,\n",
      " train_loss: 838.8792,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0080\n",
      "\n",
      "epoch: 5063/10000,\n",
      " train_loss: 838.8792,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5064/10000,\n",
      " train_loss: 838.8792,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5065/10000,\n",
      " train_loss: 838.8792,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5066/10000,\n",
      " train_loss: 838.8792,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5067/10000,\n",
      " train_loss: 838.8792,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5068/10000,\n",
      " train_loss: 838.8792,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5069/10000,\n",
      " train_loss: 838.8792,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5070/10000,\n",
      " train_loss: 838.8792,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5071/10000,\n",
      " train_loss: 838.8792,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5072/10000,\n",
      " train_loss: 838.8792,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5073/10000,\n",
      " train_loss: 838.8791,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5074/10000,\n",
      " train_loss: 838.8791,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5075/10000,\n",
      " train_loss: 838.8791,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5076/10000,\n",
      " train_loss: 838.8791,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5077/10000,\n",
      " train_loss: 838.8791,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 5078/10000,\n",
      " train_loss: 838.8791,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 5079/10000,\n",
      " train_loss: 838.8791,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5080/10000,\n",
      " train_loss: 838.8791,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5081/10000,\n",
      " train_loss: 838.8791,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5082/10000,\n",
      " train_loss: 838.8790,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5083/10000,\n",
      " train_loss: 838.8791,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5084/10000,\n",
      " train_loss: 838.8790,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 5085/10000,\n",
      " train_loss: 838.8790,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5086/10000,\n",
      " train_loss: 838.8790,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5087/10000,\n",
      " train_loss: 838.8790,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5088/10000,\n",
      " train_loss: 838.8790,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5089/10000,\n",
      " train_loss: 838.8790,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 5090/10000,\n",
      " train_loss: 838.8790,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5091/10000,\n",
      " train_loss: 838.8790,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5092/10000,\n",
      " train_loss: 838.8790,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 5093/10000,\n",
      " train_loss: 838.8790,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0083\n",
      "\n",
      "epoch: 5094/10000,\n",
      " train_loss: 838.8789,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 5095/10000,\n",
      " train_loss: 838.8789,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 5096/10000,\n",
      " train_loss: 838.8788,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5097/10000,\n",
      " train_loss: 838.8789,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5098/10000,\n",
      " train_loss: 838.8788,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5099/10000,\n",
      " train_loss: 838.8789,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5100/10000,\n",
      " train_loss: 838.8788,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5101/10000,\n",
      " train_loss: 838.8788,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5102/10000,\n",
      " train_loss: 838.8788,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5103/10000,\n",
      " train_loss: 838.8788,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5104/10000,\n",
      " train_loss: 838.8788,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5105/10000,\n",
      " train_loss: 838.8788,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5106/10000,\n",
      " train_loss: 838.8788,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5107/10000,\n",
      " train_loss: 838.8788,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5108/10000,\n",
      " train_loss: 838.8788,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5109/10000,\n",
      " train_loss: 838.8787,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5110/10000,\n",
      " train_loss: 838.8787,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5111/10000,\n",
      " train_loss: 838.8788,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 5112/10000,\n",
      " train_loss: 838.8787,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 5113/10000,\n",
      " train_loss: 838.8788,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 5114/10000,\n",
      " train_loss: 838.8787,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 5115/10000,\n",
      " train_loss: 838.8787,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5116/10000,\n",
      " train_loss: 838.8786,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5117/10000,\n",
      " train_loss: 838.8787,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0108\n",
      "\n",
      "epoch: 5118/10000,\n",
      " train_loss: 838.8787,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0125\n",
      "\n",
      "epoch: 5119/10000,\n",
      " train_loss: 838.8787,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5120/10000,\n",
      " train_loss: 838.8787,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5121/10000,\n",
      " train_loss: 838.8786,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5122/10000,\n",
      " train_loss: 838.8786,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5123/10000,\n",
      " train_loss: 838.8786,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5124/10000,\n",
      " train_loss: 838.8785,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5125/10000,\n",
      " train_loss: 838.8785,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5126/10000,\n",
      " train_loss: 838.8785,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5127/10000,\n",
      " train_loss: 838.8785,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5128/10000,\n",
      " train_loss: 838.8785,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5129/10000,\n",
      " train_loss: 838.8785,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5130/10000,\n",
      " train_loss: 838.8785,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5131/10000,\n",
      " train_loss: 838.8785,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5132/10000,\n",
      " train_loss: 838.8785,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5133/10000,\n",
      " train_loss: 838.8785,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5134/10000,\n",
      " train_loss: 838.8785,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5135/10000,\n",
      " train_loss: 838.8785,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5136/10000,\n",
      " train_loss: 838.8785,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5137/10000,\n",
      " train_loss: 838.8784,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5138/10000,\n",
      " train_loss: 838.8785,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5139/10000,\n",
      " train_loss: 838.8784,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5140/10000,\n",
      " train_loss: 838.8784,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5141/10000,\n",
      " train_loss: 838.8784,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5142/10000,\n",
      " train_loss: 838.8784,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5143/10000,\n",
      " train_loss: 838.8784,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 5144/10000,\n",
      " train_loss: 838.8784,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5145/10000,\n",
      " train_loss: 838.8784,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5146/10000,\n",
      " train_loss: 838.8784,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 5147/10000,\n",
      " train_loss: 838.8784,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0099\n",
      "\n",
      "epoch: 5148/10000,\n",
      " train_loss: 838.8783,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 5149/10000,\n",
      " train_loss: 838.8784,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0143\n",
      "\n",
      "epoch: 5150/10000,\n",
      " train_loss: 838.8784,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 5151/10000,\n",
      " train_loss: 838.8783,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5152/10000,\n",
      " train_loss: 838.8783,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5153/10000,\n",
      " train_loss: 838.8784,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5154/10000,\n",
      " train_loss: 838.8783,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5155/10000,\n",
      " train_loss: 838.8783,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5156/10000,\n",
      " train_loss: 838.8784,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5157/10000,\n",
      " train_loss: 838.8783,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5158/10000,\n",
      " train_loss: 838.8783,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5159/10000,\n",
      " train_loss: 838.8783,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5160/10000,\n",
      " train_loss: 838.8783,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5161/10000,\n",
      " train_loss: 838.8783,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5162/10000,\n",
      " train_loss: 838.8783,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5163/10000,\n",
      " train_loss: 838.8782,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5164/10000,\n",
      " train_loss: 838.8782,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5165/10000,\n",
      " train_loss: 838.8782,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5166/10000,\n",
      " train_loss: 838.8782,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5167/10000,\n",
      " train_loss: 838.8782,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5168/10000,\n",
      " train_loss: 838.8782,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5169/10000,\n",
      " train_loss: 838.8782,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5170/10000,\n",
      " train_loss: 838.8782,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5171/10000,\n",
      " train_loss: 838.8782,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5172/10000,\n",
      " train_loss: 838.8782,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5173/10000,\n",
      " train_loss: 838.8782,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5174/10000,\n",
      " train_loss: 838.8782,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5175/10000,\n",
      " train_loss: 838.8782,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5176/10000,\n",
      " train_loss: 838.8782,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5177/10000,\n",
      " train_loss: 838.8781,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 5178/10000,\n",
      " train_loss: 838.8781,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5179/10000,\n",
      " train_loss: 838.8781,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 5180/10000,\n",
      " train_loss: 838.8781,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0164\n",
      "\n",
      "epoch: 5181/10000,\n",
      " train_loss: 838.8781,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5182/10000,\n",
      " train_loss: 838.8781,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5183/10000,\n",
      " train_loss: 838.8781,\n",
      " train_mae: 25.3309,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5184/10000,\n",
      " train_loss: 838.8781,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0109\n",
      "\n",
      "epoch: 5185/10000,\n",
      " train_loss: 838.8781,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 5186/10000,\n",
      " train_loss: 838.8781,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 5187/10000,\n",
      " train_loss: 838.8781,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 5188/10000,\n",
      " train_loss: 838.8781,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 5189/10000,\n",
      " train_loss: 838.8780,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 5190/10000,\n",
      " train_loss: 838.8780,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5191/10000,\n",
      " train_loss: 838.8780,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5192/10000,\n",
      " train_loss: 838.8780,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5193/10000,\n",
      " train_loss: 838.8779,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5194/10000,\n",
      " train_loss: 838.8780,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5195/10000,\n",
      " train_loss: 838.8780,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5196/10000,\n",
      " train_loss: 838.8779,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5197/10000,\n",
      " train_loss: 838.8780,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5198/10000,\n",
      " train_loss: 838.8779,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5199/10000,\n",
      " train_loss: 838.8779,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5200/10000,\n",
      " train_loss: 838.8779,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5201/10000,\n",
      " train_loss: 838.8779,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5202/10000,\n",
      " train_loss: 838.8779,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5203/10000,\n",
      " train_loss: 838.8779,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5204/10000,\n",
      " train_loss: 838.8779,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5205/10000,\n",
      " train_loss: 838.8779,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5206/10000,\n",
      " train_loss: 838.8779,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5207/10000,\n",
      " train_loss: 838.8779,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 5208/10000,\n",
      " train_loss: 838.8779,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0113\n",
      "\n",
      "epoch: 5209/10000,\n",
      " train_loss: 838.8779,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5210/10000,\n",
      " train_loss: 838.8778,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5211/10000,\n",
      " train_loss: 838.8778,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 5212/10000,\n",
      " train_loss: 838.8778,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5213/10000,\n",
      " train_loss: 838.8778,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5214/10000,\n",
      " train_loss: 838.8778,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5215/10000,\n",
      " train_loss: 838.8778,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 5216/10000,\n",
      " train_loss: 838.8778,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 5217/10000,\n",
      " train_loss: 838.8778,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 5218/10000,\n",
      " train_loss: 838.8778,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5219/10000,\n",
      " train_loss: 838.8778,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5220/10000,\n",
      " train_loss: 838.8777,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 5221/10000,\n",
      " train_loss: 838.8777,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 5222/10000,\n",
      " train_loss: 838.8778,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5223/10000,\n",
      " train_loss: 838.8777,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 5224/10000,\n",
      " train_loss: 838.8776,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 5225/10000,\n",
      " train_loss: 838.8776,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5226/10000,\n",
      " train_loss: 838.8776,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5227/10000,\n",
      " train_loss: 838.8776,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 5228/10000,\n",
      " train_loss: 838.8776,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5229/10000,\n",
      " train_loss: 838.8776,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5230/10000,\n",
      " train_loss: 838.8776,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5231/10000,\n",
      " train_loss: 838.8776,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0133\n",
      "\n",
      "epoch: 5232/10000,\n",
      " train_loss: 838.8776,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5233/10000,\n",
      " train_loss: 838.8776,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5234/10000,\n",
      " train_loss: 838.8776,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5235/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5236/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5237/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5238/10000,\n",
      " train_loss: 838.8776,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5239/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5240/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5241/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5242/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5243/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5244/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5245/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5246/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5247/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5248/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 5249/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5250/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5251/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5252/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5253/10000,\n",
      " train_loss: 838.8774,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5254/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5255/10000,\n",
      " train_loss: 838.8774,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5256/10000,\n",
      " train_loss: 838.8774,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5257/10000,\n",
      " train_loss: 838.8774,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5258/10000,\n",
      " train_loss: 838.8774,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5259/10000,\n",
      " train_loss: 838.8774,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5260/10000,\n",
      " train_loss: 838.8775,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5261/10000,\n",
      " train_loss: 838.8774,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5262/10000,\n",
      " train_loss: 838.8774,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5263/10000,\n",
      " train_loss: 838.8774,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5264/10000,\n",
      " train_loss: 838.8773,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 5265/10000,\n",
      " train_loss: 838.8774,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 5266/10000,\n",
      " train_loss: 838.8774,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5267/10000,\n",
      " train_loss: 838.8773,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5268/10000,\n",
      " train_loss: 838.8773,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 5269/10000,\n",
      " train_loss: 838.8773,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5270/10000,\n",
      " train_loss: 838.8773,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5271/10000,\n",
      " train_loss: 838.8773,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5272/10000,\n",
      " train_loss: 838.8773,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5273/10000,\n",
      " train_loss: 838.8773,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5274/10000,\n",
      " train_loss: 838.8774,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5275/10000,\n",
      " train_loss: 838.8773,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5276/10000,\n",
      " train_loss: 838.8773,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5277/10000,\n",
      " train_loss: 838.8773,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5278/10000,\n",
      " train_loss: 838.8773,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5279/10000,\n",
      " train_loss: 838.8773,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5280/10000,\n",
      " train_loss: 838.8773,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5281/10000,\n",
      " train_loss: 838.8773,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5282/10000,\n",
      " train_loss: 838.8773,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5283/10000,\n",
      " train_loss: 838.8772,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5284/10000,\n",
      " train_loss: 838.8773,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5285/10000,\n",
      " train_loss: 838.8771,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5286/10000,\n",
      " train_loss: 838.8773,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5287/10000,\n",
      " train_loss: 838.8772,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5288/10000,\n",
      " train_loss: 838.8771,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5289/10000,\n",
      " train_loss: 838.8771,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0075\n",
      "\n",
      "epoch: 5290/10000,\n",
      " train_loss: 838.8771,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 5291/10000,\n",
      " train_loss: 838.8771,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0076\n",
      "\n",
      "epoch: 5292/10000,\n",
      " train_loss: 838.8771,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 5293/10000,\n",
      " train_loss: 838.8771,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 5294/10000,\n",
      " train_loss: 838.8771,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0077\n",
      "\n",
      "epoch: 5295/10000,\n",
      " train_loss: 838.8771,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0123\n",
      "\n",
      "epoch: 5296/10000,\n",
      " train_loss: 838.8771,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 5297/10000,\n",
      " train_loss: 838.8771,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 5298/10000,\n",
      " train_loss: 838.8771,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5299/10000,\n",
      " train_loss: 838.8771,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5300/10000,\n",
      " train_loss: 838.8771,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5301/10000,\n",
      " train_loss: 838.8771,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5302/10000,\n",
      " train_loss: 838.8770,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5303/10000,\n",
      " train_loss: 838.8771,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5304/10000,\n",
      " train_loss: 838.8771,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5305/10000,\n",
      " train_loss: 838.8770,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5306/10000,\n",
      " train_loss: 838.8770,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5307/10000,\n",
      " train_loss: 838.8770,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5308/10000,\n",
      " train_loss: 838.8770,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5309/10000,\n",
      " train_loss: 838.8770,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5310/10000,\n",
      " train_loss: 838.8770,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5311/10000,\n",
      " train_loss: 838.8770,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5312/10000,\n",
      " train_loss: 838.8770,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5313/10000,\n",
      " train_loss: 838.8770,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5314/10000,\n",
      " train_loss: 838.8770,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5315/10000,\n",
      " train_loss: 838.8770,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5316/10000,\n",
      " train_loss: 838.8770,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5317/10000,\n",
      " train_loss: 838.8770,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5318/10000,\n",
      " train_loss: 838.8770,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5319/10000,\n",
      " train_loss: 838.8770,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5320/10000,\n",
      " train_loss: 838.8769,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5321/10000,\n",
      " train_loss: 838.8770,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 5322/10000,\n",
      " train_loss: 838.8769,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5323/10000,\n",
      " train_loss: 838.8769,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5324/10000,\n",
      " train_loss: 838.8769,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5325/10000,\n",
      " train_loss: 838.8769,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5326/10000,\n",
      " train_loss: 838.8769,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5327/10000,\n",
      " train_loss: 838.8769,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5328/10000,\n",
      " train_loss: 838.8769,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5329/10000,\n",
      " train_loss: 838.8769,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5330/10000,\n",
      " train_loss: 838.8769,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0080\n",
      "\n",
      "epoch: 5331/10000,\n",
      " train_loss: 838.8768,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 5332/10000,\n",
      " train_loss: 838.8768,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5333/10000,\n",
      " train_loss: 838.8768,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 5334/10000,\n",
      " train_loss: 838.8767,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5335/10000,\n",
      " train_loss: 838.8767,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 5336/10000,\n",
      " train_loss: 838.8767,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 5337/10000,\n",
      " train_loss: 838.8768,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5338/10000,\n",
      " train_loss: 838.8768,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5339/10000,\n",
      " train_loss: 838.8767,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5340/10000,\n",
      " train_loss: 838.8767,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5341/10000,\n",
      " train_loss: 838.8767,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5342/10000,\n",
      " train_loss: 838.8767,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5343/10000,\n",
      " train_loss: 838.8767,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5344/10000,\n",
      " train_loss: 838.8767,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5345/10000,\n",
      " train_loss: 838.8767,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5346/10000,\n",
      " train_loss: 838.8767,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5347/10000,\n",
      " train_loss: 838.8766,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5348/10000,\n",
      " train_loss: 838.8767,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5349/10000,\n",
      " train_loss: 838.8766,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5350/10000,\n",
      " train_loss: 838.8766,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5351/10000,\n",
      " train_loss: 838.8766,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5352/10000,\n",
      " train_loss: 838.8766,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5353/10000,\n",
      " train_loss: 838.8766,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5354/10000,\n",
      " train_loss: 838.8766,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5355/10000,\n",
      " train_loss: 838.8766,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5356/10000,\n",
      " train_loss: 838.8766,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5357/10000,\n",
      " train_loss: 838.8766,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5358/10000,\n",
      " train_loss: 838.8766,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 5359/10000,\n",
      " train_loss: 838.8766,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0080\n",
      "\n",
      "epoch: 5360/10000,\n",
      " train_loss: 838.8766,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 5361/10000,\n",
      " train_loss: 838.8765,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 5362/10000,\n",
      " train_loss: 838.8766,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 5363/10000,\n",
      " train_loss: 838.8765,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 5364/10000,\n",
      " train_loss: 838.8765,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5365/10000,\n",
      " train_loss: 838.8766,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5366/10000,\n",
      " train_loss: 838.8764,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0095\n",
      "\n",
      "epoch: 5367/10000,\n",
      " train_loss: 838.8765,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5368/10000,\n",
      " train_loss: 838.8765,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5369/10000,\n",
      " train_loss: 838.8764,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5370/10000,\n",
      " train_loss: 838.8765,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5371/10000,\n",
      " train_loss: 838.8764,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5372/10000,\n",
      " train_loss: 838.8764,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5373/10000,\n",
      " train_loss: 838.8764,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5374/10000,\n",
      " train_loss: 838.8764,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5375/10000,\n",
      " train_loss: 838.8764,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5376/10000,\n",
      " train_loss: 838.8764,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5377/10000,\n",
      " train_loss: 838.8764,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5378/10000,\n",
      " train_loss: 838.8764,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5379/10000,\n",
      " train_loss: 838.8764,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5380/10000,\n",
      " train_loss: 838.8764,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5381/10000,\n",
      " train_loss: 838.8764,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5382/10000,\n",
      " train_loss: 838.8764,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5383/10000,\n",
      " train_loss: 838.8764,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5384/10000,\n",
      " train_loss: 838.8764,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5385/10000,\n",
      " train_loss: 838.8763,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5386/10000,\n",
      " train_loss: 838.8764,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5387/10000,\n",
      " train_loss: 838.8763,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5388/10000,\n",
      " train_loss: 838.8763,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5389/10000,\n",
      " train_loss: 838.8763,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5390/10000,\n",
      " train_loss: 838.8763,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5391/10000,\n",
      " train_loss: 838.8763,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5392/10000,\n",
      " train_loss: 838.8763,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5393/10000,\n",
      " train_loss: 838.8763,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5394/10000,\n",
      " train_loss: 838.8763,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5395/10000,\n",
      " train_loss: 838.8763,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5396/10000,\n",
      " train_loss: 838.8763,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5397/10000,\n",
      " train_loss: 838.8762,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5398/10000,\n",
      " train_loss: 838.8763,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0083\n",
      "\n",
      "epoch: 5399/10000,\n",
      " train_loss: 838.8762,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0080\n",
      "\n",
      "epoch: 5400/10000,\n",
      " train_loss: 838.8763,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5401/10000,\n",
      " train_loss: 838.8763,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5402/10000,\n",
      " train_loss: 838.8762,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5403/10000,\n",
      " train_loss: 838.8763,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5404/10000,\n",
      " train_loss: 838.8762,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5405/10000,\n",
      " train_loss: 838.8762,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5406/10000,\n",
      " train_loss: 838.8762,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 5407/10000,\n",
      " train_loss: 838.8762,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 5408/10000,\n",
      " train_loss: 838.8763,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 5409/10000,\n",
      " train_loss: 838.8762,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5410/10000,\n",
      " train_loss: 838.8762,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5411/10000,\n",
      " train_loss: 838.8762,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5412/10000,\n",
      " train_loss: 838.8762,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5413/10000,\n",
      " train_loss: 838.8762,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5414/10000,\n",
      " train_loss: 838.8762,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5415/10000,\n",
      " train_loss: 838.8762,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5416/10000,\n",
      " train_loss: 838.8762,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5417/10000,\n",
      " train_loss: 838.8762,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5418/10000,\n",
      " train_loss: 838.8761,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5419/10000,\n",
      " train_loss: 838.8762,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5420/10000,\n",
      " train_loss: 838.8762,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5421/10000,\n",
      " train_loss: 838.8761,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5422/10000,\n",
      " train_loss: 838.8761,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5423/10000,\n",
      " train_loss: 838.8761,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 5424/10000,\n",
      " train_loss: 838.8761,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0130\n",
      "\n",
      "epoch: 5425/10000,\n",
      " train_loss: 838.8761,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0084\n",
      "\n",
      "epoch: 5426/10000,\n",
      " train_loss: 838.8761,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 5427/10000,\n",
      " train_loss: 838.8761,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5428/10000,\n",
      " train_loss: 838.8761,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 5429/10000,\n",
      " train_loss: 838.8761,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5430/10000,\n",
      " train_loss: 838.8761,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5431/10000,\n",
      " train_loss: 838.8761,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5432/10000,\n",
      " train_loss: 838.8760,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5433/10000,\n",
      " train_loss: 838.8761,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5434/10000,\n",
      " train_loss: 838.8760,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5435/10000,\n",
      " train_loss: 838.8760,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5436/10000,\n",
      " train_loss: 838.8760,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5437/10000,\n",
      " train_loss: 838.8760,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5438/10000,\n",
      " train_loss: 838.8760,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5439/10000,\n",
      " train_loss: 838.8760,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5440/10000,\n",
      " train_loss: 838.8760,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5441/10000,\n",
      " train_loss: 838.8760,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5442/10000,\n",
      " train_loss: 838.8760,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5443/10000,\n",
      " train_loss: 838.8760,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5444/10000,\n",
      " train_loss: 838.8760,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5445/10000,\n",
      " train_loss: 838.8760,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5446/10000,\n",
      " train_loss: 838.8759,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5447/10000,\n",
      " train_loss: 838.8760,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5448/10000,\n",
      " train_loss: 838.8760,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5449/10000,\n",
      " train_loss: 838.8759,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5450/10000,\n",
      " train_loss: 838.8760,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5451/10000,\n",
      " train_loss: 838.8760,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5452/10000,\n",
      " train_loss: 838.8760,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5453/10000,\n",
      " train_loss: 838.8759,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0131\n",
      "\n",
      "epoch: 5454/10000,\n",
      " train_loss: 838.8759,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0119\n",
      "\n",
      "epoch: 5455/10000,\n",
      " train_loss: 838.8759,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0122\n",
      "\n",
      "epoch: 5456/10000,\n",
      " train_loss: 838.8759,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5457/10000,\n",
      " train_loss: 838.8759,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 5458/10000,\n",
      " train_loss: 838.8759,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5459/10000,\n",
      " train_loss: 838.8759,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5460/10000,\n",
      " train_loss: 838.8759,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 5461/10000,\n",
      " train_loss: 838.8759,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5462/10000,\n",
      " train_loss: 838.8759,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5463/10000,\n",
      " train_loss: 838.8759,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5464/10000,\n",
      " train_loss: 838.8759,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5465/10000,\n",
      " train_loss: 838.8757,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5466/10000,\n",
      " train_loss: 838.8757,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5467/10000,\n",
      " train_loss: 838.8759,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5468/10000,\n",
      " train_loss: 838.8759,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5469/10000,\n",
      " train_loss: 838.8757,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5470/10000,\n",
      " train_loss: 838.8757,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5471/10000,\n",
      " train_loss: 838.8757,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 5472/10000,\n",
      " train_loss: 838.8757,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5473/10000,\n",
      " train_loss: 838.8757,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5474/10000,\n",
      " train_loss: 838.8757,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5475/10000,\n",
      " train_loss: 838.8757,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5476/10000,\n",
      " train_loss: 838.8757,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5477/10000,\n",
      " train_loss: 838.8757,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5478/10000,\n",
      " train_loss: 838.8757,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 5479/10000,\n",
      " train_loss: 838.8757,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5480/10000,\n",
      " train_loss: 838.8757,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5481/10000,\n",
      " train_loss: 838.8757,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 5482/10000,\n",
      " train_loss: 838.8756,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5483/10000,\n",
      " train_loss: 838.8757,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0114\n",
      "\n",
      "epoch: 5484/10000,\n",
      " train_loss: 838.8756,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0093\n",
      "\n",
      "epoch: 5485/10000,\n",
      " train_loss: 838.8756,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5486/10000,\n",
      " train_loss: 838.8757,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5487/10000,\n",
      " train_loss: 838.8756,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5488/10000,\n",
      " train_loss: 838.8756,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5489/10000,\n",
      " train_loss: 838.8756,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 5490/10000,\n",
      " train_loss: 838.8755,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 5491/10000,\n",
      " train_loss: 838.8755,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 5492/10000,\n",
      " train_loss: 838.8755,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 5493/10000,\n",
      " train_loss: 838.8755,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 5494/10000,\n",
      " train_loss: 838.8755,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5495/10000,\n",
      " train_loss: 838.8755,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5496/10000,\n",
      " train_loss: 838.8755,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5497/10000,\n",
      " train_loss: 838.8755,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 5498/10000,\n",
      " train_loss: 838.8755,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5499/10000,\n",
      " train_loss: 838.8755,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5500/10000,\n",
      " train_loss: 838.8755,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5501/10000,\n",
      " train_loss: 838.8755,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5502/10000,\n",
      " train_loss: 838.8755,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5503/10000,\n",
      " train_loss: 838.8755,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5504/10000,\n",
      " train_loss: 838.8755,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5505/10000,\n",
      " train_loss: 838.8755,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5506/10000,\n",
      " train_loss: 838.8755,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 5507/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0074\n",
      "\n",
      "epoch: 5508/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5509/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5510/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5511/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5512/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5513/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5514/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5515/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5516/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5517/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5518/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 5519/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5520/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5521/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 5522/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5523/10000,\n",
      " train_loss: 838.8753,\n",
      " train_mae: 25.3308,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 5524/10000,\n",
      " train_loss: 838.8753,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5525/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5526/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5527/10000,\n",
      " train_loss: 838.8753,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5528/10000,\n",
      " train_loss: 838.8753,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5529/10000,\n",
      " train_loss: 838.8753,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0095\n",
      "\n",
      "epoch: 5530/10000,\n",
      " train_loss: 838.8754,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0070\n",
      "\n",
      "epoch: 5531/10000,\n",
      " train_loss: 838.8753,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 5532/10000,\n",
      " train_loss: 838.8753,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 5533/10000,\n",
      " train_loss: 838.8753,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 5534/10000,\n",
      " train_loss: 838.8753,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5535/10000,\n",
      " train_loss: 838.8753,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0068\n",
      "\n",
      "epoch: 5536/10000,\n",
      " train_loss: 838.8753,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0073\n",
      "\n",
      "epoch: 5537/10000,\n",
      " train_loss: 838.8753,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5538/10000,\n",
      " train_loss: 838.8753,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5539/10000,\n",
      " train_loss: 838.8752,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 5540/10000,\n",
      " train_loss: 838.8752,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5541/10000,\n",
      " train_loss: 838.8752,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5542/10000,\n",
      " train_loss: 838.8752,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5543/10000,\n",
      " train_loss: 838.8752,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5544/10000,\n",
      " train_loss: 838.8752,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5545/10000,\n",
      " train_loss: 838.8752,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5546/10000,\n",
      " train_loss: 838.8752,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5547/10000,\n",
      " train_loss: 838.8752,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5548/10000,\n",
      " train_loss: 838.8752,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5549/10000,\n",
      " train_loss: 838.8752,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 5550/10000,\n",
      " train_loss: 838.8752,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 5551/10000,\n",
      " train_loss: 838.8752,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5552/10000,\n",
      " train_loss: 838.8752,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5553/10000,\n",
      " train_loss: 838.8752,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5554/10000,\n",
      " train_loss: 838.8752,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 5555/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 5556/10000,\n",
      " train_loss: 838.8752,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5557/10000,\n",
      " train_loss: 838.8752,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5558/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 5559/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 5560/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 5561/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 5562/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5563/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5564/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5565/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0093\n",
      "\n",
      "epoch: 5566/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0099\n",
      "\n",
      "epoch: 5567/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5568/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5569/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5570/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5571/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5572/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5573/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5574/10000,\n",
      " train_loss: 838.8750,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5575/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5576/10000,\n",
      " train_loss: 838.8750,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5577/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5578/10000,\n",
      " train_loss: 838.8750,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5579/10000,\n",
      " train_loss: 838.8750,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5580/10000,\n",
      " train_loss: 838.8750,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5581/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5582/10000,\n",
      " train_loss: 838.8751,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5583/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5584/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5585/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5586/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 5587/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5588/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5589/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5590/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5591/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5592/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5593/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5594/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5595/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0067\n",
      "\n",
      "epoch: 5596/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0078\n",
      "\n",
      "epoch: 5597/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 5598/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0187\n",
      "\n",
      "epoch: 5599/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 5600/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 5601/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5602/10000,\n",
      " train_loss: 838.8749,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 5603/10000,\n",
      " train_loss: 838.8748,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 5604/10000,\n",
      " train_loss: 838.8748,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 5605/10000,\n",
      " train_loss: 838.8748,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 5606/10000,\n",
      " train_loss: 838.8748,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5607/10000,\n",
      " train_loss: 838.8748,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5608/10000,\n",
      " train_loss: 838.8748,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5609/10000,\n",
      " train_loss: 838.8748,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5610/10000,\n",
      " train_loss: 838.8748,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5611/10000,\n",
      " train_loss: 838.8748,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5612/10000,\n",
      " train_loss: 838.8748,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5613/10000,\n",
      " train_loss: 838.8748,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5614/10000,\n",
      " train_loss: 838.8747,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5615/10000,\n",
      " train_loss: 838.8748,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5616/10000,\n",
      " train_loss: 838.8747,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5617/10000,\n",
      " train_loss: 838.8747,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 5618/10000,\n",
      " train_loss: 838.8748,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5619/10000,\n",
      " train_loss: 838.8747,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5620/10000,\n",
      " train_loss: 838.8747,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5621/10000,\n",
      " train_loss: 838.8747,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5622/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 5623/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0100\n",
      "\n",
      "epoch: 5624/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5625/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0075\n",
      "\n",
      "epoch: 5626/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5627/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 5628/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 5629/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 5630/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5631/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 5632/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5633/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5634/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5635/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5636/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5637/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5638/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5639/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5640/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5641/10000,\n",
      " train_loss: 838.8745,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5642/10000,\n",
      " train_loss: 838.8745,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5643/10000,\n",
      " train_loss: 838.8745,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5644/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5645/10000,\n",
      " train_loss: 838.8745,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5646/10000,\n",
      " train_loss: 838.8745,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5647/10000,\n",
      " train_loss: 838.8745,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5648/10000,\n",
      " train_loss: 838.8745,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 5649/10000,\n",
      " train_loss: 838.8746,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0114\n",
      "\n",
      "epoch: 5650/10000,\n",
      " train_loss: 838.8745,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 5651/10000,\n",
      " train_loss: 838.8745,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5652/10000,\n",
      " train_loss: 838.8745,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5653/10000,\n",
      " train_loss: 838.8745,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5654/10000,\n",
      " train_loss: 838.8745,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5655/10000,\n",
      " train_loss: 838.8745,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5656/10000,\n",
      " train_loss: 838.8745,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5657/10000,\n",
      " train_loss: 838.8745,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5658/10000,\n",
      " train_loss: 838.8745,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5659/10000,\n",
      " train_loss: 838.8745,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5660/10000,\n",
      " train_loss: 838.8745,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5661/10000,\n",
      " train_loss: 838.8744,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 5662/10000,\n",
      " train_loss: 838.8744,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 5663/10000,\n",
      " train_loss: 838.8744,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 5664/10000,\n",
      " train_loss: 838.8744,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 5665/10000,\n",
      " train_loss: 838.8744,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5666/10000,\n",
      " train_loss: 838.8744,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5667/10000,\n",
      " train_loss: 838.8744,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5668/10000,\n",
      " train_loss: 838.8744,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5669/10000,\n",
      " train_loss: 838.8744,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5670/10000,\n",
      " train_loss: 838.8744,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5671/10000,\n",
      " train_loss: 838.8744,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5672/10000,\n",
      " train_loss: 838.8744,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5673/10000,\n",
      " train_loss: 838.8744,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5674/10000,\n",
      " train_loss: 838.8744,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5675/10000,\n",
      " train_loss: 838.8744,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5676/10000,\n",
      " train_loss: 838.8744,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5677/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 5678/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5679/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5680/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5681/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5682/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5683/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0090\n",
      "\n",
      "epoch: 5684/10000,\n",
      " train_loss: 838.8744,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0097\n",
      "\n",
      "epoch: 5685/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5686/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5687/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5688/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 5689/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5690/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5691/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5692/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5693/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5694/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5695/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5696/10000,\n",
      " train_loss: 838.8742,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5697/10000,\n",
      " train_loss: 838.8743,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5698/10000,\n",
      " train_loss: 838.8742,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 5699/10000,\n",
      " train_loss: 838.8742,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5700/10000,\n",
      " train_loss: 838.8742,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5701/10000,\n",
      " train_loss: 838.8742,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5702/10000,\n",
      " train_loss: 838.8742,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5703/10000,\n",
      " train_loss: 838.8742,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5704/10000,\n",
      " train_loss: 838.8742,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5705/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5706/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5707/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5708/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5709/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5710/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5711/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5712/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0123\n",
      "\n",
      "epoch: 5713/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 5714/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5715/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5716/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5717/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5718/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5719/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5720/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5721/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5722/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5723/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5724/10000,\n",
      " train_loss: 838.8741,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5725/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5726/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5727/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5728/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5729/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5730/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5731/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5732/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5733/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5734/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5735/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5736/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5737/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 5738/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5739/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5740/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0090\n",
      "\n",
      "epoch: 5741/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 5742/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5743/10000,\n",
      " train_loss: 838.8739,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5744/10000,\n",
      " train_loss: 838.8739,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5745/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5746/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5747/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5748/10000,\n",
      " train_loss: 838.8739,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5749/10000,\n",
      " train_loss: 838.8740,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5750/10000,\n",
      " train_loss: 838.8739,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5751/10000,\n",
      " train_loss: 838.8739,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5752/10000,\n",
      " train_loss: 838.8739,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5753/10000,\n",
      " train_loss: 838.8739,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5754/10000,\n",
      " train_loss: 838.8739,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5755/10000,\n",
      " train_loss: 838.8739,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5756/10000,\n",
      " train_loss: 838.8739,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 5757/10000,\n",
      " train_loss: 838.8738,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5758/10000,\n",
      " train_loss: 838.8738,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5759/10000,\n",
      " train_loss: 838.8738,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5760/10000,\n",
      " train_loss: 838.8739,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5761/10000,\n",
      " train_loss: 838.8739,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5762/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5763/10000,\n",
      " train_loss: 838.8738,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 5764/10000,\n",
      " train_loss: 838.8738,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0076\n",
      "\n",
      "epoch: 5765/10000,\n",
      " train_loss: 838.8738,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0098\n",
      "\n",
      "epoch: 5766/10000,\n",
      " train_loss: 838.8738,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 5767/10000,\n",
      " train_loss: 838.8738,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5768/10000,\n",
      " train_loss: 838.8739,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5769/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5770/10000,\n",
      " train_loss: 838.8738,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5771/10000,\n",
      " train_loss: 838.8738,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0085\n",
      "\n",
      "epoch: 5772/10000,\n",
      " train_loss: 838.8738,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 5773/10000,\n",
      " train_loss: 838.8738,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 5774/10000,\n",
      " train_loss: 838.8738,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 5775/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 5776/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 5777/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5778/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 5779/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5780/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5781/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5782/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5783/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5784/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5785/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5786/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5787/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5788/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5789/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5790/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5791/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5792/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0093\n",
      "\n",
      "epoch: 5793/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5794/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 5795/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 5796/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 5797/10000,\n",
      " train_loss: 838.8737,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 5798/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5799/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5800/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5801/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5802/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5803/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5804/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5805/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5806/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5807/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5808/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5809/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5810/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 5811/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5812/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5813/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5814/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5815/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5816/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0124\n",
      "\n",
      "epoch: 5817/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0069\n",
      "\n",
      "epoch: 5818/10000,\n",
      " train_loss: 838.8736,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5819/10000,\n",
      " train_loss: 838.8735,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5820/10000,\n",
      " train_loss: 838.8735,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5821/10000,\n",
      " train_loss: 838.8735,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5822/10000,\n",
      " train_loss: 838.8735,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5823/10000,\n",
      " train_loss: 838.8735,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5824/10000,\n",
      " train_loss: 838.8735,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5825/10000,\n",
      " train_loss: 838.8735,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5826/10000,\n",
      " train_loss: 838.8735,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5827/10000,\n",
      " train_loss: 838.8735,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5828/10000,\n",
      " train_loss: 838.8735,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5829/10000,\n",
      " train_loss: 838.8735,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0052\n",
      "\n",
      "epoch: 5830/10000,\n",
      " train_loss: 838.8735,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5831/10000,\n",
      " train_loss: 838.8735,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5832/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5833/10000,\n",
      " train_loss: 838.8735,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0075\n",
      "\n",
      "epoch: 5834/10000,\n",
      " train_loss: 838.8735,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5835/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5836/10000,\n",
      " train_loss: 838.8735,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 5837/10000,\n",
      " train_loss: 838.8735,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0189\n",
      "\n",
      "epoch: 5838/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 5839/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 5840/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5841/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5842/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5843/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5844/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5845/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5846/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5847/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5848/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5849/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5850/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5851/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5852/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5853/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5854/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5855/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5856/10000,\n",
      " train_loss: 838.8733,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5857/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5858/10000,\n",
      " train_loss: 838.8734,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5859/10000,\n",
      " train_loss: 838.8733,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5860/10000,\n",
      " train_loss: 838.8733,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 5861/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 5862/10000,\n",
      " train_loss: 838.8733,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5863/10000,\n",
      " train_loss: 838.8733,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5864/10000,\n",
      " train_loss: 838.8733,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5865/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5866/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 5867/10000,\n",
      " train_loss: 838.8733,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0064\n",
      "\n",
      "epoch: 5868/10000,\n",
      " train_loss: 838.8733,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0128\n",
      "\n",
      "epoch: 5869/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0061\n",
      "\n",
      "epoch: 5870/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0076\n",
      "\n",
      "epoch: 5871/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5872/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 5873/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 5874/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 5875/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5876/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5877/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0055\n",
      "\n",
      "epoch: 5878/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5879/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5880/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5881/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5882/10000,\n",
      " train_loss: 838.8731,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5883/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5884/10000,\n",
      " train_loss: 838.8731,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5885/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5886/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5887/10000,\n",
      " train_loss: 838.8731,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5888/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5889/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5890/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5891/10000,\n",
      " train_loss: 838.8731,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5892/10000,\n",
      " train_loss: 838.8731,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5893/10000,\n",
      " train_loss: 838.8731,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5894/10000,\n",
      " train_loss: 838.8731,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5895/10000,\n",
      " train_loss: 838.8732,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 5896/10000,\n",
      " train_loss: 838.8731,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 5897/10000,\n",
      " train_loss: 838.8731,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 5898/10000,\n",
      " train_loss: 838.8731,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5899/10000,\n",
      " train_loss: 838.8731,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0104\n",
      "\n",
      "epoch: 5900/10000,\n",
      " train_loss: 838.8731,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5901/10000,\n",
      " train_loss: 838.8731,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5902/10000,\n",
      " train_loss: 838.8731,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 5903/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0084\n",
      "\n",
      "epoch: 5904/10000,\n",
      " train_loss: 838.8731,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 5905/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 5906/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5907/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5908/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5909/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5910/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5911/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5912/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5913/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0060\n",
      "\n",
      "epoch: 5914/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5915/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 5916/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5917/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5918/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5919/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5920/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5921/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5922/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5923/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 5924/10000,\n",
      " train_loss: 838.8728,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5925/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5926/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5927/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0071\n",
      "\n",
      "epoch: 5928/10000,\n",
      " train_loss: 838.8729,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0105\n",
      "\n",
      "epoch: 5929/10000,\n",
      " train_loss: 838.8730,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5930/10000,\n",
      " train_loss: 838.8729,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5931/10000,\n",
      " train_loss: 838.8729,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5932/10000,\n",
      " train_loss: 838.8729,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5933/10000,\n",
      " train_loss: 838.8729,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5934/10000,\n",
      " train_loss: 838.8729,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 5935/10000,\n",
      " train_loss: 838.8729,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5936/10000,\n",
      " train_loss: 838.8728,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5937/10000,\n",
      " train_loss: 838.8729,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5938/10000,\n",
      " train_loss: 838.8728,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5939/10000,\n",
      " train_loss: 838.8728,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5940/10000,\n",
      " train_loss: 838.8728,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5941/10000,\n",
      " train_loss: 838.8728,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5942/10000,\n",
      " train_loss: 838.8728,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5943/10000,\n",
      " train_loss: 838.8728,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5944/10000,\n",
      " train_loss: 838.8728,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 5945/10000,\n",
      " train_loss: 838.8728,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5946/10000,\n",
      " train_loss: 838.8729,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 5947/10000,\n",
      " train_loss: 838.8728,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5948/10000,\n",
      " train_loss: 838.8728,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 5949/10000,\n",
      " train_loss: 838.8728,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 5950/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5951/10000,\n",
      " train_loss: 838.8728,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0065\n",
      "\n",
      "epoch: 5952/10000,\n",
      " train_loss: 838.8728,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0074\n",
      "\n",
      "epoch: 5953/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0066\n",
      "\n",
      "epoch: 5954/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5955/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5956/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5957/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0075\n",
      "\n",
      "epoch: 5958/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 5959/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 5960/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 5961/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0047\n",
      "\n",
      "epoch: 5962/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5963/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5964/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5965/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5966/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5967/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0043\n",
      "\n",
      "epoch: 5968/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 5969/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5970/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 5971/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0039\n",
      "\n",
      "epoch: 5972/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 5973/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 5974/10000,\n",
      " train_loss: 838.8726,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5975/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5976/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5977/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5978/10000,\n",
      " train_loss: 838.8726,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 5979/10000,\n",
      " train_loss: 838.8726,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5980/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5981/10000,\n",
      " train_loss: 838.8726,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5982/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "epoch: 5983/10000,\n",
      " train_loss: 838.8727,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5984/10000,\n",
      " train_loss: 838.8726,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0057\n",
      "\n",
      "epoch: 5985/10000,\n",
      " train_loss: 838.8726,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0089\n",
      "\n",
      "epoch: 5986/10000,\n",
      " train_loss: 838.8726,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0073\n",
      "\n",
      "epoch: 5987/10000,\n",
      " train_loss: 838.8726,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5988/10000,\n",
      " train_loss: 838.8726,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0075\n",
      "\n",
      "epoch: 5989/10000,\n",
      " train_loss: 838.8726,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 5990/10000,\n",
      " train_loss: 838.8726,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 5991/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 5992/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 5993/10000,\n",
      " train_loss: 838.8726,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 5994/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0049\n",
      "\n",
      "epoch: 5995/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 5996/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 5997/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 5998/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 5999/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 6000/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 6001/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3307,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 6002/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 6003/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 6004/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 6005/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 6006/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 6007/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 6008/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 6009/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 6010/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 6011/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0027\n",
      "\n",
      "epoch: 6012/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 6013/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 6014/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 6015/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 6016/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 6017/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0056\n",
      "\n",
      "epoch: 6018/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0131\n",
      "\n",
      "epoch: 6019/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0045\n",
      "\n",
      "epoch: 6020/10000,\n",
      " train_loss: 838.8725,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0041\n",
      "\n",
      "epoch: 6021/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 6022/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 6023/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 6024/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 6025/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 6026/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 6027/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 6028/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 6029/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 6030/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 6031/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 6032/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 6033/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 6034/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 6035/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0035\n",
      "\n",
      "epoch: 6036/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 6037/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 6038/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 6039/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 6040/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0028\n",
      "\n",
      "epoch: 6041/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0029\n",
      "\n",
      "epoch: 6042/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 6043/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 6044/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 6045/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0201\n",
      "\n",
      "epoch: 6046/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0103\n",
      "\n",
      "epoch: 6047/10000,\n",
      " train_loss: 838.8724,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0051\n",
      "\n",
      "epoch: 6048/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0062\n",
      "\n",
      "epoch: 6049/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0058\n",
      "\n",
      "epoch: 6050/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 6051/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0053\n",
      "\n",
      "epoch: 6052/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0050\n",
      "\n",
      "epoch: 6053/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0048\n",
      "\n",
      "epoch: 6054/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0059\n",
      "\n",
      "epoch: 6055/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0046\n",
      "\n",
      "epoch: 6056/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 6057/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 6058/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 6059/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 6060/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0038\n",
      "\n",
      "epoch: 6061/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 6062/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 6063/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0032\n",
      "\n",
      "epoch: 6064/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0036\n",
      "\n",
      "epoch: 6065/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0030\n",
      "\n",
      "epoch: 6066/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0026\n",
      "\n",
      "epoch: 6067/10000,\n",
      " train_loss: 838.8723,\n",
      " train_mae: 25.3306,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 6068/10000,\n",
      " train_loss: 838.8688,\n",
      " train_mae: 25.3305,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 6069/10000,\n",
      " train_loss: 838.8673,\n",
      " train_mae: 25.3304,\n",
      " epoch_time_duration: 0.0031\n",
      "\n",
      "epoch: 6070/10000,\n",
      " train_loss: 838.8661,\n",
      " train_mae: 25.3304,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 6071/10000,\n",
      " train_loss: 838.8655,\n",
      " train_mae: 25.3304,\n",
      " epoch_time_duration: 0.0034\n",
      "\n",
      "epoch: 6072/10000,\n",
      " train_loss: 838.8653,\n",
      " train_mae: 25.3304,\n",
      " epoch_time_duration: 0.0037\n",
      "\n",
      "epoch: 6073/10000,\n",
      " train_loss: 838.8653,\n",
      " train_mae: 25.3304,\n",
      " epoch_time_duration: 0.0040\n",
      "\n",
      "epoch: 6074/10000,\n",
      " train_loss: 838.8652,\n",
      " train_mae: 25.3304,\n",
      " epoch_time_duration: 0.0054\n",
      "\n",
      "epoch: 6075/10000,\n",
      " train_loss: 838.8652,\n",
      " train_mae: 25.3304,\n",
      " epoch_time_duration: 0.0063\n",
      "\n",
      "epoch: 6076/10000,\n",
      " train_loss: 838.8654,\n",
      " train_mae: 25.3304,\n",
      " epoch_time_duration: 0.0033\n",
      "\n",
      "epoch: 6077/10000,\n",
      " train_loss: 838.8666,\n",
      " train_mae: 25.3437,\n",
      " epoch_time_duration: 0.0042\n",
      "\n",
      "epoch: 6078/10000,\n",
      " train_loss: 839.5973,\n",
      " train_mae: 26.0474,\n",
      " epoch_time_duration: 0.0044\n",
      "\n",
      "early stopping activated\n",
      "== end training ==\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [tensor(5378.2949),\n",
       "  tensor(4373.0112),\n",
       "  tensor(3608.8101),\n",
       "  tensor(2933.8242),\n",
       "  tensor(2374.7063),\n",
       "  tensor(1913.9542),\n",
       "  tensor(1548.0836),\n",
       "  tensor(1272.1437),\n",
       "  tensor(1079.2406),\n",
       "  tensor(960.4120),\n",
       "  tensor(904.7333),\n",
       "  tensor(899.6686),\n",
       "  tensor(931.6901),\n",
       "  tensor(987.1213),\n",
       "  tensor(1053.0928),\n",
       "  tensor(1118.4487),\n",
       "  tensor(1174.4438),\n",
       "  tensor(1215.1248),\n",
       "  tensor(1237.3660),\n",
       "  tensor(1240.6007),\n",
       "  tensor(1226.3479),\n",
       "  tensor(1197.6333),\n",
       "  tensor(1158.4042),\n",
       "  tensor(1112.9894),\n",
       "  tensor(1065.6417),\n",
       "  tensor(1020.1714),\n",
       "  tensor(979.6805),\n",
       "  tensor(946.3939),\n",
       "  tensor(921.5851),\n",
       "  tensor(905.5898),\n",
       "  tensor(897.8991),\n",
       "  tensor(897.3178),\n",
       "  tensor(902.1699),\n",
       "  tensor(910.5287),\n",
       "  tensor(920.4459),\n",
       "  tensor(930.1548),\n",
       "  tensor(938.2309),\n",
       "  tensor(943.6913),\n",
       "  tensor(946.0344),\n",
       "  tensor(945.2186),\n",
       "  tensor(941.5944),\n",
       "  tensor(935.8018),\n",
       "  tensor(928.6495),\n",
       "  tensor(920.9962),\n",
       "  tensor(913.6401),\n",
       "  tensor(907.2347),\n",
       "  tensor(902.2318),\n",
       "  tensor(898.8561),\n",
       "  tensor(897.1130),\n",
       "  tensor(896.8178),\n",
       "  tensor(897.6477),\n",
       "  tensor(899.2006),\n",
       "  tensor(901.0580),\n",
       "  tensor(902.8398),\n",
       "  tensor(904.2454),\n",
       "  tensor(905.0812),\n",
       "  tensor(905.2675),\n",
       "  tensor(904.8304),\n",
       "  tensor(903.8821),\n",
       "  tensor(902.5900),\n",
       "  tensor(901.1459),\n",
       "  tensor(899.7350),\n",
       "  tensor(898.5111),\n",
       "  tensor(897.5797),\n",
       "  tensor(896.9905),\n",
       "  tensor(896.7395),\n",
       "  tensor(896.7774),\n",
       "  tensor(897.0237),\n",
       "  tensor(897.3837),\n",
       "  tensor(897.7636),\n",
       "  tensor(898.0848),\n",
       "  tensor(898.2922),\n",
       "  tensor(898.3590),\n",
       "  tensor(898.2855),\n",
       "  tensor(898.0945),\n",
       "  tensor(897.8248),\n",
       "  tensor(897.5209),\n",
       "  tensor(897.2268),\n",
       "  tensor(896.9779),\n",
       "  tensor(896.7975),\n",
       "  tensor(896.6945),\n",
       "  tensor(896.6647),\n",
       "  tensor(896.6934),\n",
       "  tensor(896.7594),\n",
       "  tensor(896.8398),\n",
       "  tensor(896.9133),\n",
       "  tensor(896.9642),\n",
       "  tensor(896.9833),\n",
       "  tensor(896.9687),\n",
       "  tensor(896.9245),\n",
       "  tensor(896.8597),\n",
       "  tensor(896.7845),\n",
       "  tensor(896.7097),\n",
       "  tensor(896.6436),\n",
       "  tensor(896.5905),\n",
       "  tensor(896.5512),\n",
       "  tensor(896.5206),\n",
       "  tensor(896.4871),\n",
       "  tensor(896.4255),\n",
       "  tensor(896.2666),\n",
       "  tensor(895.7122),\n",
       "  tensor(893.1200),\n",
       "  tensor(885.1932),\n",
       "  tensor(873.7490),\n",
       "  tensor(870.0516),\n",
       "  tensor(870.5014),\n",
       "  tensor(867.5229),\n",
       "  tensor(861.7965),\n",
       "  tensor(864.7092),\n",
       "  tensor(864.0833),\n",
       "  tensor(854.4782),\n",
       "  tensor(856.5946),\n",
       "  tensor(859.1998),\n",
       "  tensor(859.7930),\n",
       "  tensor(857.7511),\n",
       "  tensor(853.3597),\n",
       "  tensor(856.7487),\n",
       "  tensor(858.5410),\n",
       "  tensor(854.1005),\n",
       "  tensor(854.8033),\n",
       "  tensor(856.6354),\n",
       "  tensor(854.5654),\n",
       "  tensor(852.1403),\n",
       "  tensor(853.4782),\n",
       "  tensor(853.5333),\n",
       "  tensor(851.4791),\n",
       "  tensor(851.7664),\n",
       "  tensor(852.8923),\n",
       "  tensor(852.1617),\n",
       "  tensor(851.4123),\n",
       "  tensor(852.1057),\n",
       "  tensor(852.4152),\n",
       "  tensor(851.5861),\n",
       "  tensor(851.1813),\n",
       "  tensor(851.5658),\n",
       "  tensor(851.4209),\n",
       "  tensor(850.7806),\n",
       "  tensor(850.7025),\n",
       "  tensor(850.9678),\n",
       "  tensor(850.8686),\n",
       "  tensor(850.5676),\n",
       "  tensor(850.5908),\n",
       "  tensor(850.7903),\n",
       "  tensor(850.7383),\n",
       "  tensor(850.5205),\n",
       "  tensor(850.4570),\n",
       "  tensor(850.5292),\n",
       "  tensor(850.4819),\n",
       "  tensor(850.3145),\n",
       "  tensor(850.2391),\n",
       "  tensor(850.2735),\n",
       "  tensor(850.2885),\n",
       "  tensor(850.2267),\n",
       "  tensor(850.1678),\n",
       "  tensor(850.1709),\n",
       "  tensor(850.1974),\n",
       "  tensor(850.1793),\n",
       "  tensor(850.1208),\n",
       "  tensor(850.0800),\n",
       "  tensor(850.0727),\n",
       "  tensor(850.0664),\n",
       "  tensor(850.0370),\n",
       "  tensor(849.9974),\n",
       "  tensor(849.9750),\n",
       "  tensor(849.9756),\n",
       "  tensor(849.9736),\n",
       "  tensor(849.9525),\n",
       "  tensor(849.9280),\n",
       "  tensor(849.9171),\n",
       "  tensor(849.9117),\n",
       "  tensor(849.8975),\n",
       "  tensor(849.8740),\n",
       "  tensor(849.8530),\n",
       "  tensor(849.8417),\n",
       "  tensor(849.8331),\n",
       "  tensor(849.8188),\n",
       "  tensor(849.8013),\n",
       "  tensor(849.7888),\n",
       "  tensor(849.7803),\n",
       "  tensor(849.7695),\n",
       "  tensor(849.7550),\n",
       "  tensor(849.7389),\n",
       "  tensor(849.7258),\n",
       "  tensor(849.7150),\n",
       "  tensor(849.7018),\n",
       "  tensor(849.6865),\n",
       "  tensor(849.6728),\n",
       "  tensor(849.6615),\n",
       "  tensor(849.6494),\n",
       "  tensor(849.6364),\n",
       "  tensor(849.6221),\n",
       "  tensor(849.6096),\n",
       "  tensor(849.5977),\n",
       "  tensor(849.5837),\n",
       "  tensor(849.5697),\n",
       "  tensor(849.5560),\n",
       "  tensor(849.5429),\n",
       "  tensor(849.5292),\n",
       "  tensor(849.5147),\n",
       "  tensor(849.5012),\n",
       "  tensor(849.4873),\n",
       "  tensor(849.4738),\n",
       "  tensor(849.4594),\n",
       "  tensor(849.4448),\n",
       "  tensor(849.4305),\n",
       "  tensor(849.4160),\n",
       "  tensor(849.4007),\n",
       "  tensor(849.3856),\n",
       "  tensor(849.3711),\n",
       "  tensor(849.3560),\n",
       "  tensor(849.3405),\n",
       "  tensor(849.3253),\n",
       "  tensor(849.3098),\n",
       "  tensor(849.2939),\n",
       "  tensor(849.2778),\n",
       "  tensor(849.2621),\n",
       "  tensor(849.2457),\n",
       "  tensor(849.2291),\n",
       "  tensor(849.2126),\n",
       "  tensor(849.1959),\n",
       "  tensor(849.1791),\n",
       "  tensor(849.1620),\n",
       "  tensor(849.1448),\n",
       "  tensor(849.1271),\n",
       "  tensor(849.1099),\n",
       "  tensor(849.0915),\n",
       "  tensor(849.0737),\n",
       "  tensor(849.0557),\n",
       "  tensor(849.0374),\n",
       "  tensor(849.0189),\n",
       "  tensor(849.0002),\n",
       "  tensor(848.9812),\n",
       "  tensor(848.9623),\n",
       "  tensor(848.9433),\n",
       "  tensor(848.9235),\n",
       "  tensor(848.9047),\n",
       "  tensor(848.8844),\n",
       "  tensor(848.8645),\n",
       "  tensor(848.8450),\n",
       "  tensor(848.8243),\n",
       "  tensor(848.8040),\n",
       "  tensor(848.7834),\n",
       "  tensor(848.7628),\n",
       "  tensor(848.7416),\n",
       "  tensor(848.7206),\n",
       "  tensor(848.6993),\n",
       "  tensor(848.6780),\n",
       "  tensor(848.6562),\n",
       "  tensor(848.6345),\n",
       "  tensor(848.6125),\n",
       "  tensor(848.5901),\n",
       "  tensor(848.5680),\n",
       "  tensor(848.5456),\n",
       "  tensor(848.5232),\n",
       "  tensor(848.5009),\n",
       "  tensor(848.4779),\n",
       "  tensor(848.4548),\n",
       "  tensor(848.4319),\n",
       "  tensor(848.4088),\n",
       "  tensor(848.3853),\n",
       "  tensor(848.3620),\n",
       "  tensor(848.3382),\n",
       "  tensor(848.3146),\n",
       "  tensor(848.2905),\n",
       "  tensor(848.2667),\n",
       "  tensor(848.2430),\n",
       "  tensor(848.2187),\n",
       "  tensor(848.1947),\n",
       "  tensor(848.1706),\n",
       "  tensor(848.1458),\n",
       "  tensor(848.1213),\n",
       "  tensor(848.0968),\n",
       "  tensor(848.0722),\n",
       "  tensor(848.0474),\n",
       "  tensor(848.0225),\n",
       "  tensor(847.9977),\n",
       "  tensor(847.9730),\n",
       "  tensor(847.9478),\n",
       "  tensor(847.9231),\n",
       "  tensor(847.8984),\n",
       "  tensor(847.8730),\n",
       "  tensor(847.8479),\n",
       "  tensor(847.8229),\n",
       "  tensor(847.7980),\n",
       "  tensor(847.7729),\n",
       "  tensor(847.7477),\n",
       "  tensor(847.7221),\n",
       "  tensor(847.6971),\n",
       "  tensor(847.6721),\n",
       "  tensor(847.6474),\n",
       "  tensor(847.6217),\n",
       "  tensor(847.5967),\n",
       "  tensor(847.5717),\n",
       "  tensor(847.5466),\n",
       "  tensor(847.5217),\n",
       "  tensor(847.4965),\n",
       "  tensor(847.4718),\n",
       "  tensor(847.4467),\n",
       "  tensor(847.4221),\n",
       "  tensor(847.3972),\n",
       "  tensor(847.3725),\n",
       "  tensor(847.3481),\n",
       "  tensor(847.3234),\n",
       "  tensor(847.2991),\n",
       "  tensor(847.2745),\n",
       "  tensor(847.2503),\n",
       "  tensor(847.2260),\n",
       "  tensor(847.2019),\n",
       "  tensor(847.1777),\n",
       "  tensor(847.1537),\n",
       "  tensor(847.1301),\n",
       "  tensor(847.1063),\n",
       "  tensor(847.0827),\n",
       "  tensor(847.0591),\n",
       "  tensor(847.0358),\n",
       "  tensor(847.0121),\n",
       "  tensor(846.9893),\n",
       "  tensor(846.9661),\n",
       "  tensor(846.9434),\n",
       "  tensor(846.9203),\n",
       "  tensor(846.8975),\n",
       "  tensor(846.8755),\n",
       "  tensor(846.8527),\n",
       "  tensor(846.8305),\n",
       "  tensor(846.8087),\n",
       "  tensor(846.7864),\n",
       "  tensor(846.7649),\n",
       "  tensor(846.7433),\n",
       "  tensor(846.7213),\n",
       "  tensor(846.6998),\n",
       "  tensor(846.6787),\n",
       "  tensor(846.6575),\n",
       "  tensor(846.6367),\n",
       "  tensor(846.6158),\n",
       "  tensor(846.5952),\n",
       "  tensor(846.5748),\n",
       "  tensor(846.5546),\n",
       "  tensor(846.5342),\n",
       "  tensor(846.5142),\n",
       "  tensor(846.4944),\n",
       "  tensor(846.4744),\n",
       "  tensor(846.4550),\n",
       "  tensor(846.4356),\n",
       "  tensor(846.4166),\n",
       "  tensor(846.3973),\n",
       "  tensor(846.3783),\n",
       "  tensor(846.3595),\n",
       "  tensor(846.3411),\n",
       "  tensor(846.3226),\n",
       "  tensor(846.3040),\n",
       "  tensor(846.2860),\n",
       "  tensor(846.2680),\n",
       "  tensor(846.2498),\n",
       "  tensor(846.2322),\n",
       "  tensor(846.2148),\n",
       "  tensor(846.1975),\n",
       "  tensor(846.1801),\n",
       "  tensor(846.1632),\n",
       "  tensor(846.1462),\n",
       "  tensor(846.1292),\n",
       "  tensor(846.1125),\n",
       "  tensor(846.0961),\n",
       "  tensor(846.0801),\n",
       "  tensor(846.0638),\n",
       "  tensor(846.0476),\n",
       "  tensor(846.0317),\n",
       "  tensor(846.0159),\n",
       "  tensor(846.0002),\n",
       "  tensor(845.9847),\n",
       "  tensor(845.9694),\n",
       "  tensor(845.9543),\n",
       "  tensor(845.9391),\n",
       "  tensor(845.9244),\n",
       "  tensor(845.9094),\n",
       "  tensor(845.8948),\n",
       "  tensor(845.8801),\n",
       "  tensor(845.8657),\n",
       "  tensor(845.8515),\n",
       "  tensor(845.8375),\n",
       "  tensor(845.8232),\n",
       "  tensor(845.8095),\n",
       "  tensor(845.7958),\n",
       "  tensor(845.7821),\n",
       "  tensor(845.7686),\n",
       "  tensor(845.7548),\n",
       "  tensor(845.7418),\n",
       "  tensor(845.7286),\n",
       "  tensor(845.7157),\n",
       "  tensor(845.7026),\n",
       "  tensor(845.6900),\n",
       "  tensor(845.6771),\n",
       "  tensor(845.6645),\n",
       "  tensor(845.6523),\n",
       "  tensor(845.6398),\n",
       "  tensor(845.6275),\n",
       "  tensor(845.6154),\n",
       "  tensor(845.6035),\n",
       "  tensor(845.5912),\n",
       "  tensor(845.5795),\n",
       "  tensor(845.5676),\n",
       "  tensor(845.5561),\n",
       "  tensor(845.5446),\n",
       "  tensor(845.5331),\n",
       "  tensor(845.5217),\n",
       "  tensor(845.5108),\n",
       "  tensor(845.4991),\n",
       "  tensor(845.4882),\n",
       "  tensor(845.4772),\n",
       "  tensor(845.4664),\n",
       "  tensor(845.4556),\n",
       "  tensor(845.4450),\n",
       "  tensor(845.4344),\n",
       "  tensor(845.4238),\n",
       "  tensor(845.4133),\n",
       "  tensor(845.4030),\n",
       "  tensor(845.3928),\n",
       "  tensor(845.3825),\n",
       "  tensor(845.3725),\n",
       "  tensor(845.3625),\n",
       "  tensor(845.3524),\n",
       "  tensor(845.3423),\n",
       "  tensor(845.3329),\n",
       "  tensor(845.3228),\n",
       "  tensor(845.3135),\n",
       "  tensor(845.3037),\n",
       "  tensor(845.2943),\n",
       "  tensor(845.2850),\n",
       "  tensor(845.2755),\n",
       "  tensor(845.2661),\n",
       "  tensor(845.2569),\n",
       "  tensor(845.2477),\n",
       "  tensor(845.2386),\n",
       "  tensor(845.2296),\n",
       "  tensor(845.2205),\n",
       "  tensor(845.2115),\n",
       "  tensor(845.2030),\n",
       "  tensor(845.1940),\n",
       "  tensor(845.1850),\n",
       "  tensor(845.1765),\n",
       "  tensor(845.1682),\n",
       "  tensor(845.1594),\n",
       "  tensor(845.1511),\n",
       "  tensor(845.1426),\n",
       "  tensor(845.1342),\n",
       "  tensor(845.1260),\n",
       "  tensor(845.1175),\n",
       "  tensor(845.1094),\n",
       "  tensor(845.1010),\n",
       "  tensor(845.0928),\n",
       "  tensor(845.0848),\n",
       "  tensor(845.0769),\n",
       "  tensor(845.0688),\n",
       "  tensor(845.0611),\n",
       "  tensor(845.0529),\n",
       "  tensor(845.0451),\n",
       "  tensor(845.0374),\n",
       "  tensor(845.0295),\n",
       "  tensor(845.0218),\n",
       "  tensor(845.0142),\n",
       "  tensor(845.0065),\n",
       "  tensor(844.9990),\n",
       "  tensor(844.9916),\n",
       "  tensor(844.9840),\n",
       "  tensor(844.9766),\n",
       "  tensor(844.9692),\n",
       "  tensor(844.9620),\n",
       "  tensor(844.9543),\n",
       "  tensor(844.9473),\n",
       "  tensor(844.9399),\n",
       "  tensor(844.9326),\n",
       "  tensor(844.9254),\n",
       "  tensor(844.9183),\n",
       "  tensor(844.9111),\n",
       "  tensor(844.9042),\n",
       "  tensor(844.8971),\n",
       "  tensor(844.8901),\n",
       "  tensor(844.8831),\n",
       "  tensor(844.8762),\n",
       "  tensor(844.8692),\n",
       "  tensor(844.8623),\n",
       "  tensor(844.8552),\n",
       "  tensor(844.8486),\n",
       "  tensor(844.8419),\n",
       "  tensor(844.8350),\n",
       "  tensor(844.8283),\n",
       "  tensor(844.8217),\n",
       "  tensor(844.8149),\n",
       "  tensor(844.8082),\n",
       "  tensor(844.8015),\n",
       "  tensor(844.7952),\n",
       "  tensor(844.7885),\n",
       "  tensor(844.7819),\n",
       "  tensor(844.7753),\n",
       "  tensor(844.7689),\n",
       "  tensor(844.7623),\n",
       "  tensor(844.7559),\n",
       "  tensor(844.7494),\n",
       "  tensor(844.7432),\n",
       "  tensor(844.7366),\n",
       "  tensor(844.7305),\n",
       "  tensor(844.7238),\n",
       "  tensor(844.7175),\n",
       "  tensor(844.7111),\n",
       "  tensor(844.7051),\n",
       "  tensor(844.6986),\n",
       "  tensor(844.6924),\n",
       "  tensor(844.6861),\n",
       "  tensor(844.6799),\n",
       "  tensor(844.6738),\n",
       "  tensor(844.6675),\n",
       "  tensor(844.6615),\n",
       "  tensor(844.6553),\n",
       "  tensor(844.6494),\n",
       "  tensor(844.6431),\n",
       "  tensor(844.6369),\n",
       "  tensor(844.6310),\n",
       "  tensor(844.6248),\n",
       "  tensor(844.6190),\n",
       "  tensor(844.6127),\n",
       "  tensor(844.6067),\n",
       "  tensor(844.6008),\n",
       "  tensor(844.5947),\n",
       "  tensor(844.5888),\n",
       "  tensor(844.5828),\n",
       "  tensor(844.5770),\n",
       "  tensor(844.5709),\n",
       "  tensor(844.5650),\n",
       "  tensor(844.5590),\n",
       "  tensor(844.5530),\n",
       "  tensor(844.5472),\n",
       "  tensor(844.5413),\n",
       "  tensor(844.5355),\n",
       "  tensor(844.5295),\n",
       "  tensor(844.5238),\n",
       "  tensor(844.5180),\n",
       "  tensor(844.5121),\n",
       "  tensor(844.5061),\n",
       "  tensor(844.5003),\n",
       "  tensor(844.4946),\n",
       "  tensor(844.4888),\n",
       "  tensor(844.4830),\n",
       "  tensor(844.4771),\n",
       "  tensor(844.4713),\n",
       "  tensor(844.4653),\n",
       "  tensor(844.4598),\n",
       "  tensor(844.4541),\n",
       "  tensor(844.4484),\n",
       "  tensor(844.4427),\n",
       "  tensor(844.4370),\n",
       "  tensor(844.4310),\n",
       "  tensor(844.4254),\n",
       "  tensor(844.4197),\n",
       "  tensor(844.4138),\n",
       "  tensor(844.4083),\n",
       "  tensor(844.4025),\n",
       "  tensor(844.3969),\n",
       "  tensor(844.3912),\n",
       "  tensor(844.3852),\n",
       "  tensor(844.3798),\n",
       "  tensor(844.3741),\n",
       "  tensor(844.3684),\n",
       "  tensor(844.3625),\n",
       "  tensor(844.3569),\n",
       "  tensor(844.3513),\n",
       "  tensor(844.3453),\n",
       "  tensor(844.3400),\n",
       "  tensor(844.3343),\n",
       "  tensor(844.3286),\n",
       "  tensor(844.3231),\n",
       "  tensor(844.3174),\n",
       "  tensor(844.3119),\n",
       "  tensor(844.3062),\n",
       "  tensor(844.3007),\n",
       "  tensor(844.2948),\n",
       "  tensor(844.2892),\n",
       "  tensor(844.2838),\n",
       "  tensor(844.2779),\n",
       "  tensor(844.2725),\n",
       "  tensor(844.2665),\n",
       "  tensor(844.2609),\n",
       "  tensor(844.2554),\n",
       "  tensor(844.2499),\n",
       "  tensor(844.2443),\n",
       "  tensor(844.2387),\n",
       "  tensor(844.2327),\n",
       "  tensor(844.2271),\n",
       "  tensor(844.2217),\n",
       "  tensor(844.2160),\n",
       "  tensor(844.2103),\n",
       "  tensor(844.2050),\n",
       "  tensor(844.1992),\n",
       "  tensor(844.1933),\n",
       "  tensor(844.1880),\n",
       "  tensor(844.1822),\n",
       "  tensor(844.1766),\n",
       "  tensor(844.1708),\n",
       "  tensor(844.1653),\n",
       "  tensor(844.1596),\n",
       "  tensor(844.1542),\n",
       "  tensor(844.1483),\n",
       "  tensor(844.1430),\n",
       "  tensor(844.1371),\n",
       "  tensor(844.1317),\n",
       "  tensor(844.1260),\n",
       "  tensor(844.1205),\n",
       "  tensor(844.1148),\n",
       "  tensor(844.1090),\n",
       "  tensor(844.1033),\n",
       "  tensor(844.0978),\n",
       "  tensor(844.0922),\n",
       "  tensor(844.0866),\n",
       "  tensor(844.0806),\n",
       "  tensor(844.0753),\n",
       "  tensor(844.0697),\n",
       "  tensor(844.0638),\n",
       "  tensor(844.0583),\n",
       "  tensor(844.0527),\n",
       "  tensor(844.0471),\n",
       "  tensor(844.0414),\n",
       "  tensor(844.0360),\n",
       "  tensor(844.0300),\n",
       "  tensor(844.0245),\n",
       "  tensor(844.0189),\n",
       "  tensor(844.0131),\n",
       "  tensor(844.0073),\n",
       "  tensor(844.0014),\n",
       "  tensor(843.9962),\n",
       "  tensor(843.9904),\n",
       "  tensor(843.9848),\n",
       "  tensor(843.9791),\n",
       "  tensor(843.9735),\n",
       "  tensor(843.9677),\n",
       "  tensor(843.9621),\n",
       "  tensor(843.9563),\n",
       "  tensor(843.9508),\n",
       "  tensor(843.9449),\n",
       "  tensor(843.9396),\n",
       "  tensor(843.9335),\n",
       "  tensor(843.9279),\n",
       "  tensor(843.9227),\n",
       "  tensor(843.9163),\n",
       "  tensor(843.9108),\n",
       "  tensor(843.9052),\n",
       "  tensor(843.8996),\n",
       "  tensor(843.8941),\n",
       "  tensor(843.8882),\n",
       "  tensor(843.8823),\n",
       "  tensor(843.8768),\n",
       "  tensor(843.8707),\n",
       "  tensor(843.8654),\n",
       "  tensor(843.8595),\n",
       "  tensor(843.8539),\n",
       "  tensor(843.8481),\n",
       "  tensor(843.8424),\n",
       "  tensor(843.8368),\n",
       "  tensor(843.8307),\n",
       "  tensor(843.8254),\n",
       "  tensor(843.8195),\n",
       "  tensor(843.8137),\n",
       "  tensor(843.8077),\n",
       "  tensor(843.8020),\n",
       "  tensor(843.7963),\n",
       "  tensor(843.7905),\n",
       "  tensor(843.7851),\n",
       "  tensor(843.7794),\n",
       "  tensor(843.7734),\n",
       "  tensor(843.7677),\n",
       "  tensor(843.7618),\n",
       "  tensor(843.7562),\n",
       "  tensor(843.7505),\n",
       "  tensor(843.7443),\n",
       "  tensor(843.7389),\n",
       "  tensor(843.7330),\n",
       "  tensor(843.7271),\n",
       "  tensor(843.7211),\n",
       "  tensor(843.7158),\n",
       "  tensor(843.7098),\n",
       "  tensor(843.7038),\n",
       "  tensor(843.6984),\n",
       "  tensor(843.6921),\n",
       "  tensor(843.6862),\n",
       "  tensor(843.6807),\n",
       "  tensor(843.6747),\n",
       "  tensor(843.6691),\n",
       "  tensor(843.6633),\n",
       "  tensor(843.6572),\n",
       "  tensor(843.6516),\n",
       "  tensor(843.6456),\n",
       "  tensor(843.6400),\n",
       "  tensor(843.6340),\n",
       "  tensor(843.6282),\n",
       "  tensor(843.6225),\n",
       "  tensor(843.6164),\n",
       "  tensor(843.6104),\n",
       "  tensor(843.6045),\n",
       "  tensor(843.5991),\n",
       "  tensor(843.5929),\n",
       "  tensor(843.5873),\n",
       "  tensor(843.5815),\n",
       "  tensor(843.5756),\n",
       "  tensor(843.5697),\n",
       "  tensor(843.5638),\n",
       "  tensor(843.5579),\n",
       "  tensor(843.5519),\n",
       "  tensor(843.5459),\n",
       "  tensor(843.5403),\n",
       "  tensor(843.5342),\n",
       "  tensor(843.5287),\n",
       "  tensor(843.5229),\n",
       "  tensor(843.5166),\n",
       "  tensor(843.5104),\n",
       "  tensor(843.5049),\n",
       "  tensor(843.4990),\n",
       "  tensor(843.4929),\n",
       "  tensor(843.4872),\n",
       "  tensor(843.4811),\n",
       "  tensor(843.4752),\n",
       "  tensor(843.4694),\n",
       "  tensor(843.4640),\n",
       "  tensor(843.4576),\n",
       "  tensor(843.4520),\n",
       "  tensor(843.4457),\n",
       "  tensor(843.4403),\n",
       "  tensor(843.4339),\n",
       "  tensor(843.4280),\n",
       "  tensor(843.4223),\n",
       "  tensor(843.4161),\n",
       "  tensor(843.4103),\n",
       "  tensor(843.4043),\n",
       "  tensor(843.3984),\n",
       "  tensor(843.3925),\n",
       "  tensor(843.3864),\n",
       "  tensor(843.3804),\n",
       "  tensor(843.3746),\n",
       "  tensor(843.3684),\n",
       "  tensor(843.3625),\n",
       "  tensor(843.3564),\n",
       "  tensor(843.3506),\n",
       "  tensor(843.3448),\n",
       "  tensor(843.3386),\n",
       "  tensor(843.3327),\n",
       "  tensor(843.3270),\n",
       "  tensor(843.3206),\n",
       "  tensor(843.3150),\n",
       "  tensor(843.3089),\n",
       "  tensor(843.3030),\n",
       "  tensor(843.2968),\n",
       "  tensor(843.2911),\n",
       "  tensor(843.2848),\n",
       "  tensor(843.2787),\n",
       "  tensor(843.2731),\n",
       "  tensor(843.2668),\n",
       "  tensor(843.2607),\n",
       "  tensor(843.2549),\n",
       "  tensor(843.2491),\n",
       "  tensor(843.2431),\n",
       "  tensor(843.2369),\n",
       "  tensor(843.2310),\n",
       "  tensor(843.2250),\n",
       "  tensor(843.2188),\n",
       "  tensor(843.2128),\n",
       "  tensor(843.2070),\n",
       "  tensor(843.2009),\n",
       "  tensor(843.1947),\n",
       "  tensor(843.1888),\n",
       "  tensor(843.1826),\n",
       "  tensor(843.1768),\n",
       "  tensor(843.1710),\n",
       "  tensor(843.1649),\n",
       "  tensor(843.1586),\n",
       "  tensor(843.1529),\n",
       "  tensor(843.1469),\n",
       "  tensor(843.1409),\n",
       "  tensor(843.1348),\n",
       "  tensor(843.1289),\n",
       "  tensor(843.1227),\n",
       "  tensor(843.1166),\n",
       "  tensor(843.1105),\n",
       "  tensor(843.1044),\n",
       "  tensor(843.0990),\n",
       "  tensor(843.0923),\n",
       "  tensor(843.0865),\n",
       "  tensor(843.0805),\n",
       "  tensor(843.0744),\n",
       "  tensor(843.0684),\n",
       "  tensor(843.0626),\n",
       "  tensor(843.0561),\n",
       "  tensor(843.0505),\n",
       "  tensor(843.0442),\n",
       "  tensor(843.0381),\n",
       "  tensor(843.0319),\n",
       "  tensor(843.0262),\n",
       "  tensor(843.0200),\n",
       "  tensor(843.0141),\n",
       "  tensor(843.0079),\n",
       "  tensor(843.0018),\n",
       "  tensor(842.9957),\n",
       "  tensor(842.9897),\n",
       "  tensor(842.9836),\n",
       "  tensor(842.9779),\n",
       "  tensor(842.9713),\n",
       "  tensor(842.9654),\n",
       "  tensor(842.9591),\n",
       "  tensor(842.9534),\n",
       "  tensor(842.9476),\n",
       "  tensor(842.9413),\n",
       "  tensor(842.9352),\n",
       "  tensor(842.9291),\n",
       "  tensor(842.9230),\n",
       "  tensor(842.9172),\n",
       "  tensor(842.9111),\n",
       "  tensor(842.9052),\n",
       "  tensor(842.8990),\n",
       "  tensor(842.8928),\n",
       "  tensor(842.8867),\n",
       "  tensor(842.8805),\n",
       "  tensor(842.8748),\n",
       "  tensor(842.8685),\n",
       "  tensor(842.8624),\n",
       "  tensor(842.8564),\n",
       "  tensor(842.8506),\n",
       "  tensor(842.8443),\n",
       "  tensor(842.8385),\n",
       "  tensor(842.8323),\n",
       "  tensor(842.8262),\n",
       "  tensor(842.8201),\n",
       "  tensor(842.8139),\n",
       "  tensor(842.8079),\n",
       "  tensor(842.8018),\n",
       "  tensor(842.7959),\n",
       "  tensor(842.7899),\n",
       "  tensor(842.7836),\n",
       "  tensor(842.7775),\n",
       "  tensor(842.7714),\n",
       "  tensor(842.7653),\n",
       "  tensor(842.7594),\n",
       "  tensor(842.7535),\n",
       "  tensor(842.7469),\n",
       "  tensor(842.7412),\n",
       "  tensor(842.7348),\n",
       "  tensor(842.7289),\n",
       "  tensor(842.7230),\n",
       "  tensor(842.7167),\n",
       "  tensor(842.7110),\n",
       "  tensor(842.7048),\n",
       "  tensor(842.6989),\n",
       "  tensor(842.6926),\n",
       "  tensor(842.6864),\n",
       "  tensor(842.6802),\n",
       "  tensor(842.6739),\n",
       "  tensor(842.6684),\n",
       "  tensor(842.6622),\n",
       "  tensor(842.6564),\n",
       "  tensor(842.6504),\n",
       "  tensor(842.6442),\n",
       "  tensor(842.6379),\n",
       "  tensor(842.6319),\n",
       "  tensor(842.6261),\n",
       "  tensor(842.6200),\n",
       "  tensor(842.6139),\n",
       "  tensor(842.6080),\n",
       "  tensor(842.6017),\n",
       "  tensor(842.5956),\n",
       "  tensor(842.5894),\n",
       "  tensor(842.5834),\n",
       "  tensor(842.5771),\n",
       "  tensor(842.5713),\n",
       "  tensor(842.5654),\n",
       "  tensor(842.5591),\n",
       "  tensor(842.5532),\n",
       "  tensor(842.5471),\n",
       "  tensor(842.5411),\n",
       "  tensor(842.5349),\n",
       "  tensor(842.5289),\n",
       "  tensor(842.5231),\n",
       "  tensor(842.5170),\n",
       "  tensor(842.5107),\n",
       "  tensor(842.5051),\n",
       "  tensor(842.4991),\n",
       "  tensor(842.4927),\n",
       "  tensor(842.4869),\n",
       "  tensor(842.4809),\n",
       "  tensor(842.4745),\n",
       "  tensor(842.4688),\n",
       "  tensor(842.4625),\n",
       "  tensor(842.4564),\n",
       "  tensor(842.4503),\n",
       "  tensor(842.4446),\n",
       "  tensor(842.4384),\n",
       "  tensor(842.4322),\n",
       "  tensor(842.4265),\n",
       "  tensor(842.4201),\n",
       "  tensor(842.4140),\n",
       "  tensor(842.4081),\n",
       "  tensor(842.4023),\n",
       "  tensor(842.3964),\n",
       "  tensor(842.3898),\n",
       "  tensor(842.3839),\n",
       "  tensor(842.3779),\n",
       "  tensor(842.3717),\n",
       "  tensor(842.3658),\n",
       "  tensor(842.3599),\n",
       "  tensor(842.3542),\n",
       "  tensor(842.3482),\n",
       "  tensor(842.3420),\n",
       "  tensor(842.3359),\n",
       "  tensor(842.3302),\n",
       "  tensor(842.3236),\n",
       "  tensor(842.3181),\n",
       "  tensor(842.3120),\n",
       "  tensor(842.3058),\n",
       "  tensor(842.2998),\n",
       "  tensor(842.2935),\n",
       "  tensor(842.2878),\n",
       "  tensor(842.2819),\n",
       "  tensor(842.2759),\n",
       "  tensor(842.2701),\n",
       "  tensor(842.2641),\n",
       "  tensor(842.2576),\n",
       "  tensor(842.2521),\n",
       "  tensor(842.2457),\n",
       "  tensor(842.2399),\n",
       "  tensor(842.2338),\n",
       "  tensor(842.2278),\n",
       "  tensor(842.2219),\n",
       "  tensor(842.2162),\n",
       "  tensor(842.2101),\n",
       "  tensor(842.2042),\n",
       "  tensor(842.1984),\n",
       "  tensor(842.1924),\n",
       "  tensor(842.1862),\n",
       "  tensor(842.1802),\n",
       "  tensor(842.1742),\n",
       "  tensor(842.1680),\n",
       "  tensor(842.1627),\n",
       "  tensor(842.1563),\n",
       "  tensor(842.1500),\n",
       "  tensor(842.1443),\n",
       "  tensor(842.1384),\n",
       "  tensor(842.1328),\n",
       "  tensor(842.1264),\n",
       "  tensor(842.1205),\n",
       "  tensor(842.1146),\n",
       "  tensor(842.1086),\n",
       "  tensor(842.1027),\n",
       "  tensor(842.0971),\n",
       "  tensor(842.0911),\n",
       "  tensor(842.0850),\n",
       "  tensor(842.0788),\n",
       "  tensor(842.0734),\n",
       "  tensor(842.0675),\n",
       "  tensor(842.0615),\n",
       "  tensor(842.0552),\n",
       "  tensor(842.0494),\n",
       "  tensor(842.0438),\n",
       "  tensor(842.0375),\n",
       "  tensor(842.0320),\n",
       "  tensor(842.0261),\n",
       "  tensor(842.0201),\n",
       "  tensor(842.0141),\n",
       "  tensor(842.0085),\n",
       "  tensor(842.0026),\n",
       "  tensor(841.9966),\n",
       "  tensor(841.9905),\n",
       "  tensor(841.9852),\n",
       "  tensor(841.9786),\n",
       "  tensor(841.9732),\n",
       "  tensor(841.9673),\n",
       "  tensor(841.9614),\n",
       "  tensor(841.9554),\n",
       "  tensor(841.9498),\n",
       "  tensor(841.9437),\n",
       "  tensor(841.9378),\n",
       "  tensor(841.9321),\n",
       "  tensor(841.9262),\n",
       "  tensor(841.9207),\n",
       "  tensor(841.9152),\n",
       "  tensor(841.9088),\n",
       "  tensor(841.9030),\n",
       "  tensor(841.8976),\n",
       "  tensor(841.8915),\n",
       "  tensor(841.8856),\n",
       "  tensor(841.8798),\n",
       "  tensor(841.8741),\n",
       "  tensor(841.8682),\n",
       "  tensor(841.8625),\n",
       "  tensor(841.8566),\n",
       "  tensor(841.8510),\n",
       "  tensor(841.8450),\n",
       "  tensor(841.8392),\n",
       "  tensor(841.8339),\n",
       "  tensor(841.8278),\n",
       "  tensor(841.8220),\n",
       "  tensor(841.8163),\n",
       "  tensor(841.8104),\n",
       "  tensor(841.8050),\n",
       "  tensor(841.7991),\n",
       "  tensor(841.7930),\n",
       "  tensor(841.7874),\n",
       "  tensor(841.7816),\n",
       "  ...],\n",
       " 'train_mae': [tensor(59.1030),\n",
       "  tensor(52.4354),\n",
       "  tensor(46.4671),\n",
       "  tensor(41.5882),\n",
       "  tensor(37.6855),\n",
       "  tensor(34.3410),\n",
       "  tensor(31.4919),\n",
       "  tensor(29.1906),\n",
       "  tensor(27.6272),\n",
       "  tensor(26.8701),\n",
       "  tensor(26.4361),\n",
       "  tensor(26.1699),\n",
       "  tensor(25.9395),\n",
       "  tensor(25.8003),\n",
       "  tensor(25.8521),\n",
       "  tensor(26.0728),\n",
       "  tensor(26.3219),\n",
       "  tensor(26.4862),\n",
       "  tensor(26.5097),\n",
       "  tensor(26.4055),\n",
       "  tensor(26.2112),\n",
       "  tensor(25.9928),\n",
       "  tensor(25.8440),\n",
       "  tensor(25.8067),\n",
       "  tensor(25.8414),\n",
       "  tensor(25.9701),\n",
       "  tensor(26.1025),\n",
       "  tensor(26.2349),\n",
       "  tensor(26.3641),\n",
       "  tensor(26.4870),\n",
       "  tensor(26.6469),\n",
       "  tensor(26.8023),\n",
       "  tensor(26.9827),\n",
       "  tensor(27.1339),\n",
       "  tensor(27.2673),\n",
       "  tensor(27.3756),\n",
       "  tensor(27.4430),\n",
       "  tensor(27.4707),\n",
       "  tensor(27.4611),\n",
       "  tensor(27.4176),\n",
       "  tensor(27.3443),\n",
       "  tensor(27.2457),\n",
       "  tensor(27.1413),\n",
       "  tensor(27.0349),\n",
       "  tensor(26.9209),\n",
       "  tensor(26.8038),\n",
       "  tensor(26.7168),\n",
       "  tensor(26.6331),\n",
       "  tensor(26.5551),\n",
       "  tensor(26.4941),\n",
       "  tensor(26.4541),\n",
       "  tensor(26.4213),\n",
       "  tensor(26.3960),\n",
       "  tensor(26.3786),\n",
       "  tensor(26.3690),\n",
       "  tensor(26.3669),\n",
       "  tensor(26.3717),\n",
       "  tensor(26.3827),\n",
       "  tensor(26.3989),\n",
       "  tensor(26.4194),\n",
       "  tensor(26.4429),\n",
       "  tensor(26.4684),\n",
       "  tensor(26.4948),\n",
       "  tensor(26.5260),\n",
       "  tensor(26.5640),\n",
       "  tensor(26.5989),\n",
       "  tensor(26.6297),\n",
       "  tensor(26.6555),\n",
       "  tensor(26.6757),\n",
       "  tensor(26.6900),\n",
       "  tensor(26.6984),\n",
       "  tensor(26.7011),\n",
       "  tensor(26.6984),\n",
       "  tensor(26.6909),\n",
       "  tensor(26.6793),\n",
       "  tensor(26.6645),\n",
       "  tensor(26.6474),\n",
       "  tensor(26.6288),\n",
       "  tensor(26.6098),\n",
       "  tensor(26.5910),\n",
       "  tensor(26.5733),\n",
       "  tensor(26.5574),\n",
       "  tensor(26.5437),\n",
       "  tensor(26.5327),\n",
       "  tensor(26.5246),\n",
       "  tensor(26.5195),\n",
       "  tensor(26.5174),\n",
       "  tensor(26.5180),\n",
       "  tensor(26.5211),\n",
       "  tensor(26.5263),\n",
       "  tensor(26.5332),\n",
       "  tensor(26.5414),\n",
       "  tensor(26.5502),\n",
       "  tensor(26.5593),\n",
       "  tensor(26.5681),\n",
       "  tensor(26.5762),\n",
       "  tensor(26.5831),\n",
       "  tensor(26.5882),\n",
       "  tensor(26.5903),\n",
       "  tensor(26.5845),\n",
       "  tensor(26.5432),\n",
       "  tensor(26.4135),\n",
       "  tensor(26.2009),\n",
       "  tensor(25.9632),\n",
       "  tensor(25.8510),\n",
       "  tensor(25.7739),\n",
       "  tensor(25.6307),\n",
       "  tensor(25.5927),\n",
       "  tensor(25.6561),\n",
       "  tensor(25.5727),\n",
       "  tensor(25.8484),\n",
       "  tensor(25.9758),\n",
       "  tensor(26.0241),\n",
       "  tensor(25.9942),\n",
       "  tensor(25.8220),\n",
       "  tensor(25.8322),\n",
       "  tensor(25.8709),\n",
       "  tensor(25.7909),\n",
       "  tensor(25.9063),\n",
       "  tensor(25.9781),\n",
       "  tensor(25.9019),\n",
       "  tensor(25.7416),\n",
       "  tensor(25.7391),\n",
       "  tensor(25.7251),\n",
       "  tensor(25.6410),\n",
       "  tensor(25.7086),\n",
       "  tensor(25.7579),\n",
       "  tensor(25.6961),\n",
       "  tensor(25.5788),\n",
       "  tensor(25.6017),\n",
       "  tensor(25.6118),\n",
       "  tensor(25.5874),\n",
       "  tensor(25.5914),\n",
       "  tensor(25.6651),\n",
       "  tensor(25.6731),\n",
       "  tensor(25.6224),\n",
       "  tensor(25.6233),\n",
       "  tensor(25.6531),\n",
       "  tensor(25.6601),\n",
       "  tensor(25.6491),\n",
       "  tensor(25.6560),\n",
       "  tensor(25.6860),\n",
       "  tensor(25.6815),\n",
       "  tensor(25.6509),\n",
       "  tensor(25.6593),\n",
       "  tensor(25.6671),\n",
       "  tensor(25.6630),\n",
       "  tensor(25.6491),\n",
       "  tensor(25.6305),\n",
       "  tensor(25.6143),\n",
       "  tensor(25.6134),\n",
       "  tensor(25.6055),\n",
       "  tensor(25.6128),\n",
       "  tensor(25.6198),\n",
       "  tensor(25.6237),\n",
       "  tensor(25.6238),\n",
       "  tensor(25.6209),\n",
       "  tensor(25.6169),\n",
       "  tensor(25.6141),\n",
       "  tensor(25.6150),\n",
       "  tensor(25.6203),\n",
       "  tensor(25.6283),\n",
       "  tensor(25.6365),\n",
       "  tensor(25.6428),\n",
       "  tensor(25.6463),\n",
       "  tensor(25.6468),\n",
       "  tensor(25.6451),\n",
       "  tensor(25.6426),\n",
       "  tensor(25.6407),\n",
       "  tensor(25.6400),\n",
       "  tensor(25.6405),\n",
       "  tensor(25.6414),\n",
       "  tensor(25.6419),\n",
       "  tensor(25.6414),\n",
       "  tensor(25.6398),\n",
       "  tensor(25.6375),\n",
       "  tensor(25.6352),\n",
       "  tensor(25.6336),\n",
       "  tensor(25.6332),\n",
       "  tensor(25.6339),\n",
       "  tensor(25.6355),\n",
       "  tensor(25.6373),\n",
       "  tensor(25.6389),\n",
       "  tensor(25.6399),\n",
       "  tensor(25.6404),\n",
       "  tensor(25.6407),\n",
       "  tensor(25.6410),\n",
       "  tensor(25.6415),\n",
       "  tensor(25.6423),\n",
       "  tensor(25.6432),\n",
       "  tensor(25.6439),\n",
       "  tensor(25.6441),\n",
       "  tensor(25.6438),\n",
       "  tensor(25.6431),\n",
       "  tensor(25.6423),\n",
       "  tensor(25.6415),\n",
       "  tensor(25.6410),\n",
       "  tensor(25.6407),\n",
       "  tensor(25.6406),\n",
       "  tensor(25.6405),\n",
       "  tensor(25.6403),\n",
       "  tensor(25.6401),\n",
       "  tensor(25.6398),\n",
       "  tensor(25.6397),\n",
       "  tensor(25.6397),\n",
       "  tensor(25.6400),\n",
       "  tensor(25.6403),\n",
       "  tensor(25.6406),\n",
       "  tensor(25.6408),\n",
       "  tensor(25.6408),\n",
       "  tensor(25.6407),\n",
       "  tensor(25.6405),\n",
       "  tensor(25.6404),\n",
       "  tensor(25.6402),\n",
       "  tensor(25.6401),\n",
       "  tensor(25.6398),\n",
       "  tensor(25.6394),\n",
       "  tensor(25.6390),\n",
       "  tensor(25.6385),\n",
       "  tensor(25.6381),\n",
       "  tensor(25.6378),\n",
       "  tensor(25.6375),\n",
       "  tensor(25.6372),\n",
       "  tensor(25.6370),\n",
       "  tensor(25.6367),\n",
       "  tensor(25.6364),\n",
       "  tensor(25.6361),\n",
       "  tensor(25.6359),\n",
       "  tensor(25.6357),\n",
       "  tensor(25.6355),\n",
       "  tensor(25.6353),\n",
       "  tensor(25.6350),\n",
       "  tensor(25.6347),\n",
       "  tensor(25.6344),\n",
       "  tensor(25.6340),\n",
       "  tensor(25.6337),\n",
       "  tensor(25.6333),\n",
       "  tensor(25.6329),\n",
       "  tensor(25.6324),\n",
       "  tensor(25.6320),\n",
       "  tensor(25.6315),\n",
       "  tensor(25.6311),\n",
       "  tensor(25.6307),\n",
       "  tensor(25.6303),\n",
       "  tensor(25.6299),\n",
       "  tensor(25.6295),\n",
       "  tensor(25.6290),\n",
       "  tensor(25.6286),\n",
       "  tensor(25.6283),\n",
       "  tensor(25.6279),\n",
       "  tensor(25.6275),\n",
       "  tensor(25.6270),\n",
       "  tensor(25.6266),\n",
       "  tensor(25.6262),\n",
       "  tensor(25.6257),\n",
       "  tensor(25.6253),\n",
       "  tensor(25.6248),\n",
       "  tensor(25.6244),\n",
       "  tensor(25.6239),\n",
       "  tensor(25.6234),\n",
       "  tensor(25.6229),\n",
       "  tensor(25.6224),\n",
       "  tensor(25.6219),\n",
       "  tensor(25.6214),\n",
       "  tensor(25.6210),\n",
       "  tensor(25.6205),\n",
       "  tensor(25.6200),\n",
       "  tensor(25.6195),\n",
       "  tensor(25.6191),\n",
       "  tensor(25.6186),\n",
       "  tensor(25.6181),\n",
       "  tensor(25.6176),\n",
       "  tensor(25.6171),\n",
       "  tensor(25.6167),\n",
       "  tensor(25.6162),\n",
       "  tensor(25.6157),\n",
       "  tensor(25.6152),\n",
       "  tensor(25.6147),\n",
       "  tensor(25.6142),\n",
       "  tensor(25.6137),\n",
       "  tensor(25.6132),\n",
       "  tensor(25.6127),\n",
       "  tensor(25.6122),\n",
       "  tensor(25.6117),\n",
       "  tensor(25.6112),\n",
       "  tensor(25.6107),\n",
       "  tensor(25.6103),\n",
       "  tensor(25.6098),\n",
       "  tensor(25.6093),\n",
       "  tensor(25.6088),\n",
       "  tensor(25.6083),\n",
       "  tensor(25.6079),\n",
       "  tensor(25.6074),\n",
       "  tensor(25.6069),\n",
       "  tensor(25.6064),\n",
       "  tensor(25.6059),\n",
       "  tensor(25.6055),\n",
       "  tensor(25.6050),\n",
       "  tensor(25.6045),\n",
       "  tensor(25.6040),\n",
       "  tensor(25.6036),\n",
       "  tensor(25.6031),\n",
       "  tensor(25.6026),\n",
       "  tensor(25.6022),\n",
       "  tensor(25.6017),\n",
       "  tensor(25.6013),\n",
       "  tensor(25.6008),\n",
       "  tensor(25.6004),\n",
       "  tensor(25.5999),\n",
       "  tensor(25.5995),\n",
       "  tensor(25.5990),\n",
       "  tensor(25.5986),\n",
       "  tensor(25.5982),\n",
       "  tensor(25.5977),\n",
       "  tensor(25.5973),\n",
       "  tensor(25.5969),\n",
       "  tensor(25.5964),\n",
       "  tensor(25.5960),\n",
       "  tensor(25.5956),\n",
       "  tensor(25.5952),\n",
       "  tensor(25.5948),\n",
       "  tensor(25.5944),\n",
       "  tensor(25.5940),\n",
       "  tensor(25.5936),\n",
       "  tensor(25.5932),\n",
       "  tensor(25.5928),\n",
       "  tensor(25.5924),\n",
       "  tensor(25.5920),\n",
       "  tensor(25.5916),\n",
       "  tensor(25.5912),\n",
       "  tensor(25.5908),\n",
       "  tensor(25.5904),\n",
       "  tensor(25.5901),\n",
       "  tensor(25.5897),\n",
       "  tensor(25.5893),\n",
       "  tensor(25.5890),\n",
       "  tensor(25.5886),\n",
       "  tensor(25.5882),\n",
       "  tensor(25.5879),\n",
       "  tensor(25.5875),\n",
       "  tensor(25.5872),\n",
       "  tensor(25.5868),\n",
       "  tensor(25.5865),\n",
       "  tensor(25.5861),\n",
       "  tensor(25.5858),\n",
       "  tensor(25.5855),\n",
       "  tensor(25.5851),\n",
       "  tensor(25.5848),\n",
       "  tensor(25.5845),\n",
       "  tensor(25.5842),\n",
       "  tensor(25.5838),\n",
       "  tensor(25.5835),\n",
       "  tensor(25.5832),\n",
       "  tensor(25.5829),\n",
       "  tensor(25.5826),\n",
       "  tensor(25.5823),\n",
       "  tensor(25.5820),\n",
       "  tensor(25.5817),\n",
       "  tensor(25.5814),\n",
       "  tensor(25.5811),\n",
       "  tensor(25.5808),\n",
       "  tensor(25.5805),\n",
       "  tensor(25.5802),\n",
       "  tensor(25.5799),\n",
       "  tensor(25.5796),\n",
       "  tensor(25.5794),\n",
       "  tensor(25.5791),\n",
       "  tensor(25.5788),\n",
       "  tensor(25.5785),\n",
       "  tensor(25.5783),\n",
       "  tensor(25.5780),\n",
       "  tensor(25.5777),\n",
       "  tensor(25.5775),\n",
       "  tensor(25.5772),\n",
       "  tensor(25.5769),\n",
       "  tensor(25.5767),\n",
       "  tensor(25.5764),\n",
       "  tensor(25.5762),\n",
       "  tensor(25.5759),\n",
       "  tensor(25.5757),\n",
       "  tensor(25.5754),\n",
       "  tensor(25.5752),\n",
       "  tensor(25.5749),\n",
       "  tensor(25.5747),\n",
       "  tensor(25.5745),\n",
       "  tensor(25.5742),\n",
       "  tensor(25.5740),\n",
       "  tensor(25.5738),\n",
       "  tensor(25.5735),\n",
       "  tensor(25.5733),\n",
       "  tensor(25.5731),\n",
       "  tensor(25.5729),\n",
       "  tensor(25.5726),\n",
       "  tensor(25.5724),\n",
       "  tensor(25.5722),\n",
       "  tensor(25.5720),\n",
       "  tensor(25.5718),\n",
       "  tensor(25.5715),\n",
       "  tensor(25.5713),\n",
       "  tensor(25.5711),\n",
       "  tensor(25.5709),\n",
       "  tensor(25.5707),\n",
       "  tensor(25.5705),\n",
       "  tensor(25.5703),\n",
       "  tensor(25.5701),\n",
       "  tensor(25.5698),\n",
       "  tensor(25.5696),\n",
       "  tensor(25.5694),\n",
       "  tensor(25.5692),\n",
       "  tensor(25.5690),\n",
       "  tensor(25.5689),\n",
       "  tensor(25.5687),\n",
       "  tensor(25.5685),\n",
       "  tensor(25.5683),\n",
       "  tensor(25.5681),\n",
       "  tensor(25.5679),\n",
       "  tensor(25.5677),\n",
       "  tensor(25.5675),\n",
       "  tensor(25.5673),\n",
       "  tensor(25.5671),\n",
       "  tensor(25.5669),\n",
       "  tensor(25.5667),\n",
       "  tensor(25.5665),\n",
       "  tensor(25.5664),\n",
       "  tensor(25.5662),\n",
       "  tensor(25.5660),\n",
       "  tensor(25.5658),\n",
       "  tensor(25.5656),\n",
       "  tensor(25.5655),\n",
       "  tensor(25.5653),\n",
       "  tensor(25.5651),\n",
       "  tensor(25.5649),\n",
       "  tensor(25.5647),\n",
       "  tensor(25.5646),\n",
       "  tensor(25.5644),\n",
       "  tensor(25.5642),\n",
       "  tensor(25.5640),\n",
       "  tensor(25.5639),\n",
       "  tensor(25.5637),\n",
       "  tensor(25.5635),\n",
       "  tensor(25.5633),\n",
       "  tensor(25.5632),\n",
       "  tensor(25.5630),\n",
       "  tensor(25.5628),\n",
       "  tensor(25.5627),\n",
       "  tensor(25.5625),\n",
       "  tensor(25.5623),\n",
       "  tensor(25.5621),\n",
       "  tensor(25.5620),\n",
       "  tensor(25.5618),\n",
       "  tensor(25.5616),\n",
       "  tensor(25.5615),\n",
       "  tensor(25.5613),\n",
       "  tensor(25.5611),\n",
       "  tensor(25.5610),\n",
       "  tensor(25.5608),\n",
       "  tensor(25.5606),\n",
       "  tensor(25.5605),\n",
       "  tensor(25.5603),\n",
       "  tensor(25.5601),\n",
       "  tensor(25.5600),\n",
       "  tensor(25.5598),\n",
       "  tensor(25.5597),\n",
       "  tensor(25.5595),\n",
       "  tensor(25.5593),\n",
       "  tensor(25.5592),\n",
       "  tensor(25.5590),\n",
       "  tensor(25.5588),\n",
       "  tensor(25.5587),\n",
       "  tensor(25.5585),\n",
       "  tensor(25.5584),\n",
       "  tensor(25.5582),\n",
       "  tensor(25.5580),\n",
       "  tensor(25.5579),\n",
       "  tensor(25.5577),\n",
       "  tensor(25.5576),\n",
       "  tensor(25.5574),\n",
       "  tensor(25.5572),\n",
       "  tensor(25.5571),\n",
       "  tensor(25.5569),\n",
       "  tensor(25.5567),\n",
       "  tensor(25.5566),\n",
       "  tensor(25.5564),\n",
       "  tensor(25.5563),\n",
       "  tensor(25.5561),\n",
       "  tensor(25.5559),\n",
       "  tensor(25.5558),\n",
       "  tensor(25.5556),\n",
       "  tensor(25.5555),\n",
       "  tensor(25.5553),\n",
       "  tensor(25.5551),\n",
       "  tensor(25.5550),\n",
       "  tensor(25.5548),\n",
       "  tensor(25.5547),\n",
       "  tensor(25.5545),\n",
       "  tensor(25.5543),\n",
       "  tensor(25.5542),\n",
       "  tensor(25.5540),\n",
       "  tensor(25.5539),\n",
       "  tensor(25.5537),\n",
       "  tensor(25.5536),\n",
       "  tensor(25.5534),\n",
       "  tensor(25.5532),\n",
       "  tensor(25.5531),\n",
       "  tensor(25.5529),\n",
       "  tensor(25.5527),\n",
       "  tensor(25.5526),\n",
       "  tensor(25.5524),\n",
       "  tensor(25.5523),\n",
       "  tensor(25.5521),\n",
       "  tensor(25.5520),\n",
       "  tensor(25.5518),\n",
       "  tensor(25.5516),\n",
       "  tensor(25.5515),\n",
       "  tensor(25.5513),\n",
       "  tensor(25.5511),\n",
       "  tensor(25.5510),\n",
       "  tensor(25.5508),\n",
       "  tensor(25.5507),\n",
       "  tensor(25.5505),\n",
       "  tensor(25.5503),\n",
       "  tensor(25.5502),\n",
       "  tensor(25.5500),\n",
       "  tensor(25.5498),\n",
       "  tensor(25.5497),\n",
       "  tensor(25.5495),\n",
       "  tensor(25.5493),\n",
       "  tensor(25.5492),\n",
       "  tensor(25.5490),\n",
       "  tensor(25.5489),\n",
       "  tensor(25.5487),\n",
       "  tensor(25.5485),\n",
       "  tensor(25.5484),\n",
       "  tensor(25.5482),\n",
       "  tensor(25.5480),\n",
       "  tensor(25.5479),\n",
       "  tensor(25.5477),\n",
       "  tensor(25.5475),\n",
       "  tensor(25.5474),\n",
       "  tensor(25.5472),\n",
       "  tensor(25.5470),\n",
       "  tensor(25.5469),\n",
       "  tensor(25.5467),\n",
       "  tensor(25.5465),\n",
       "  tensor(25.5464),\n",
       "  tensor(25.5462),\n",
       "  tensor(25.5460),\n",
       "  tensor(25.5459),\n",
       "  tensor(25.5457),\n",
       "  tensor(25.5455),\n",
       "  tensor(25.5454),\n",
       "  tensor(25.5452),\n",
       "  tensor(25.5450),\n",
       "  tensor(25.5449),\n",
       "  tensor(25.5447),\n",
       "  tensor(25.5445),\n",
       "  tensor(25.5443),\n",
       "  tensor(25.5442),\n",
       "  tensor(25.5440),\n",
       "  tensor(25.5438),\n",
       "  tensor(25.5437),\n",
       "  tensor(25.5435),\n",
       "  tensor(25.5433),\n",
       "  tensor(25.5432),\n",
       "  tensor(25.5430),\n",
       "  tensor(25.5428),\n",
       "  tensor(25.5426),\n",
       "  tensor(25.5425),\n",
       "  tensor(25.5423),\n",
       "  tensor(25.5421),\n",
       "  tensor(25.5420),\n",
       "  tensor(25.5418),\n",
       "  tensor(25.5416),\n",
       "  tensor(25.5414),\n",
       "  tensor(25.5413),\n",
       "  tensor(25.5411),\n",
       "  tensor(25.5409),\n",
       "  tensor(25.5407),\n",
       "  tensor(25.5406),\n",
       "  tensor(25.5404),\n",
       "  tensor(25.5402),\n",
       "  tensor(25.5400),\n",
       "  tensor(25.5399),\n",
       "  tensor(25.5397),\n",
       "  tensor(25.5395),\n",
       "  tensor(25.5393),\n",
       "  tensor(25.5391),\n",
       "  tensor(25.5390),\n",
       "  tensor(25.5388),\n",
       "  tensor(25.5386),\n",
       "  tensor(25.5384),\n",
       "  tensor(25.5383),\n",
       "  tensor(25.5381),\n",
       "  tensor(25.5379),\n",
       "  tensor(25.5377),\n",
       "  tensor(25.5375),\n",
       "  tensor(25.5374),\n",
       "  tensor(25.5372),\n",
       "  tensor(25.5370),\n",
       "  tensor(25.5369),\n",
       "  tensor(25.5367),\n",
       "  tensor(25.5365),\n",
       "  tensor(25.5363),\n",
       "  tensor(25.5361),\n",
       "  tensor(25.5359),\n",
       "  tensor(25.5358),\n",
       "  tensor(25.5356),\n",
       "  tensor(25.5354),\n",
       "  tensor(25.5352),\n",
       "  tensor(25.5350),\n",
       "  tensor(25.5348),\n",
       "  tensor(25.5347),\n",
       "  tensor(25.5345),\n",
       "  tensor(25.5343),\n",
       "  tensor(25.5341),\n",
       "  tensor(25.5340),\n",
       "  tensor(25.5338),\n",
       "  tensor(25.5336),\n",
       "  tensor(25.5334),\n",
       "  tensor(25.5332),\n",
       "  tensor(25.5331),\n",
       "  tensor(25.5329),\n",
       "  tensor(25.5327),\n",
       "  tensor(25.5325),\n",
       "  tensor(25.5323),\n",
       "  tensor(25.5321),\n",
       "  tensor(25.5319),\n",
       "  tensor(25.5318),\n",
       "  tensor(25.5316),\n",
       "  tensor(25.5314),\n",
       "  tensor(25.5312),\n",
       "  tensor(25.5310),\n",
       "  tensor(25.5308),\n",
       "  tensor(25.5306),\n",
       "  tensor(25.5305),\n",
       "  tensor(25.5303),\n",
       "  tensor(25.5301),\n",
       "  tensor(25.5299),\n",
       "  tensor(25.5297),\n",
       "  tensor(25.5295),\n",
       "  tensor(25.5293),\n",
       "  tensor(25.5292),\n",
       "  tensor(25.5290),\n",
       "  tensor(25.5288),\n",
       "  tensor(25.5286),\n",
       "  tensor(25.5284),\n",
       "  tensor(25.5282),\n",
       "  tensor(25.5280),\n",
       "  tensor(25.5279),\n",
       "  tensor(25.5277),\n",
       "  tensor(25.5275),\n",
       "  tensor(25.5273),\n",
       "  tensor(25.5271),\n",
       "  tensor(25.5269),\n",
       "  tensor(25.5267),\n",
       "  tensor(25.5265),\n",
       "  tensor(25.5264),\n",
       "  tensor(25.5262),\n",
       "  tensor(25.5260),\n",
       "  tensor(25.5258),\n",
       "  tensor(25.5256),\n",
       "  tensor(25.5254),\n",
       "  tensor(25.5252),\n",
       "  tensor(25.5250),\n",
       "  tensor(25.5248),\n",
       "  tensor(25.5247),\n",
       "  tensor(25.5244),\n",
       "  tensor(25.5242),\n",
       "  tensor(25.5241),\n",
       "  tensor(25.5239),\n",
       "  tensor(25.5237),\n",
       "  tensor(25.5235),\n",
       "  tensor(25.5233),\n",
       "  tensor(25.5231),\n",
       "  tensor(25.5229),\n",
       "  tensor(25.5227),\n",
       "  tensor(25.5225),\n",
       "  tensor(25.5224),\n",
       "  tensor(25.5222),\n",
       "  tensor(25.5219),\n",
       "  tensor(25.5217),\n",
       "  tensor(25.5216),\n",
       "  tensor(25.5214),\n",
       "  tensor(25.5212),\n",
       "  tensor(25.5210),\n",
       "  tensor(25.5208),\n",
       "  tensor(25.5206),\n",
       "  tensor(25.5204),\n",
       "  tensor(25.5202),\n",
       "  tensor(25.5200),\n",
       "  tensor(25.5198),\n",
       "  tensor(25.5196),\n",
       "  tensor(25.5194),\n",
       "  tensor(25.5192),\n",
       "  tensor(25.5190),\n",
       "  tensor(25.5189),\n",
       "  tensor(25.5187),\n",
       "  tensor(25.5185),\n",
       "  tensor(25.5183),\n",
       "  tensor(25.5181),\n",
       "  tensor(25.5179),\n",
       "  tensor(25.5177),\n",
       "  tensor(25.5175),\n",
       "  tensor(25.5173),\n",
       "  tensor(25.5171),\n",
       "  tensor(25.5169),\n",
       "  tensor(25.5167),\n",
       "  tensor(25.5165),\n",
       "  tensor(25.5163),\n",
       "  tensor(25.5161),\n",
       "  tensor(25.5159),\n",
       "  tensor(25.5157),\n",
       "  tensor(25.5155),\n",
       "  tensor(25.5153),\n",
       "  tensor(25.5151),\n",
       "  tensor(25.5149),\n",
       "  tensor(25.5147),\n",
       "  tensor(25.5146),\n",
       "  tensor(25.5144),\n",
       "  tensor(25.5142),\n",
       "  tensor(25.5140),\n",
       "  tensor(25.5138),\n",
       "  tensor(25.5136),\n",
       "  tensor(25.5134),\n",
       "  tensor(25.5132),\n",
       "  tensor(25.5130),\n",
       "  tensor(25.5128),\n",
       "  tensor(25.5126),\n",
       "  tensor(25.5124),\n",
       "  tensor(25.5122),\n",
       "  tensor(25.5120),\n",
       "  tensor(25.5118),\n",
       "  tensor(25.5116),\n",
       "  tensor(25.5114),\n",
       "  tensor(25.5112),\n",
       "  tensor(25.5110),\n",
       "  tensor(25.5108),\n",
       "  tensor(25.5106),\n",
       "  tensor(25.5104),\n",
       "  tensor(25.5102),\n",
       "  tensor(25.5100),\n",
       "  tensor(25.5098),\n",
       "  tensor(25.5096),\n",
       "  tensor(25.5094),\n",
       "  tensor(25.5092),\n",
       "  tensor(25.5090),\n",
       "  tensor(25.5088),\n",
       "  tensor(25.5086),\n",
       "  tensor(25.5084),\n",
       "  tensor(25.5082),\n",
       "  tensor(25.5080),\n",
       "  tensor(25.5078),\n",
       "  tensor(25.5076),\n",
       "  tensor(25.5074),\n",
       "  tensor(25.5072),\n",
       "  tensor(25.5070),\n",
       "  tensor(25.5068),\n",
       "  tensor(25.5066),\n",
       "  tensor(25.5064),\n",
       "  tensor(25.5062),\n",
       "  tensor(25.5060),\n",
       "  tensor(25.5058),\n",
       "  tensor(25.5056),\n",
       "  tensor(25.5054),\n",
       "  tensor(25.5052),\n",
       "  tensor(25.5050),\n",
       "  tensor(25.5048),\n",
       "  tensor(25.5046),\n",
       "  tensor(25.5044),\n",
       "  tensor(25.5042),\n",
       "  tensor(25.5040),\n",
       "  tensor(25.5038),\n",
       "  tensor(25.5036),\n",
       "  tensor(25.5034),\n",
       "  tensor(25.5032),\n",
       "  tensor(25.5030),\n",
       "  tensor(25.5027),\n",
       "  tensor(25.5025),\n",
       "  tensor(25.5024),\n",
       "  tensor(25.5022),\n",
       "  tensor(25.5019),\n",
       "  tensor(25.5017),\n",
       "  tensor(25.5015),\n",
       "  tensor(25.5013),\n",
       "  tensor(25.5011),\n",
       "  tensor(25.5009),\n",
       "  tensor(25.5007),\n",
       "  tensor(25.5005),\n",
       "  tensor(25.5003),\n",
       "  tensor(25.5001),\n",
       "  tensor(25.4999),\n",
       "  tensor(25.4997),\n",
       "  tensor(25.4995),\n",
       "  tensor(25.4993),\n",
       "  tensor(25.4991),\n",
       "  tensor(25.4989),\n",
       "  tensor(25.4987),\n",
       "  tensor(25.4985),\n",
       "  tensor(25.4983),\n",
       "  tensor(25.4981),\n",
       "  tensor(25.4979),\n",
       "  tensor(25.4977),\n",
       "  tensor(25.4975),\n",
       "  tensor(25.4973),\n",
       "  tensor(25.4971),\n",
       "  tensor(25.4969),\n",
       "  tensor(25.4967),\n",
       "  tensor(25.4964),\n",
       "  tensor(25.4962),\n",
       "  tensor(25.4960),\n",
       "  tensor(25.4958),\n",
       "  tensor(25.4956),\n",
       "  tensor(25.4954),\n",
       "  tensor(25.4952),\n",
       "  tensor(25.4950),\n",
       "  tensor(25.4948),\n",
       "  tensor(25.4946),\n",
       "  tensor(25.4944),\n",
       "  tensor(25.4942),\n",
       "  tensor(25.4940),\n",
       "  tensor(25.4938),\n",
       "  tensor(25.4936),\n",
       "  tensor(25.4934),\n",
       "  tensor(25.4932),\n",
       "  tensor(25.4930),\n",
       "  tensor(25.4928),\n",
       "  tensor(25.4926),\n",
       "  tensor(25.4923),\n",
       "  tensor(25.4921),\n",
       "  tensor(25.4919),\n",
       "  tensor(25.4918),\n",
       "  tensor(25.4915),\n",
       "  tensor(25.4913),\n",
       "  tensor(25.4911),\n",
       "  tensor(25.4909),\n",
       "  tensor(25.4907),\n",
       "  tensor(25.4905),\n",
       "  tensor(25.4903),\n",
       "  tensor(25.4901),\n",
       "  tensor(25.4899),\n",
       "  tensor(25.4897),\n",
       "  tensor(25.4895),\n",
       "  tensor(25.4893),\n",
       "  tensor(25.4891),\n",
       "  tensor(25.4889),\n",
       "  tensor(25.4887),\n",
       "  tensor(25.4884),\n",
       "  tensor(25.4882),\n",
       "  tensor(25.4880),\n",
       "  tensor(25.4878),\n",
       "  tensor(25.4876),\n",
       "  tensor(25.4874),\n",
       "  tensor(25.4872),\n",
       "  tensor(25.4870),\n",
       "  tensor(25.4868),\n",
       "  tensor(25.4866),\n",
       "  tensor(25.4864),\n",
       "  tensor(25.4862),\n",
       "  tensor(25.4860),\n",
       "  tensor(25.4858),\n",
       "  tensor(25.4856),\n",
       "  tensor(25.4854),\n",
       "  tensor(25.4851),\n",
       "  tensor(25.4849),\n",
       "  tensor(25.4847),\n",
       "  tensor(25.4845),\n",
       "  tensor(25.4843),\n",
       "  tensor(25.4841),\n",
       "  tensor(25.4839),\n",
       "  tensor(25.4837),\n",
       "  tensor(25.4835),\n",
       "  tensor(25.4833),\n",
       "  tensor(25.4831),\n",
       "  tensor(25.4829),\n",
       "  tensor(25.4827),\n",
       "  tensor(25.4825),\n",
       "  tensor(25.4823),\n",
       "  tensor(25.4821),\n",
       "  tensor(25.4818),\n",
       "  tensor(25.4816),\n",
       "  tensor(25.4814),\n",
       "  tensor(25.4812),\n",
       "  tensor(25.4810),\n",
       "  tensor(25.4808),\n",
       "  tensor(25.4806),\n",
       "  tensor(25.4804),\n",
       "  tensor(25.4802),\n",
       "  tensor(25.4800),\n",
       "  tensor(25.4798),\n",
       "  tensor(25.4796),\n",
       "  tensor(25.4794),\n",
       "  tensor(25.4791),\n",
       "  tensor(25.4789),\n",
       "  tensor(25.4787),\n",
       "  tensor(25.4785),\n",
       "  tensor(25.4783),\n",
       "  tensor(25.4781),\n",
       "  tensor(25.4779),\n",
       "  tensor(25.4777),\n",
       "  tensor(25.4775),\n",
       "  tensor(25.4773),\n",
       "  tensor(25.4771),\n",
       "  tensor(25.4769),\n",
       "  tensor(25.4767),\n",
       "  tensor(25.4765),\n",
       "  tensor(25.4762),\n",
       "  tensor(25.4760),\n",
       "  tensor(25.4758),\n",
       "  tensor(25.4756),\n",
       "  tensor(25.4754),\n",
       "  tensor(25.4752),\n",
       "  tensor(25.4750),\n",
       "  tensor(25.4748),\n",
       "  tensor(25.4746),\n",
       "  tensor(25.4744),\n",
       "  tensor(25.4742),\n",
       "  tensor(25.4740),\n",
       "  tensor(25.4738),\n",
       "  tensor(25.4736),\n",
       "  tensor(25.4734),\n",
       "  tensor(25.4731),\n",
       "  tensor(25.4729),\n",
       "  tensor(25.4727),\n",
       "  tensor(25.4725),\n",
       "  tensor(25.4723),\n",
       "  tensor(25.4721),\n",
       "  tensor(25.4719),\n",
       "  tensor(25.4717),\n",
       "  tensor(25.4715),\n",
       "  tensor(25.4713),\n",
       "  tensor(25.4711),\n",
       "  tensor(25.4709),\n",
       "  tensor(25.4707),\n",
       "  tensor(25.4704),\n",
       "  tensor(25.4702),\n",
       "  tensor(25.4700),\n",
       "  tensor(25.4698),\n",
       "  tensor(25.4696),\n",
       "  tensor(25.4694),\n",
       "  tensor(25.4692),\n",
       "  tensor(25.4690),\n",
       "  tensor(25.4688),\n",
       "  tensor(25.4686),\n",
       "  tensor(25.4684),\n",
       "  tensor(25.4682),\n",
       "  tensor(25.4680),\n",
       "  tensor(25.4678),\n",
       "  tensor(25.4675),\n",
       "  tensor(25.4674),\n",
       "  tensor(25.4672),\n",
       "  tensor(25.4669),\n",
       "  tensor(25.4667),\n",
       "  tensor(25.4665),\n",
       "  tensor(25.4663),\n",
       "  tensor(25.4661),\n",
       "  tensor(25.4659),\n",
       "  tensor(25.4657),\n",
       "  tensor(25.4655),\n",
       "  tensor(25.4653),\n",
       "  tensor(25.4651),\n",
       "  tensor(25.4649),\n",
       "  tensor(25.4647),\n",
       "  tensor(25.4645),\n",
       "  tensor(25.4643),\n",
       "  tensor(25.4641),\n",
       "  tensor(25.4639),\n",
       "  tensor(25.4636),\n",
       "  tensor(25.4634),\n",
       "  tensor(25.4632),\n",
       "  tensor(25.4630),\n",
       "  tensor(25.4628),\n",
       "  tensor(25.4626),\n",
       "  tensor(25.4624),\n",
       "  tensor(25.4622),\n",
       "  tensor(25.4620),\n",
       "  tensor(25.4618),\n",
       "  tensor(25.4616),\n",
       "  tensor(25.4614),\n",
       "  tensor(25.4612),\n",
       "  tensor(25.4610),\n",
       "  tensor(25.4608),\n",
       "  tensor(25.4606),\n",
       "  tensor(25.4604),\n",
       "  tensor(25.4602),\n",
       "  tensor(25.4599),\n",
       "  tensor(25.4598),\n",
       "  tensor(25.4596),\n",
       "  tensor(25.4593),\n",
       "  tensor(25.4591),\n",
       "  tensor(25.4589),\n",
       "  tensor(25.4587),\n",
       "  tensor(25.4585),\n",
       "  tensor(25.4583),\n",
       "  tensor(25.4581),\n",
       "  tensor(25.4579),\n",
       "  tensor(25.4577),\n",
       "  tensor(25.4575),\n",
       "  tensor(25.4573),\n",
       "  tensor(25.4571),\n",
       "  tensor(25.4569),\n",
       "  ...],\n",
       " 'val_loss': [],\n",
       " 'val_mae': []}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from examples.introduction.my_GAT_implem import myGATConv\n",
    "\n",
    "my_module = myGATConv(\n",
    "    in_channels=(1,1),\n",
    "    out_channels=1,\n",
    "    heads=1,\n",
    "    negative_slope=0.2,\n",
    "    add_self_loops=False,\n",
    "    edge_dim=1)\n",
    "# my_module(x=participant_graph.x,edge_index=participant_graph.edge_index,edge_attr=participant_graph.edge_attr)\n",
    "\n",
    "complete_model = GNN_naive_framework(my_module,device)\n",
    "opt = complete_model.configure_optimizer(lr=1)\n",
    "scheduler = complete_model.configure_scheduler(opt,1,1,10)\n",
    "\n",
    "complete_model.train([participant_graph],10000,1,opt,scheduler,\"train_loss\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           6.430633544921875,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375,
           62.5609130859375
          ]
         },
         "mode": "markers",
         "type": "scatter",
         "x": [
          21,
          21,
          80,
          78,
          59,
          75,
          88,
          90,
          88,
          85,
          7,
          29,
          77,
          53,
          89,
          53,
          77,
          38,
          50,
          87,
          83,
          84,
          100,
          100,
          94,
          50,
          20,
          94,
          13,
          90,
          17,
          77,
          53,
          83,
          76,
          100,
          83,
          35,
          56,
          24,
          22,
          90,
          29,
          79,
          100,
          6,
          62,
          46,
          17,
          92,
          46,
          40,
          9,
          79,
          13,
          73,
          45,
          73,
          100,
          100
         ],
         "y": [
          41.5609130859375,
          41.5609130859375,
          17.4390869140625,
          15.4390869140625,
          3.5609130859375,
          12.4390869140625,
          25.4390869140625,
          27.4390869140625,
          25.4390869140625,
          22.4390869140625,
          55.5609130859375,
          33.5609130859375,
          14.4390869140625,
          9.5609130859375,
          26.4390869140625,
          9.5609130859375,
          14.4390869140625,
          24.5609130859375,
          12.5609130859375,
          24.4390869140625,
          20.4390869140625,
          21.4390869140625,
          37.4390869140625,
          37.4390869140625,
          31.4390869140625,
          12.5609130859375,
          42.5609130859375,
          31.4390869140625,
          49.5609130859375,
          27.4390869140625,
          45.5609130859375,
          14.4390869140625,
          9.5609130859375,
          20.4390869140625,
          13.4390869140625,
          37.4390869140625,
          20.4390869140625,
          27.5609130859375,
          6.5609130859375,
          38.5609130859375,
          40.5609130859375,
          27.4390869140625,
          33.5609130859375,
          16.4390869140625,
          37.4390869140625,
          0.430633544921875,
          0.5609130859375,
          16.5609130859375,
          45.5609130859375,
          29.4390869140625,
          16.5609130859375,
          22.5609130859375,
          53.5609130859375,
          16.4390869140625,
          49.5609130859375,
          10.4390869140625,
          17.5609130859375,
          10.4390869140625,
          37.4390869140625,
          37.4390869140625
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Residual depending on label value"
        },
        "xaxis": {
         "title": {
          "text": "Label"
         }
        },
        "yaxis": {
         "title": {
          "text": "Residual"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "type": "histogram",
         "x": [
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          6.430633544921875,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375,
          62.5609130859375
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Residual depending on label value"
        },
        "xaxis": {
         "title": {
          "text": "Label"
         }
        },
        "yaxis": {
         "title": {
          "text": "Residual"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "preds, (adj, alpha) = complete_model.predict(participant_graph.x,\n",
    "                               participant_graph.edge_index,\n",
    "                               participant_graph.edge_attr,\n",
    "                               return_attention_weights=True)\n",
    "preds = np.array(preds.detach().to(\"cpu\"))\n",
    "preds = np.squeeze(preds)\n",
    "labels = np.array(participant_graph.y)\n",
    "labels = np.squeeze(labels)\n",
    "\n",
    "errors = np.abs(labels-preds)\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = labels,\n",
    "    y = errors,\n",
    "    mode = \"markers\",\n",
    "    marker=dict(color=preds)\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=\"Residual depending on label value\",\n",
    "    xaxis_title=\"Label\",\n",
    "    yaxis_title=\"Residual\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(\n",
    "    x = preds)\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Residual depending on label value\",\n",
    "    xaxis_title=\"Label\",\n",
    "    yaxis_title=\"Residual\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "xaxis": "x",
         "yaxis": "y",
         "z": [
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           2.6359491546001705e-20,
           2.6343235443636992e-18,
           3.6446187673229444e-21,
           4.5553459949928917e-20,
           1.057254798568867e-22,
           2.8849697777459854e-20,
           2.6405473175316207e-21,
           7.348664181062841e-22,
           1.062055652355316e-17,
           2.2300876152238073e-21,
           0,
           6.470853606628799e-18,
           1.148573884742264e-19,
           1.433403188917242e-20,
           9.434594592111382e-18,
           6.916179524286522e-17,
           8.478936512998816e-22,
           7.706455318714132e-20,
           1.6674750950745648e-15,
           6.765686419541687e-18,
           1.7447037267223376e-20,
           7.349761467649341e-21,
           1.9630601829494436e-20,
           4.908429877191516e-22,
           3.494410926798341e-20,
           3.2296111032226923e-18,
           2.0133507608646217e-19,
           2.5733334401551103e-22,
           8.668303519930207e-20,
           2.665556404415887e-20,
           2.4548762488967584e-20,
           1.312281753111227e-21,
           1.0721579704023349e-19,
           4.044306510103786e-19,
           4.56159088550024e-19,
           2.298882656742266e-17,
           2.9305609099567025e-18,
           8.711263920524972e-20,
           1.2220204145535836e-20,
           8.750221865554232e-22,
           1.100967986128612e-18,
           8.682600173595449e-20,
           2.9556486048485553e-21,
           2.2645301469373088e-23,
           3.092099357420958e-20,
           1,
           5.807362628120548e-19,
           3.0837228612491516e-21,
           1.5990334242219848e-21,
           8.5213648619763e-18,
           1.3073079210962657e-17,
           1.385275494433434e-19,
           1.8869287411920504e-19,
           1.314401625574125e-20,
           1.463231838793787e-19,
           5.746170598138442e-18,
           3.73753191040732e-20,
           1.1033453288516884e-15,
           6.543043326376121e-23,
           9.187710130585742e-19
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1.4690909298474317e-37,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Alpha: the message passing strength between nodes"
        },
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "y"
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch_geometric.utils import (\n",
    "    add_self_loops,\n",
    "    is_torch_sparse_tensor,\n",
    "    remove_self_loops,\n",
    "    softmax,\n",
    "    to_dense_adj\n",
    ")\n",
    "\n",
    "matrix_alpha = to_dense_adj(adj, edge_attr = alpha).cpu().detach()\n",
    "matrix_alpha = matrix_alpha.squeeze()\n",
    "fig = px.imshow(matrix_alpha)\n",
    "fig.update_layout(\n",
    "    title=\"Alpha: the message passing strength between nodes\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_x_src, param_x_dst = -42.656185150146484 5.747586250305176\n",
      "lin_content_src_params, lin_content_src_params = 6.158009052276611 -1.351555585861206\n",
      "params_edge tensor(-26.7681, device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "tensor(399.3427, device='cuda:0', grad_fn=<UnbindBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lin_src_params = [param for param in complete_model.update_node_module.lin_src.parameters()][0]\n",
    "lin_dst_params = [param for param in complete_model.update_node_module.lin_dst.parameters()][0]\n",
    "\n",
    "src_att_lin_params = lin_src_params * complete_model.update_node_module.att_src\n",
    "dst_att_lin_params = lin_dst_params * complete_model.update_node_module.att_dst\n",
    "#lin_params.squeeze()\n",
    "print(\"param_x_src, param_x_dst =\", float(src_att_lin_params), float(dst_att_lin_params))\n",
    "\n",
    "\n",
    "lin_content_src_params = [param for param in complete_model.update_node_module.lin_src.parameters()][0]\n",
    "lin_content_dst_params = [param for param in complete_model.update_node_module.lin_dst.parameters()][0]\n",
    "\n",
    "#lin_params.squeeze()\n",
    "print(\"lin_content_src_params, lin_content_src_params =\", float(lin_content_src_params), float(lin_content_dst_params))\n",
    "\n",
    "lin_edge_params = [param for param in complete_model.update_node_module.lin_edge.parameters()][0]\n",
    "att_lin_edge_params = lin_edge_params * complete_model.update_node_module.att_edge\n",
    "print(\"params_edge\", att_lin_edge_params.squeeze())\n",
    "\n",
    "bias = [param for param in complete_model.update_node_module.bias][0]\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participant_graph.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "loader = NeighborLoader(\n",
    "    participant_graph,\n",
    "    # Sample 30 neighbors for each node for 2 iterations\n",
    "    num_neighbors=[60],\n",
    "    # Use a batch size of 128 for sampling training nodes\n",
    "    batch_size=1,\n",
    "    input_nodes=participant_graph.train_mask,\n",
    ")\n",
    "\n",
    "sampled_data = next(iter(loader))\n",
    "print(sampled_data.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in complete_model.update_node_module.parameters():\n",
    "    print(param)\n",
    "\n",
    "print(\"1\")\n",
    "print([param for param in complete_model.update_node_module.lin.parameters()])\n",
    "\n",
    "print(\"att_src\",complete_model.update_node_module.att_src)\n",
    "print(\"att_dst\",complete_model.update_node_module.att_dst)\n",
    "print(\"lin\",[param for param in complete_model.update_node_module.lin.parameters()])\n",
    "print(\"lin_edge\",[param for param in complete_model.update_node_module.lin_edge.parameters()])\n",
    "print(\"att_edge\",complete_model.update_node_module.att_edge[0])\n",
    "print(\"bias\",complete_model.update_node_module.bias)\n",
    "\n",
    "lin_params = [param for param in complete_model.update_node_module.lin.parameters()][0]\n",
    "src_att_lin_params = lin_params * complete_model.update_node_module.att_src\n",
    "dst_att_lin_params = lin_params * complete_model.update_node_module.att_dst\n",
    "#lin_params.squeeze()\n",
    "print(\"param_x_src, param_x_dst =\", float(src_att_lin_params), float(dst_att_lin_params))\n",
    "\n",
    "\n",
    "lin_edge_params = [param for param in complete_model.update_node_module.lin_edge.parameters()][0]\n",
    "att_lin_edge_params = lin_edge_params * complete_model.update_node_module.att_edge\n",
    "print(\"params_edge\", att_lin_edge_params.squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_semantic_to_liking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
